#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæ™ºèƒ½æ—…è¡Œå¯¹è¯Agent
ä½¿ç”¨è±†åŒ…Agentä½œä¸ºæ ¸å¿ƒæ¨ç†å¼•æ“ï¼ŒMCPæœåŠ¡æä¾›å®æ—¶æ•°æ®æ”¯æŒ
"""

import json
import logging
import os
import re
import requests
import urllib3
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import pandas as pd
from pathlib import Path
from threading import Lock
import jieba
import jieba.analyse

from config import (
    API_KEYS, AMAP_CONFIG, RAG_CONFIG, DEFAULT_CONFIG,
    get_api_key, get_config
)

# å¯¼å…¥æ–°çš„æ¨¡å—åŒ–ç»„ä»¶
# ä½¿ç”¨try-exceptå¤„ç†ç›¸å¯¹å¯¼å…¥å’Œç»å¯¹å¯¼å…¥ä¸¤ç§æƒ…å†µ
try:
    # ç›¸å¯¹å¯¼å…¥ï¼ˆä½œä¸ºåŒ…çš„ä¸€éƒ¨åˆ†ï¼‰
    from .mcp import MCPServiceType, MCPClient, WeatherInfo, RouteInfo, POIInfo
    from .rag import RAGClient, SearchMode
    from .model.doubao_agent import DouBaoAgent
    try:
        from .model.deepseek_agent import DeepSeekAgent
        DEEPSEEK_AVAILABLE = True
    except ImportError:
        DeepSeekAgent = None
        DEEPSEEK_AVAILABLE = False
    from .model.models import TravelPreference, ThoughtProcess, UserContext, WeatherCondition, TrafficCondition, CrowdLevel
except ImportError:
    # ç»å¯¹å¯¼å…¥ï¼ˆç›´æ¥ä½œä¸ºæ¨¡å—å¯¼å…¥ï¼‰
    from mcp import MCPServiceType, MCPClient, WeatherInfo, RouteInfo, POIInfo
    from rag import RAGClient, SearchMode
    from model.doubao_agent import DouBaoAgent
    try:
        from model.deepseek_agent import DeepSeekAgent
        DEEPSEEK_AVAILABLE = True
    except ImportError:
        DeepSeekAgent = None
        DEEPSEEK_AVAILABLE = False
    from model.models import TravelPreference, ThoughtProcess, UserContext, WeatherCondition, TrafficCondition, CrowdLevel

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ³¨æ„ï¼šæ‰€æœ‰æšä¸¾å’Œæ•°æ®ç»“æ„å·²ç§»è‡³æ¨¡å—åŒ–ç»„ä»¶
# - MCPServiceType, WeatherInfo, RouteInfo, POIInfo ä» .mcp å¯¼å…¥
# - TravelPreference, ThoughtProcess, UserContext, WeatherCondition, TrafficCondition, CrowdLevel ä» .model.models å¯¼å…¥
# - DouBaoAgent ä» .model.doubao_agent å¯¼å…¥
# - DeepSeekAgent ä» .model.deepseek_agent å¯¼å…¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰

# ä¸ºäº†å‘åå…¼å®¹ï¼Œé‡æ–°å¯¼å‡ºè¿™äº›ç±»ä¾›å¤–éƒ¨ç›´æ¥å¯¼å…¥
__all__ = ['EnhancedTravelAgent', 'TravelPreference', 'UserContext', 'ThoughtProcess', 
           'WeatherCondition', 'TrafficCondition', 'CrowdLevel', 'MCPServiceType',
           'WeatherInfo', 'RouteInfo', 'POIInfo']

class EnhancedTravelAgent:
    """å¢å¼ºç‰ˆæ™ºèƒ½æ—…è¡Œå¯¹è¯Agent"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆAgent"""
        self.config = get_config()
        self.user_contexts = {}
        
        # æ ¹æ®é…ç½®é€‰æ‹©AI Providerï¼ˆä¼˜å…ˆä½¿ç”¨DeepSeekï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨è±†åŒ…ï¼‰
        ai_provider = os.getenv('AI_PROVIDER', 'deepseek').lower()
        deepseek_api_key = get_api_key("DEEPSEEK")
        doubao_api_key = get_api_key("DOUBAO")
        
        # åˆå§‹åŒ–AI Agent
        if ai_provider == 'deepseek' and deepseek_api_key and DEEPSEEK_AVAILABLE and DeepSeekAgent:
            try:
                from config import Config
                self.ai_agent = DeepSeekAgent(
                    api_key=deepseek_api_key,
                    base_url=Config.DEEPSEEK_API_BASE,
                    model=Config.DEEPSEEK_MODEL
                )
                self.doubao_agent = self.ai_agent  # ä¿æŒå‘åå…¼å®¹
                logger.info("âœ… ä½¿ç”¨DeepSeek Agent")
            except Exception as e:
                logger.warning(f"âš ï¸ DeepSeek Agentåˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°è¯•ä½¿ç”¨è±†åŒ…Agent")
                if doubao_api_key:
                    self.ai_agent = DouBaoAgent(doubao_api_key)
                    self.doubao_agent = self.ai_agent  # ä¿æŒå‘åå…¼å®¹
                    logger.info("âœ… ä½¿ç”¨è±†åŒ…Agentï¼ˆDeepSeekåˆå§‹åŒ–å¤±è´¥åçš„å¤‡é€‰ï¼‰")
                else:
                    raise ValueError("DeepSeekå’Œè±†åŒ…APIå¯†é’¥éƒ½æœªé…ç½®æˆ–åˆå§‹åŒ–å¤±è´¥")
        elif doubao_api_key:
            self.ai_agent = DouBaoAgent(doubao_api_key)
            self.doubao_agent = self.ai_agent  # ä¿æŒå‘åå…¼å®¹
            logger.info("âœ… ä½¿ç”¨è±†åŒ…Agent")
        else:
            raise ValueError("ç¼ºå°‘AI APIå¯†é’¥é…ç½®ï¼ˆéœ€è¦DEEPSEEK_API_KEYæˆ–DOUBAO_API_KEYï¼‰")
        
        # APIè¯·æ±‚é™æµæ§åˆ¶
        self._api_lock = Lock()
        self._last_api_call = {}  # è®°å½•æ¯ä¸ªAPIçš„æœ€åè°ƒç”¨æ—¶é—´
        self._min_interval = 0.35  # æœ€å°è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œç¡®ä¿ä¸è¶…è¿‡3æ¬¡/ç§’
        
        # åŠ è½½Excelæ™¯ç‚¹æ•°æ®
        self.qunar_places = self._load_qunar_places()
        
        # åˆå§‹åŒ–MCPå®¢æˆ·ç«¯
        self.mcp_client = MCPClient(
            api_lock=self._api_lock,
            last_api_call=self._last_api_call,
            min_interval=self._min_interval,
            qunar_places=self.qunar_places
        )
        
        # åˆå§‹åŒ–RAGå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨BERT embeddingï¼‰
        self.rag_client = None
        self._init_rag_client()
        
        # ä¸Šæµ·åœ°åŒºå…³é”®è¯æ˜ å°„
        self.location_keywords = {
            # æµ¦ä¸œæ–°åŒº
            "æµ¦ä¸œ": ["ä¸œæ–¹æ˜ç ", "é™†å®¶å˜´", "ä¸Šæµ·ä¸­å¿ƒ", "ç¯çƒé‡‘èä¸­å¿ƒ", "é‡‘èŒ‚å¤§å¦", "æµ·æ´‹é¦†", "ç§‘æŠ€é¦†", "è¿ªå£«å°¼", "æµ¦ä¸œæœºåœº"],
            "é™†å®¶å˜´": ["ä¸œæ–¹æ˜ç ", "ä¸Šæµ·ä¸­å¿ƒ", "ç¯çƒé‡‘èä¸­å¿ƒ", "é‡‘èŒ‚å¤§å¦", "æ­£å¤§å¹¿åœº"],
            "è¿ªå£«å°¼": ["ä¸Šæµ·è¿ªå£«å°¼ä¹å›­", "è¿ªå£«å°¼å°é•‡", "å¥•æ¬§æ¥å¥¥ç‰¹è±æ–¯"],
            
            # é»„æµ¦åŒº
            "å¤–æ»©": ["å¤–æ»©", "å—äº¬è·¯", "å’Œå¹³é¥­åº—", "å¤–ç™½æ¸¡æ¡¥"],
            "äººæ°‘å¹¿åœº": ["äººæ°‘å¹¿åœº", "ä¸Šæµ·åšç‰©é¦†", "ä¸Šæµ·å¤§å‰§é™¢", "äººæ°‘å…¬å›­"],
            "è±«å›­": ["è±«å›­", "åŸéšåº™", "å—ç¿”é¦’å¤´åº—"],
            "å—äº¬è·¯": ["å—äº¬è·¯æ­¥è¡Œè¡—", "ç¬¬ä¸€ç™¾è´§", "æ–°ä¸–ç•Œ"],
            
            # å¾æ±‡åŒº
            "å¾å®¶æ±‡": ["å¾å®¶æ±‡", "å¤ªå¹³æ´‹ç™¾è´§", "æ¸¯æ±‡æ’éš†", "ä¸Šæµ·ä½“è‚²é¦†"],
            "æ·®æµ·è·¯": ["æ·®æµ·è·¯", "æ–°å¤©åœ°", "ç”°å­åŠ", "æ€å—è·¯"],
            
            # é™å®‰åŒº
            "é™å®‰å¯º": ["é™å®‰å¯º", "ä¹…å…‰ç™¾è´§", "å˜‰é‡Œä¸­å¿ƒ"],
            "å—äº¬è¥¿è·¯": ["é™å®‰å˜‰é‡Œä¸­å¿ƒ", "æ¢…é¾™é•‡å¹¿åœº", "ä¸­ä¿¡æ³°å¯Œ"],
            
            # é•¿å®åŒº
            "è™¹æ¡¥": ["è™¹æ¡¥æœºåœº", "è™¹æ¡¥ç«è½¦ç«™", "é¾™ä¹‹æ¢¦"],
            
            # æ™®é™€åŒº
            "é•¿é£å…¬å›­": ["é•¿é£å…¬å›­", "é•¿é£æµ·æ´‹ä¸–ç•Œ"],
            
            # è™¹å£åŒº
            "å››å·åŒ—è·¯": ["å¤šä¼¦è·¯", "é²è¿…å…¬å›­", "è™¹å£è¶³çƒåœº"],
            
            # æ¨æµ¦åŒº
            "äº”è§’åœº": ["äº”è§’åœº", "åˆç”Ÿæ±‡", "å¤§å­¦è·¯"],
            
            # é—µè¡ŒåŒº
            "ä¸ƒå®": ["ä¸ƒå®å¤é•‡", "ä¸ƒå®è€è¡—"],
            
            # é’æµ¦åŒº
            "æœ±å®¶è§’": ["æœ±å®¶è§’å¤é•‡", "è¯¾æ¤å›­", "å¤§æ¸…é‚®å±€"],
            
            # æ¾æ±ŸåŒº
            "ä½˜å±±": ["ä½˜å±±", "æ¬¢ä¹è°·", "ç›é›…æµ·æ»©"],
            
            # å˜‰å®šåŒº
            "å—ç¿”": ["å¤æ¼ªå›­", "å—ç¿”è€è¡—"]
        }
        
        # æ´»åŠ¨ç±»å‹å…³é”®è¯
        self.activity_keywords = {
            "è´­ç‰©": ["shopping", "ä¹°", "å•†åœº", "ç™¾è´§", "å¥¥ç‰¹è±æ–¯", "ä¸“å–åº—"],
            "ç¾é£Ÿ": ["åƒ", "é¤å…", "å°åƒ", "ç¾é£Ÿ", "èœ", "æ–™ç†", "ç«é”…", "çƒ§çƒ¤"],
            "æ–‡åŒ–": ["åšç‰©é¦†", "å±•è§ˆ", "å†å²", "æ–‡åŒ–", "å¤è¿¹", "è‰ºæœ¯"],
            "å¨±ä¹": ["æ¸¸ä¹", "å¨±ä¹", "KTV", "ç”µå½±", "é…’å§", "å¤œç”Ÿæ´»"],
            "è‡ªç„¶": ["å…¬å›­", "èŠ±å›­", "æ¹–", "æ±Ÿ", "å±±", "æµ·", "è‡ªç„¶"],
            "å•†åŠ¡": ["ä¼šè®®", "å•†åŠ¡", "åŠå…¬", "å·¥ä½œ"],
            "äº²å­": ["å­©å­", "å„¿ç«¥", "äº²å­", "å®¶åº­", "å¸¦å¨ƒ"]
        }
        
        # å¤©æ°”ç›¸å…³å…³é”®è¯
        self.weather_keywords = ["å¤©æ°”", "ä¸‹é›¨", "æ™´å¤©", "é˜´å¤©", "æ¸©åº¦", "å†·", "çƒ­", "é£", "é›ª"]
        
        # äº¤é€šç›¸å…³å…³é”®è¯
        self.traffic_keywords = ["å¼€è½¦", "è‡ªé©¾", "åœ°é“", "å…¬äº¤", "æ‰“è½¦", "èµ°è·¯", "éª‘è½¦", "äº¤é€š", "å µè½¦"]
        
        # æ—¶é—´ç›¸å…³å…³é”®è¯
        self.time_keywords = ["ä»Šå¤©", "æ˜å¤©", "å‘¨æœ«", "æ—©ä¸Š", "ä¸Šåˆ", "ä¸‹åˆ", "æ™šä¸Š", "å¤œé‡Œ"]
        
        logger.info("ğŸ¤– å¢å¼ºç‰ˆæ™ºèƒ½æ—…è¡Œå¯¹è¯Agentåˆå§‹åŒ–å®Œæˆ")
    
    def _init_rag_client(self):
        """åˆå§‹åŒ–RAGå®¢æˆ·ç«¯ï¼ˆå¯é€‰åŠŸèƒ½ï¼Œæ”¯æŒæ•°æ®åº“å’Œæ–‡ä»¶ä¸¤ç§æ¨¡å¼ï¼‰"""
        try:
            import os
            
            # ä¼˜å…ˆå°è¯•ä½¿ç”¨æ•°æ®åº“æ¨¡å¼
            db_url = os.getenv('RAG_DB_URL', '')
            
            if db_url:
                # æ•°æ®åº“æ¨¡å¼
                try:
                    from Rag import RAGClient, SearchMode
                    from langchain_openai import OpenAIEmbeddings
                    openai_api_key = os.getenv('OPENAI_API_KEY', '')
                    if openai_api_key:
                        embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)
                        self.rag_client = RAGClient(db_url, embedding_model)
                        logger.info("âœ… RAGå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆæ•°æ®åº“æ¨¡å¼ï¼‰")
                        return
                except Exception as e:
                    logger.warning(f"âš ï¸ æ•°æ®åº“æ¨¡å¼RAGåˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°è¯•æ–‡ä»¶æ¨¡å¼")
            
            # æ–‡ä»¶æ¨¡å¼ï¼ˆæ— éœ€æ•°æ®åº“ï¼‰- ä½¿ç”¨æ–°çš„RAGæ¨¡å—ï¼ˆBERT embeddingï¼‰
            try:
                from .rag import RAGClient
                
                # è®¾ç½®å­˜å‚¨è·¯å¾„
                storage_path = os.getenv('RAG_STORAGE_PATH', './rag_storage')
                
                # ä½¿ç”¨BERT Embeddingï¼ˆé»˜è®¤ï¼‰
                # RAGClientä¼šè‡ªåŠ¨åˆå§‹åŒ–BERTæ¨¡å‹ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å…³é”®è¯æ£€ç´¢
                self.rag_client = RAGClient(storage_path=storage_path)
                logger.info(f"âœ… RAGå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆBERT Embeddingï¼Œå­˜å‚¨è·¯å¾„: {storage_path}ï¼‰")
                
                # è‡ªåŠ¨ä»dataç›®å½•åŠ è½½æ–‡æ¡£
                self._load_rag_documents_from_data()
                
            except ImportError:
                logger.warning("âš ï¸ æ–‡ä»¶RAGæ¨¡å—å¯¼å…¥å¤±è´¥ï¼ŒRAGåŠŸèƒ½å°†ä¸å¯ç”¨")
                self.rag_client = None
            
        except Exception as e:
            logger.warning(f"âš ï¸ RAGå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.info("   RAGåŠŸèƒ½å°†ä¸å¯ç”¨ï¼Œä½†ä¸å½±å“å…¶ä»–åŠŸèƒ½")
            self.rag_client = None
    
    def _load_rag_documents_from_data(self):
        """ä»dataç›®å½•è‡ªåŠ¨åŠ è½½RAGæ–‡æ¡£"""
        if not self.rag_client:
            return
        
        try:
            from pathlib import Path
            import json
            import glob
            
            data_dir = Path(__file__).parent / "data"
            if not data_dir.exists():
                logger.warning(f"dataç›®å½•ä¸å­˜åœ¨: {data_dir}")
                return
            
            knowledge_id = "travel_kb_001"
            documents = []
            doc_count = 0
            
            # 1. åŠ è½½rag_corpus/text_documentsç›®å½•ä¸‹çš„æ‰€æœ‰txtæ–‡ä»¶
            text_docs_dir = data_dir / "rag_corpus" / "text_documents"
            if text_docs_dir.exists():
                txt_files = list(text_docs_dir.glob("*.txt"))
                logger.info(f"ğŸ“š å‘ç° {len(txt_files)} ä¸ªæ–‡æœ¬æ–‡æ¡£")
                
                for txt_file in txt_files:
                    try:
                        with open(txt_file, 'r', encoding='utf-8') as f:
                            text = f.read()
                        
                        if text.strip():
                            # æ–‡æœ¬åˆ†å—
                            from .rag.vector_store import text_to_chunk
                            chunks = text_to_chunk(text, chunk_size=500, chunk_overlap=50)
                            
                            for idx, chunk in enumerate(chunks):
                                documents.append({
                                    'text': chunk,
                                    'knowledge_id': knowledge_id,
                                    'document_id': f"txt_{doc_count}",
                                    'paragraph_id': f"para_{doc_count}_{idx}",
                                    'meta': {
                                        'file_name': txt_file.name,
                                        'source': 'rag_corpus',
                                        'chunk_index': idx
                                    }
                                })
                            
                            doc_count += 1
                            logger.debug(f"  âœ… å·²åŠ è½½: {txt_file.name} ({len(chunks)}ä¸ªæ®µè½)")
                    except Exception as e:
                        logger.warning(f"  âš ï¸ åŠ è½½æ–‡ä»¶å¤±è´¥ {txt_file.name}: {e}")
            
            # 2. åŠ è½½attractionsç›®å½•ä¸‹çš„JSONæ–‡ä»¶
            attractions_dir = data_dir / "attractions"
            if attractions_dir.exists():
                json_files = list(attractions_dir.glob("*.json"))
                logger.info(f"ğŸ›ï¸ å‘ç° {len(json_files)} ä¸ªæ™¯ç‚¹JSONæ–‡ä»¶")
                
                for json_file in json_files:
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        # æå–æ™¯ç‚¹ä¿¡æ¯æ–‡æœ¬
                        text_parts = []
                        if isinstance(data, dict):
                            # æå–æ‰€æœ‰æœ‰ç”¨çš„å­—æ®µ
                            if 'attraction_name' in data:
                                text_parts.append(f"æ™¯ç‚¹åç§°ï¼š{data['attraction_name']}")
                            elif 'name' in data:
                                text_parts.append(f"æ™¯ç‚¹åç§°ï¼š{data['name']}")
                            elif 'title' in data:
                                text_parts.append(f"æ™¯ç‚¹åç§°ï¼š{data['title']}")
                            
                            if 'address' in data:
                                text_parts.append(f"åœ°å€ï¼š{data['address']}")
                            
                            if 'intro' in data:
                                text_parts.append(f"ç®€ä»‹ï¼š{data['intro']}")
                            
                            if 'description' in data:
                                text_parts.append(f"è¯¦ç»†æè¿°ï¼š{data['description']}")
                            
                            # æå–äº¤é€šæŒ‡å—
                            if 'transportation_guide' in data:
                                text_parts.append(f"äº¤é€šæŒ‡å—ï¼š{data['transportation_guide']}")
                            elif 'transportation' in data:
                                text_parts.append(f"äº¤é€šæŒ‡å—ï¼š{data['transportation']}")
                            
                            # æå–æœ€ä½³å­£èŠ‚
                            if 'best_season' in data:
                                text_parts.append(f"æœ€ä½³å­£èŠ‚ï¼š{data['best_season']}")
                            
                            # æå–å¼€æ”¾æ—¶é—´
                            if 'opening_hours' in data:
                                text_parts.append(f"å¼€æ”¾æ—¶é—´ï¼š{data['opening_hours']}")
                            
                            # æå–é—¨ç¥¨ä¿¡æ¯
                            if 'ticket_info' in data:
                                text_parts.append(f"é—¨ç¥¨ä¿¡æ¯ï¼š{data['ticket_info']}")
                            
                            # æå–è¯„åˆ†
                            if 'rating' in data:
                                text_parts.append(f"è¯„åˆ†ï¼š{data['rating']}")
                            
                            # æå–æ ‡ç­¾
                            if 'tags' in data:
                                tags = data['tags']
                                if isinstance(tags, list):
                                    # è¿‡æ»¤æ‰æ— æ•ˆæ ‡ç­¾
                                    valid_tags = [t for t in tags if t and isinstance(t, str) and len(t.strip()) > 0 and t != '0']
                                    if valid_tags:
                                        text_parts.append(f"æ ‡ç­¾ï¼š{', '.join(valid_tags[:5])}")
                                elif isinstance(tags, str):
                                    text_parts.append(f"æ ‡ç­¾ï¼š{tags}")
                        
                        text = '\n'.join(text_parts)
                        if text.strip():
                            from .rag.vector_store import text_to_chunk
                            chunks = text_to_chunk(text, chunk_size=500, chunk_overlap=50)
                            
                            for idx, chunk in enumerate(chunks):
                                documents.append({
                                    'text': chunk,
                                    'knowledge_id': knowledge_id,
                                    'document_id': f"attraction_{doc_count}",
                                    'paragraph_id': f"para_{doc_count}_{idx}",
                                    'meta': {
                                        'file_name': json_file.name,
                                        'source': 'attractions',
                                        'chunk_index': idx
                                    }
                                })
                            
                            doc_count += 1
                            logger.debug(f"  âœ… å·²åŠ è½½: {json_file.name} ({len(chunks)}ä¸ªæ®µè½)")
                    except Exception as e:
                        logger.warning(f"  âš ï¸ åŠ è½½JSONæ–‡ä»¶å¤±è´¥ {json_file.name}: {e}")
            
            # 3. åŠ è½½reviewsç›®å½•ä¸‹çš„è¯„è®ºæ•°æ®
            reviews_dir = data_dir / "reviews"
            if reviews_dir.exists():
                review_files = list(reviews_dir.glob("*.json"))
                logger.info(f"ğŸ’¬ å‘ç° {len(review_files)} ä¸ªè¯„è®ºJSONæ–‡ä»¶")
                
                for review_file in review_files[:10]:  # é™åˆ¶åŠ è½½å‰10ä¸ªï¼Œé¿å…è¿‡å¤š
                    try:
                        with open(review_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        # æå–è¯„è®ºæ–‡æœ¬
                        text_parts = []
                        if isinstance(data, list):
                            for review in data[:10]:  # æ¯ä¸ªæ–‡ä»¶å–å‰10æ¡è¯„è®º
                                if isinstance(review, dict):
                                    content = review.get('content', '')
                                    if content and len(content.strip()) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„è¯„è®º
                                        # æå–æ™¯ç‚¹åç§°
                                        attraction = review.get('attraction_name', '')
                                        if attraction:
                                            text_parts.append(f"{attraction}çš„è¯„è®ºï¼š{content[:200]}")  # é™åˆ¶é•¿åº¦
                                        else:
                                            text_parts.append(f"è¯„è®ºï¼š{content[:200]}")
                                    
                                    rating = review.get('rating')
                                    if rating and rating > 0:
                                        text_parts.append(f"è¯„åˆ†ï¼š{rating}åˆ†")
                        elif isinstance(data, dict):
                            if 'reviews' in data:
                                for review in data['reviews'][:10]:
                                    content = review.get('content', '')
                                    if content and len(content.strip()) > 10:
                                        text_parts.append(f"è¯„è®ºï¼š{content[:200]}")
                        
                        text = '\n'.join(text_parts)
                        if text.strip():
                            from .rag.vector_store import text_to_chunk
                            chunks = text_to_chunk(text, chunk_size=500, chunk_overlap=50)
                            
                            for idx, chunk in enumerate(chunks):
                                documents.append({
                                    'text': chunk,
                                    'knowledge_id': knowledge_id,
                                    'document_id': f"review_{doc_count}",
                                    'paragraph_id': f"para_{doc_count}_{idx}",
                                    'meta': {
                                        'file_name': review_file.name,
                                        'source': 'reviews',
                                        'chunk_index': idx
                                    }
                                })
                            
                            doc_count += 1
                            logger.debug(f"  âœ… å·²åŠ è½½: {review_file.name} ({len(chunks)}ä¸ªæ®µè½)")
                    except Exception as e:
                        logger.warning(f"  âš ï¸ åŠ è½½è¯„è®ºæ–‡ä»¶å¤±è´¥ {review_file.name}: {e}")
            
            # æ‰¹é‡æ·»åŠ åˆ°RAGçŸ¥è¯†åº“
            if documents:
                if hasattr(self.rag_client, 'add_documents'):
                    self.rag_client.add_documents(documents)
                    logger.info(f"âœ… æˆåŠŸä»dataç›®å½•åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£æ®µè½åˆ°RAGçŸ¥è¯†åº“ï¼ˆæ¥è‡ª {doc_count} ä¸ªæ–‡ä»¶ï¼‰")
                elif hasattr(self.rag_client, 'batch_save'):
                    # å¦‚æœRAGå®¢æˆ·ç«¯æ”¯æŒbatch_save
                    self.rag_client.batch_save(documents)
                    logger.info(f"âœ… æˆåŠŸä»dataç›®å½•åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£æ®µè½åˆ°RAGçŸ¥è¯†åº“ï¼ˆæ¥è‡ª {doc_count} ä¸ªæ–‡ä»¶ï¼‰")
                else:
                    logger.warning("RAGå®¢æˆ·ç«¯ä¸æ”¯æŒæ‰¹é‡æ·»åŠ æ–‡æ¡£")
            else:
                logger.info("â„¹ï¸ dataç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°å¯åŠ è½½çš„æ–‡æ¡£")
        
        except Exception as e:
            logger.error(f"ä»dataç›®å½•åŠ è½½RAGæ–‡æ¡£å¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())
    
    def add_documents_from_files(self, file_paths: List[str], knowledge_id: str = "travel_kb_001"):
        """
        ä»æ–‡ä»¶åŠ è½½æ–‡æ¡£åˆ°RAGçŸ¥è¯†åº“
        
        :param file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆæ”¯æŒ.txt, .md, .docxç­‰ï¼‰
        :param knowledge_id: çŸ¥è¯†åº“ID
        """
        if not self.rag_client:
            logger.warning("RAGå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•åŠ è½½æ–‡æ¡£")
            return
        
        try:
            from pathlib import Path
            import docx
            
            documents = []
            doc_id = 0
            
            for file_path in file_paths:
                path = Path(file_path)
                if not path.exists():
                    logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    continue
                
                # è¯»å–æ–‡ä»¶å†…å®¹
                text = ""
                if path.suffix == '.txt' or path.suffix == '.md':
                    with open(path, 'r', encoding='utf-8') as f:
                        text = f.read()
                elif path.suffix == '.docx':
                    try:
                        doc = docx.Document(path)
                        text = '\n'.join([para.text for para in doc.paragraphs])
                    except Exception as e:
                        logger.warning(f"è¯»å–docxæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                        continue
                else:
                    logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path.suffix}")
                    continue
                
                # æ–‡æœ¬åˆ†å—
                from .rag.vector_store import text_to_chunk
                chunks = text_to_chunk(text, chunk_size=500, chunk_overlap=50)
                
                # æ·»åŠ åˆ°æ–‡æ¡£åˆ—è¡¨
                for idx, chunk in enumerate(chunks):
                    documents.append({
                        'text': chunk,
                        'knowledge_id': knowledge_id,
                        'document_id': f"doc_{doc_id}",
                        'paragraph_id': f"para_{doc_id}_{idx}",
                        'meta': {
                            'file_path': str(file_path),
                            'file_name': path.name,
                            'chunk_index': idx
                        }
                    })
                
                doc_id += 1
                logger.info(f"âœ… å·²åŠ è½½æ–‡ä»¶: {path.name} ({len(chunks)}ä¸ªæ®µè½)")
            
            # æ‰¹é‡æ·»åŠ åˆ°RAG
            if documents:
                if hasattr(self.rag_client, 'add_documents'):
                    self.rag_client.add_documents(documents)
                    logger.info(f"âœ… æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£æ®µè½åˆ°RAGçŸ¥è¯†åº“")
                else:
                    logger.warning("RAGå®¢æˆ·ç«¯ä¸æ”¯æŒæ‰¹é‡æ·»åŠ æ–‡æ¡£")
        
        except Exception as e:
            logger.error(f"ä»æ–‡ä»¶åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
    
    def _load_qunar_places(self) -> pd.DataFrame:
        """åŠ è½½å»å“ªå„¿æ™¯ç‚¹æ•°æ®"""
        try:
            excel_path = Path(__file__).parent / "data" / "qunar_place.xlsx"
            if excel_path.exists():
                df = pd.read_excel(excel_path)
                logger.info(f"âœ… æˆåŠŸåŠ è½½å»å“ªå„¿æ™¯ç‚¹æ•°æ®: {len(df)}æ¡è®°å½•")
                return df
            else:
                logger.warning(f"âš ï¸ å»å“ªå„¿æ™¯ç‚¹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
                return pd.DataFrame()
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å»å“ªå„¿æ™¯ç‚¹æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _search_qunar_places(self, keyword: str, limit: int = 10) -> List[POIInfo]:
        """ä»Excelæ•°æ®ä¸­æœç´¢æ™¯ç‚¹"""
        if self.qunar_places.empty:
            return []
        
        try:
            # åœ¨nameå’Œintroåˆ—ä¸­æœç´¢å…³é”®è¯
            mask = (
                self.qunar_places['name'].str.contains(keyword, case=False, na=False) |
                self.qunar_places['intro'].str.contains(keyword, case=False, na=False)
            )
            results = self.qunar_places[mask].head(limit)
            
            pois = []
            for _, row in results.iterrows():
                # è§£ædistrictsè·å–åŒºåŸŸä¿¡æ¯
                districts = str(row.get('districts', ''))
                address = districts.replace('Â·', '') if districts else ''
                
                # è§£æpointè·å–åæ ‡
                point = str(row.get('point', ''))
                
                poi = POIInfo(
                    name=str(row.get('name', '')),
                    address=address,
                    rating=float(row.get('score', 0) or 0),
                    business_hours="",
                    price=f"{row.get('price', 0)}å…ƒ" if row.get('price', 0) else "å…è´¹",
                    distance="",
                    category=str(row.get('star', '')),
                    reviews=[]
                )
                pois.append(poi)
            
            logger.info(f"ä»Excelæ•°æ®ä¸­æœç´¢åˆ°{len(pois)}ä¸ªæ™¯ç‚¹: {keyword}")
            return pois
        except Exception as e:
            logger.error(f"æœç´¢Excelæ•°æ®å¤±è´¥: {e}")
            return []
    
    def process_user_request(self, user_input: str, user_id: str = "default", show_thoughts: bool = True, return_thoughts: bool = False) -> Any:
        """
        å¤„ç†ç”¨æˆ·è¯·æ±‚çš„ä¸»å…¥å£ - åŸºäºæ€è€ƒé“¾çš„æ™ºèƒ½Agentç³»ç»Ÿ
        
        æµç¨‹ï¼š
        1. æ·±åº¦ç†è§£ç”¨æˆ·éœ€æ±‚ï¼Œç”Ÿæˆæ€è€ƒé“¾ï¼ˆThoughtsï¼‰
        2. ä»æ€è€ƒé“¾ä¸­æå–å…³é”®è¯å’Œæ‰€éœ€API
        3. æ ¹æ®å…³é”®è¯æ™ºèƒ½è°ƒç”¨ç›¸åº”çš„API
        4. æ”¶é›†å¹¶æ•´ç†å®æ—¶æ•°æ®
        5. åŸºäºæ•°æ®ç”Ÿæˆæœ€ç»ˆå†³ç­–
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            user_id: ç”¨æˆ·ID
            show_thoughts: æ˜¯å¦å±•ç¤ºæ€è€ƒè¿‡ç¨‹ç»™ç”¨æˆ·ï¼ˆæ§åˆ¶å°è¾“å‡ºï¼‰
            return_thoughts: æ˜¯å¦åœ¨è¿”å›ç»“æœä¸­åŒ…å«æ€è€ƒè¿‡ç¨‹ï¼ˆä¾›APIä½¿ç”¨ï¼‰
            
        Returns:
            å¦‚æœreturn_thoughts=Trueï¼Œè¿”å›å­—å…¸ {"response": str, "thoughts": list}
            å¦åˆ™è¿”å›å­—ç¬¦ä¸²ï¼ˆå›å¤å†…å®¹ï¼‰
        """
        logger.info(f"ğŸ‘¤ ç”¨æˆ· {user_id} è¾“å…¥: {user_input}")
        
        # è·å–æˆ–åˆ›å»ºç”¨æˆ·ä¸Šä¸‹æ–‡
        if user_id not in self.user_contexts:
            self.user_contexts[user_id] = UserContext(
                user_id=user_id,
                conversation_history=[],
                travel_preferences=TravelPreference()
            )
        
        context = self.user_contexts[user_id]
        
        # è®°å½•ç”¨æˆ·è¾“å…¥
        context.conversation_history.append({
            "role": "user",
            "content": user_input,
            "timestamp": datetime.now().isoformat()
        })
        
        print("\n" + "="*80)
        print("ğŸ§  çŸ¥å°æ—… - æ™ºèƒ½æ—…æ¸¸è§„åˆ’åŠ©æ‰‹")
        print("="*80)
        
        # ============ Step 0: è§£ææ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ ============
        tags = self._parse_tags_from_input(user_input)
        if any(tags.values()):
            print(f"\nğŸ·ï¸  æ£€æµ‹åˆ°æ ‡ç­¾ï¼šåŸºç¡€æ ‡ç­¾{len(tags['åŸºç¡€æ ‡ç­¾'])}ä¸ªï¼Œåå¥½æ ‡ç­¾{len(tags['åå¥½æ ‡ç­¾'])}ä¸ªï¼Œç‰¹æ®Šæ ‡ç­¾{len(tags['ç‰¹æ®Šæ ‡ç­¾'])}ä¸ª")
        
        # ============ Step 1: æ·±åº¦ç†è§£éœ€æ±‚å¹¶ç”Ÿæˆæ€è€ƒé“¾ ============
        print("\nğŸ“‹ Step 1: æ·±åº¦ç†è§£æ‚¨çš„éœ€æ±‚...")
        thoughts = self._generate_thought_chain(user_input, context)
        
        if show_thoughts:
            self._display_thoughts(thoughts)
        
        # ============ Step 2: ä»æ€è€ƒé“¾ä¸­æå–å…³é”®ä¿¡æ¯å¹¶è¿›è¡Œåˆ†è¯ ============
        print("\nğŸ” Step 2: æå–å…³é”®ä¿¡æ¯ã€åˆ†è¯å¹¶è§„åˆ’ç­–ç•¥...")
        extracted_info = self._extract_info_from_thoughts(thoughts, user_input)
        # ä¿å­˜åˆ†è¯ç»“æœåˆ°extracted_infoä¸­
        if thoughts:
            extracted_info['tokenized_data'] = self._tokenize_thoughts(thoughts)
        # ä¿å­˜æ ‡ç­¾ä¿¡æ¯
        extracted_info['tags'] = tags
        # ç”Ÿæˆç”¨æˆ·ç”»åƒ
        user_profile = self._generate_user_profile(extracted_info, tags)
        extracted_info['user_profile'] = user_profile
        self._display_extracted_info(extracted_info)
        
        # å¦‚æœreturn_thoughts=Trueï¼Œåœ¨step2åè¿”å›æ€è€ƒç»“æœï¼ˆä»…ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ï¼‰
        # é€šè¿‡æ£€æŸ¥contextä¸­æ˜¯å¦å·²æœ‰æ€è€ƒç»“æœæ¥åˆ¤æ–­æ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡è°ƒç”¨
        if return_thoughts and not hasattr(context, '_thinking_sent'):
            simplified_thoughts = []
            for t in thoughts[:2]:  # åªè¿”å›å‰2æ­¥çš„æ€è€ƒè¿‡ç¨‹
                simplified_thoughts.append({
                    "step": t.step,
                    "thought": t.thought,
                    "keywords": t.keywords[:15],  # è¿”å›æ›´å¤šå…³é”®è¯ç”¨äºå±•ç¤º
                    "reasoning": t.reasoning,
                    "icon": self._get_thought_icon(t.step)
                })
            
            # æ ‡è®°å·²å‘é€æ€è€ƒç»“æœ
            context._thinking_sent = True
            
            # è¿”å›step1ã€2çš„æ€è€ƒç»“æœ
            return {
                "response": "æ­£åœ¨åˆ†æä½ çš„éœ€æ±‚ï¼Œè¯·ç¨å€™...",  # æç¤ºä¿¡æ¯
                "thoughts": simplified_thoughts,
                "extracted_info": {
                    "travel_days": extracted_info.get('travel_days', 1),
                    "locations": extracted_info.get('locations', []),
                    "enhanced_locations": extracted_info.get('enhanced_locations', []),  # åŒ…å«å®Œæ•´çš„æ™¯ç‚¹ä¿¡æ¯
                    "keywords": extracted_info.get('keywords', []),
                    "activity_types": extracted_info.get('activity_types', []),
                    "budget_info": extracted_info.get('budget_info', {}),
                    "companions": self._format_companions(extracted_info.get('companions', {})) if extracted_info.get('companions') else None,
                    "emotional_context": self._format_emotional_context(extracted_info.get('emotional_context', {})) if extracted_info.get('emotional_context') else None,
                    "preferences": extracted_info.get('preferences', {}),
                    "user_intent_summary": extracted_info.get('user_intent_summary', ''),
                    "tags": extracted_info.get('tags', {})  # åŒ…å«æ ‡ç­¾ä¿¡æ¯
                },
                "status": "thinking"  # æ ‡è¯†è¿™æ˜¯æ€è€ƒé˜¶æ®µ
            }
        
        # ============ Step 3: æ™ºèƒ½APIè°ƒç”¨å†³ç­– ============
        print("\nğŸ¤– Step 3: å†³å®šéœ€è¦è°ƒç”¨çš„APIæœåŠ¡...")
        api_plan = self._plan_api_calls(extracted_info, thoughts)
        self._display_api_plan(api_plan)
        
        # ============ Step 4: æ‰§è¡ŒAPIè°ƒç”¨å¹¶æ”¶é›†æ•°æ®ï¼ˆåŒ…æ‹¬MCPå’ŒRAGï¼‰ ============
        print("\nğŸ“¡ Step 4: è°ƒç”¨MCPå’ŒRAGæœåŠ¡æ”¶é›†å®æ—¶æ•°æ®å’ŒçŸ¥è¯†...")
        real_time_data = self._execute_api_calls(api_plan, extracted_info, context, thoughts)
        
        # ============ Step 5: ç»¼åˆåˆ†æå¹¶ç”Ÿæˆæœ€ç»ˆå†³ç­– ============
        print("\nğŸ’¡ Step 5: ç»¼åˆåˆ†æï¼Œç”Ÿæˆæœ€ä¼˜æ—…æ¸¸æ”»ç•¥...")
        final_response = self._generate_final_decision(
            user_input, thoughts, extracted_info, real_time_data, context
        )
        
        # è®°å½•Agentå›å¤
        context.conversation_history.append({
            "role": "assistant",
            "content": final_response,
            "thoughts": [{"step": t.step, "thought": t.thought, "keywords": t.keywords} for t in thoughts],
            "timestamp": datetime.now().isoformat()
        })
        
        # è®°å¿†æ²‰æ·€ï¼šè®°å½•ç”¨æˆ·åå¥½ï¼ˆå¦‚æœå‡ºç°3æ¬¡ä»¥ä¸Šï¼‰
        self._update_user_memory(context, extracted_info, tags)
        
        print("\n" + "="*80)
        print("âœ… è§„åˆ’å®Œæˆï¼")
        print("="*80 + "\n")
        
        # æ ¹æ®å‚æ•°å†³å®šè¿”å›æ ¼å¼
        if return_thoughts:
            # è¿”å›å®Œæ•´ä¿¡æ¯ï¼ŒåŒ…å«æ€è€ƒè¿‡ç¨‹ï¼ˆä¾›APIä½¿ç”¨ï¼‰
            simplified_thoughts = []
            for t in thoughts:
                simplified_thoughts.append({
                    "step": t.step,
                    "thought": t.thought,
                    "keywords": t.keywords[:5],  # åªè¿”å›å‰5ä¸ªå…³é”®è¯
                    "reasoning": t.reasoning,
                    "icon": self._get_thought_icon(t.step)
                })
            
            return {
                "response": final_response,
                "thoughts": [],  # æœ€ç»ˆå›å¤æ—¶ä¸è¿”å›æ€è€ƒè¿‡ç¨‹
                "extracted_info": {
                    "travel_days": extracted_info.get('travel_days', 1),
                    "locations": extracted_info.get('locations', []),
                    "enhanced_locations": extracted_info.get('enhanced_locations', []),  # åŒ…å«å®Œæ•´çš„æ™¯ç‚¹ä¿¡æ¯
                    "keywords": extracted_info.get('keywords', []),
                    "activity_types": extracted_info.get('activity_types', []),
                    "budget_info": extracted_info.get('budget_info', {}),
                    "companions": self._format_companions(extracted_info.get('companions', {})) if extracted_info.get('companions') else None,
                    "emotional_context": self._format_emotional_context(extracted_info.get('emotional_context', {})) if extracted_info.get('emotional_context') else None,
                    "preferences": extracted_info.get('preferences', {}),
                    "user_intent_summary": extracted_info.get('user_intent_summary', ''),
                    "tags": extracted_info.get('tags', {}),  # åŒ…å«æ ‡ç­¾ä¿¡æ¯
                    "user_profile": extracted_info.get('user_profile', {})  # åŒ…å«ç”¨æˆ·ç”»åƒ
                },
                "status": "completed"  # æ ‡è¯†å·²å®Œæˆ
            }
        else:
            # ä»…è¿”å›å›å¤æ–‡æœ¬
            return final_response
    
    def _get_thought_icon(self, step: int) -> str:
        """æ ¹æ®æ­¥éª¤è·å–åˆé€‚çš„å›¾æ ‡"""
        icons = ["ğŸ¤”", "ğŸ’¡", "ğŸŒ¤ï¸", "ğŸ—ºï¸", "ğŸš¦", "ğŸ“Š", "âœ¨"]
        return icons[min(step - 1, len(icons) - 1)]
    
    # ==================== æ€è€ƒé“¾ç³»ç»Ÿæ ¸å¿ƒæ–¹æ³• ====================
    
    def _generate_thought_chain(self, user_input: str, context: UserContext) -> List[ThoughtProcess]:
        """ç”Ÿæˆæ€è€ƒé“¾ - é€šè¿‡Agentå¼•å¯¼ç”Ÿæˆè¯¦ç»†çš„æ€è€ƒè¿‡ç¨‹"""
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸Šæµ·æ—…æ¸¸è§„åˆ’ä¸“å®¶ã€‚è¯·æ·±å…¥åˆ†æç”¨æˆ·çš„éœ€æ±‚ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„ã€ç»“æ„åŒ–çš„æ€è€ƒè¿‡ç¨‹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. **æ·±åº¦ç†è§£ç”¨æˆ·éœ€æ±‚**ï¼šåˆ†æç”¨æˆ·çš„æ ¸å¿ƒæ„å›¾ã€æƒ…æ„Ÿéœ€æ±‚ã€åŒä¼´å…³ç³»ã€æ—¶é—´å®‰æ’ã€é¢„ç®—ç­‰
2. **è¯†åˆ«å…³é”®ä¿¡æ¯**ï¼šæå–åœ°ç‚¹ã€æ—¶é—´ã€æ´»åŠ¨ç±»å‹ã€ç‰¹æ®Šåå¥½ç­‰å…³é”®è¦ç´ 
3. **è§„åˆ’ä¿¡æ¯æ”¶é›†ç­–ç•¥**ï¼šæ˜ç¡®éœ€è¦å“ªäº›å®æ—¶æ•°æ®ï¼ˆå¤©æ°”ã€POIã€äº¤é€šã€äººæµç­‰ï¼‰æ¥æ”¯æŒå†³ç­–
4. **æ€è€ƒæ¨ç†è¿‡ç¨‹**ï¼šè¯¦ç»†è¯´æ˜æ¯ä¸€æ­¥çš„æ¨ç†é€»è¾‘å’ŒåŸå› 

è¯·ä»¥JSONæ ¼å¼è¿”å›ä½ çš„æ€è€ƒè¿‡ç¨‹ï¼Œè¦æ±‚ï¼š
- æ€è€ƒæ­¥éª¤è¦è¯¦ç»†ã€å…·ä½“ï¼Œä½“ç°ä½ çš„æ¨ç†è¿‡ç¨‹
- å…³é”®è¯è¦å…¨é¢ï¼ŒåŒ…æ‹¬åœ°ç‚¹ã€æ—¶é—´ã€æ´»åŠ¨ã€æƒ…æ„Ÿç­‰å„ä¸ªæ–¹é¢
- æ˜ç¡®è¯´æ˜éœ€è¦å“ªäº›APIæœåŠ¡æ¥è·å–æ•°æ®
- æ¯ä¸ªæ­¥éª¤éƒ½è¦æœ‰æ¸…æ™°çš„æ¨ç†åŸå› 

æ ¼å¼ç¤ºä¾‹ï¼š
{
  "thoughts": [
    {
      "step": 1,
      "thought": "é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£ç”¨æˆ·çš„æ ¸å¿ƒéœ€æ±‚ã€‚ç”¨æˆ·æƒ³è¦è§„åˆ’3å¤©çš„ä¸Šæµ·æ—…æ¸¸ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ—¥è¡Œç¨‹è§„åˆ’éœ€æ±‚ã€‚",
      "keywords": ["3å¤©", "ä¸Šæµ·", "æ—…æ¸¸", "è¡Œç¨‹è§„åˆ’"],
      "api_needs": ["å¤©æ°”", "æ™¯ç‚¹", "POI"],
      "reasoning": "å¤šæ—¥è¡Œç¨‹éœ€è¦æŸ¥è¯¢æœªæ¥3å¤©çš„å¤©æ°”æƒ…å†µï¼Œä»¥ä¾¿åˆç†å®‰æ’å®¤å†…å¤–æ´»åŠ¨ï¼›åŒæ—¶éœ€è¦æœç´¢é€‚åˆ3å¤©æ¸¸è§ˆçš„æ™¯ç‚¹å’ŒPOIä¿¡æ¯"
    },
    {
      "step": 2,
      "thought": "ç”¨æˆ·æåˆ°äº†å…·ä½“åœ°ç‚¹ï¼šå¤–æ»©ã€è±«å›­ã€‚è¿™äº›æ˜¯ä¸Šæµ·çš„çƒ­é—¨æ™¯ç‚¹ï¼Œéœ€è¦æŸ¥è¯¢è¿™äº›åœ°ç‚¹çš„è¯¦ç»†ä¿¡æ¯ã€å¼€æ”¾æ—¶é—´ã€å‘¨è¾¹æ¨èç­‰ã€‚",
      "keywords": ["å¤–æ»©", "è±«å›­", "æ™¯ç‚¹", "å¼€æ”¾æ—¶é—´"],
      "api_needs": ["POI", "å¯¼èˆª"],
      "reasoning": "éœ€è¦è°ƒç”¨POIæœç´¢APIè·å–è¿™äº›æ™¯ç‚¹çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¹¶å¯èƒ½éœ€è¦è§„åˆ’è¿™äº›æ™¯ç‚¹ä¹‹é—´çš„è·¯çº¿"
    }
  ]
}

è¯·ç¡®ä¿æ€è€ƒè¿‡ç¨‹è¯¦ç»†ã€å…¨é¢ï¼Œèƒ½å¤Ÿä¸ºåç»­çš„ä¿¡æ¯æ”¶é›†å’Œæ–¹æ¡ˆç”Ÿæˆæä¾›å……åˆ†çš„åŸºç¡€ã€‚"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"è¯·è¯¦ç»†åˆ†æè¿™ä¸ªæ—…æ¸¸éœ€æ±‚ï¼Œå¹¶ç»™å‡ºå®Œæ•´çš„æ€è€ƒè¿‡ç¨‹ï¼š\n\n{user_input}"}
        ]
        
        try:
            response = self.doubao_agent.generate_response(messages)
            
            # å°è¯•è§£æJSONå“åº”
            æ€è€ƒæ•°æ® = self._parse_thought_response(response)
            
            # è½¬æ¢ä¸ºThoughtProcesså¯¹è±¡
            thoughts = []
            for idx, thought_data in enumerate(æ€è€ƒæ•°æ®.get("thoughts", []), 1):
                thought = ThoughtProcess(
                    step=idx,
                    thought=thought_data.get("thought", ""),
                    keywords=thought_data.get("keywords", []),
                    mcp_services=self._map_api_needs_to_services(thought_data.get("api_needs", [])),
                    reasoning=thought_data.get("reasoning", ""),
                    timestamp=datetime.now().isoformat()
                )
                thoughts.append(thought)
            
            # å¦‚æœAIæ²¡æœ‰è¿”å›æœ‰æ•ˆçš„æ€è€ƒé“¾ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            if not thoughts:
                logger.warning("Agentæœªè¿”å›æœ‰æ•ˆæ€è€ƒé“¾ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
                thoughts = self._fallback_thought_generation(user_input, context)
            
            return thoughts
            
        except Exception as e:
            logger.error(f"æ€è€ƒé“¾ç”Ÿæˆå¤±è´¥: {e}")
            # ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            return self._fallback_thought_generation(user_input, context)
    
    def _parse_thought_response(self, response: str) -> Dict:
        """è§£æAIçš„æ€è€ƒå“åº”"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                return json.loads(json_match.group())
            else:
                # å¦‚æœæ²¡æœ‰JSONï¼Œè¿”å›ç©ºå­—å…¸
                return {"thoughts": []}
        except:
            return {"thoughts": []}
    
    def _map_api_needs_to_services(self, api_needs: List[str]) -> List[MCPServiceType]:
        """å°†APIéœ€æ±‚æ˜ å°„åˆ°æœåŠ¡ç±»å‹"""
        service_map = {
            "å¤©æ°”": MCPServiceType.WEATHER,
            "weather": MCPServiceType.WEATHER,
            "æ™¯ç‚¹": MCPServiceType.POI,
            "poi": MCPServiceType.POI,
            "é¤å…": MCPServiceType.POI,
            "ç¾é£Ÿ": MCPServiceType.POI,
            "å¯¼èˆª": MCPServiceType.NAVIGATION,
            "è·¯çº¿": MCPServiceType.NAVIGATION,
            "navigation": MCPServiceType.NAVIGATION,
            "äº¤é€š": MCPServiceType.TRAFFIC,
            "è·¯å†µ": MCPServiceType.TRAFFIC,
            "traffic": MCPServiceType.TRAFFIC,
            "äººæµ": MCPServiceType.CROWD,
            "crowd": MCPServiceType.CROWD
        }
        
        services = []
        for need in api_needs:
            service = service_map.get(need.lower())
            if service and service not in services:
                services.append(service)
        
        return services
    
    def _fallback_thought_generation(self, user_input: str, context: UserContext) -> List[ThoughtProcess]:
        """å¤‡ç”¨æ€è€ƒé“¾ç”Ÿæˆæ–¹æ³• - åŸºäºè§„åˆ™"""
        thoughts = []
        keywords = self._extract_keywords(user_input)
        detected_locations, activity_types = self._analyze_user_intent(user_input)
        travel_days = self._extract_travel_days(user_input)
        
        # Thought 1: ç†è§£éœ€æ±‚
        thoughts.append(ThoughtProcess(
            step=1,
            thought=f"ç”¨æˆ·éœ€è¦è§„åˆ’{travel_days}å¤©çš„ä¸Šæµ·æ—…æ¸¸æ”»ç•¥",
            keywords=keywords + [f"{travel_days}å¤©"] + detected_locations,
            mcp_services=[],
            reasoning="é¦–å…ˆç†è§£ç”¨æˆ·çš„åŸºæœ¬éœ€æ±‚å’Œæ—¶é—´å®‰æ’",
            timestamp=datetime.now().isoformat()
        ))
        
        # Thought 2: åœ°ç‚¹åˆ†æ
        if detected_locations:
            thoughts.append(ThoughtProcess(
                step=2,
                thought=f"ç”¨æˆ·æåˆ°äº†å…·ä½“åœ°ç‚¹ï¼š{', '.join(detected_locations)}",
                keywords=detected_locations,
                mcp_services=[MCPServiceType.POI],
                reasoning="éœ€è¦æœç´¢è¿™äº›åœ°ç‚¹çš„è¯¦ç»†ä¿¡æ¯å’Œå‘¨è¾¹æ™¯ç‚¹",
                timestamp=datetime.now().isoformat()
            ))
        else:
            thoughts.append(ThoughtProcess(
                step=2,
                thought="ç”¨æˆ·æ²¡æœ‰æŒ‡å®šå…·ä½“åœ°ç‚¹ï¼Œéœ€è¦æ¨èä¸Šæµ·ç»å…¸æ™¯ç‚¹",
                keywords=["ä¸Šæµ·", "ç»å…¸æ™¯ç‚¹"],
                mcp_services=[MCPServiceType.POI],
                reasoning="æ¨èé€‚åˆæ¸¸è§ˆæ—¶é•¿çš„ç»å…¸æ™¯ç‚¹ç»„åˆ",
                timestamp=datetime.now().isoformat()
            ))
        
        # Thought 3: å¤©æ°”è€ƒè™‘
        thoughts.append(ThoughtProcess(
            step=3,
            thought=f"éœ€è¦æŸ¥è¯¢æœªæ¥{travel_days}å¤©çš„å¤©æ°”æƒ…å†µ",
            keywords=["å¤©æ°”", "é¢„æŠ¥"],
            mcp_services=[MCPServiceType.WEATHER],
            reasoning="æ ¹æ®å¤©æ°”æƒ…å†µè°ƒæ•´å®¤å†…å¤–æ´»åŠ¨å®‰æ’",
            timestamp=datetime.now().isoformat()
        ))
        
        # Thought 4: äº¤é€šè§„åˆ’
        if len(detected_locations) > 1 or "äº¤é€š" in user_input or "è·¯çº¿" in user_input:
            thoughts.append(ThoughtProcess(
                step=4,
                thought="éœ€è¦è§„åˆ’æ™¯ç‚¹é—´çš„äº¤é€šè·¯çº¿",
                keywords=["å¯¼èˆª", "è·¯çº¿", "äº¤é€š"],
                mcp_services=[MCPServiceType.NAVIGATION, MCPServiceType.TRAFFIC],
                reasoning="æä¾›æœ€ä¼˜äº¤é€šæ–¹æ¡ˆï¼Œè€ƒè™‘è·¯å†µé¿å…æ‹¥å µ",
                timestamp=datetime.now().isoformat()
            ))
        
        return thoughts
    
    def _display_thoughts(self, thoughts: List[ThoughtProcess]):
        """å±•ç¤ºæ€è€ƒè¿‡ç¨‹"""
        print("\nğŸ’­ AIæ€è€ƒè¿‡ç¨‹ï¼š")
        print("-" * 80)
        for thought in thoughts:
            print(f"\n  æ­¥éª¤ {thought.step}: {thought.thought}")
            if thought.keywords:
                print(f"  å…³é”®è¯: {', '.join(thought.keywords)}")
            if thought.mcp_services:
                services = [s.value for s in thought.mcp_services]
                print(f"  éœ€è¦API: {', '.join(services)}")
            print(f"  åŸå› : {thought.reasoning}")
    
    def _tokenize_thoughts(self, thoughts: List[ThoughtProcess]) -> Dict[str, Any]:
        """å¯¹Agentç»™å‡ºçš„æ€è€ƒè¿‡ç¨‹è¿›è¡Œåˆ†è¯ï¼Œæå–å…³é”®ä¿¡æ¯ç”¨äºMCPå’ŒRAGè°ƒç”¨"""
        # åˆå¹¶æ‰€æœ‰æ€è€ƒè¿‡ç¨‹çš„æ–‡æœ¬
        all_thought_text = []
        all_keywords = []
        
        for thought in thoughts:
            # åˆå¹¶æ€è€ƒå†…å®¹ã€å…³é”®è¯å’Œæ¨ç†è¿‡ç¨‹
            thought_text = f"{thought.thought} {thought.reasoning}"
            all_thought_text.append(thought_text)
            all_keywords.extend(thought.keywords)
        
        combined_text = " ".join(all_thought_text)
        
        # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯å’Œå…³é”®è¯æå–
        # æå–å…³é”®è¯ï¼ˆä½¿ç”¨TF-IDFç®—æ³•ï¼‰
        keywords_tfidf = jieba.analyse.extract_tags(combined_text, topK=20, withWeight=False)
        
        # æå–å…³é”®è¯ï¼ˆä½¿ç”¨TextRankç®—æ³•ï¼‰
        keywords_textrank = jieba.analyse.textrank(combined_text, topK=20, withWeight=False)
        
        # åˆå¹¶å…³é”®è¯ï¼Œå»é‡
        all_extracted_keywords = list(set(keywords_tfidf + keywords_textrank + all_keywords))
        
        # åˆ†è¯ç»“æœ
        words = list(jieba.cut(combined_text))
        
        # æå–åœ°ç‚¹ã€æ—¶é—´ã€æ´»åŠ¨ç­‰ç‰¹å®šç±»å‹çš„å…³é”®è¯
        location_keywords = []
        time_keywords = []
        activity_keywords = []
        
        # åœ°ç‚¹ç›¸å…³å…³é”®è¯
        location_patterns = ["ä¸Šæµ·", "å¤–æ»©", "è±«å›­", "ä¸œæ–¹æ˜ç ", "å—äº¬è·¯", "äººæ°‘å¹¿åœº", "ç”°å­åŠ", 
                            "æ–°å¤©åœ°", "åŸéšåº™", "æœ±å®¶è§’", "è¿ªå£«å°¼", "é™†å®¶å˜´", "å¾å®¶æ±‡", "é™å®‰å¯º"]
        # æ—¶é—´ç›¸å…³å…³é”®è¯
        time_patterns = ["å¤©", "æ—¥", "å°æ—¶", "æ—©ä¸Š", "ä¸Šåˆ", "ä¸‹åˆ", "æ™šä¸Š", "å‘¨æœ«", "å·¥ä½œæ—¥"]
        # æ´»åŠ¨ç›¸å…³å…³é”®è¯
        activity_patterns = ["æ—…æ¸¸", "æ¸¸è§ˆ", "å‚è§‚", "ç¾é£Ÿ", "è´­ç‰©", "æ‹ç…§", "ä½“éªŒ", "æ¢ç´¢"]
        
        for keyword in all_extracted_keywords:
            if any(pattern in keyword for pattern in location_patterns):
                location_keywords.append(keyword)
            elif any(pattern in keyword for pattern in time_patterns):
                time_keywords.append(keyword)
            elif any(pattern in keyword for pattern in activity_patterns):
                activity_keywords.append(keyword)
        
        return {
            "words": words,
            "keywords": all_extracted_keywords,
            "location_keywords": location_keywords,
            "time_keywords": time_keywords,
            "activity_keywords": activity_keywords,
            "thought_text": combined_text
        }
    
    def _extract_info_from_thoughts(self, thoughts: List[ThoughtProcess], user_input: str) -> Dict[str, Any]:
        """ä»æ€è€ƒé“¾ä¸­æå–å…³é”®ä¿¡æ¯ - åŒ…æ‹¬äººæ–‡å› ç´ å’Œåˆ†è¯ç»“æœ"""
        # å¯¹æ€è€ƒè¿‡ç¨‹è¿›è¡Œåˆ†è¯
        tokenized_data = self._tokenize_thoughts(thoughts)
        
        # æ”¶é›†æ‰€æœ‰å…³é”®è¯ï¼ˆåŒ…æ‹¬Agentç»™å‡ºçš„å’Œåˆ†è¯æå–çš„ï¼‰
        all_keywords = []
        for thought in thoughts:
            all_keywords.extend(thought.keywords)
        all_keywords.extend(tokenized_data["keywords"])
        all_keywords = list(set(all_keywords))  # å»é‡
        
        # æå–åœ°ç‚¹ï¼ˆä¼˜å…ˆä½¿ç”¨åˆ†è¯ç»“æœä¸­çš„åœ°ç‚¹å…³é”®è¯ï¼‰
        locations = self._extract_locations_from_input(user_input)
        if tokenized_data["location_keywords"]:
            locations.extend(tokenized_data["location_keywords"])
            locations = list(set(locations))  # å»é‡
        
        # æ™ºèƒ½é€‰æ‹©å…³é”®è¯è¿›è¡Œè¾“å…¥æç¤ºAPIè°ƒç”¨
        enhanced_locations = []
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºå…³é”®è¯
        priority_keywords = self._prioritize_keywords_for_inputtips(all_keywords, user_input)
        
        # åªå¯¹å‰5ä¸ªæœ€é‡è¦çš„å…³é”®è¯ä½¿ç”¨è¾“å…¥æç¤ºAPIï¼ˆåˆ†æ‰¹è°ƒç”¨é¿å…QPSè¶…é™ï¼‰
        # è¿‡æ»¤æ‰çº¯æ•°å­—å’Œæ— æ•ˆå…³é”®è¯
        valid_keywords = [kw for kw in priority_keywords[:5] if not kw.isdigit() and len(kw.strip()) > 1]
        
        for i, keyword in enumerate(valid_keywords):
            try:
                # æ¯æ¬¡è°ƒç”¨é—´éš”0.4ç§’ï¼Œç¡®ä¿ä¸è¶…è¿‡QPSé™åˆ¶
                if i > 0:
                    time.sleep(0.4)
                
                # ä½¿ç”¨è¾“å…¥æç¤ºAPIéªŒè¯å’Œå¢å¼ºåœ°ç‚¹ä¿¡æ¯
                tips = self.get_inputtips(keyword, city="ä¸Šæµ·", citylimit=True)
                if tips:
                    # åªä¿ç•™æœ‰æ•ˆçš„åœ°ç‚¹å»ºè®®ï¼ˆè¿‡æ»¤æ‰ä¸ç›¸å…³çš„ç»“æœï¼‰
                    valid_tips = [tip for tip in tips if self._is_valid_location(tip.get('name', ''), keyword)]
                    if valid_tips:
                        # ç¡®ä¿åŒ…å«å®Œæ•´çš„æ™¯ç‚¹ä¿¡æ¯ï¼ˆåç§°ã€åœ°å€ã€åŒºåŸŸç­‰ï¼‰
                        full_suggestions = []
                        for tip in valid_tips[:5]:
                            full_suggestions.append({
                                "name": tip.get('name', ''),  # å®Œæ•´æ™¯ç‚¹åç§°
                                "address": tip.get('address', ''),  # å®Œæ•´åœ°å€
                                "district": tip.get('district', ''),  # åŒºåŸŸ
                                "location": tip.get('location', ''),  # åæ ‡
                                "typecode": tip.get('typecode', ''),  # ç±»å‹ä»£ç 
                                "id": tip.get('id', '')  # ID
                            })
                        
                        enhanced_locations.append({
                            "keyword": keyword,
                            "suggestions": full_suggestions,  # å®Œæ•´çš„æ™¯ç‚¹ä¿¡æ¯
                            "priority": i + 1
                        })
                        logger.info(f"è¾“å…¥æç¤ºAPIæˆåŠŸ: {keyword} -> {len(valid_tips)}ä¸ªæœ‰æ•ˆå»ºè®®")
            except Exception as e:
                logger.warning(f"è¾“å…¥æç¤ºAPIè°ƒç”¨å¤±è´¥ for {keyword}: {e}")
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªå…³é”®è¯ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
        
        # æå–æ´»åŠ¨ç±»å‹
        activity_types = []
        for activity, kws in self.activity_keywords.items():
            if any(kw in user_input for kw in kws):
                activity_types.append(activity)
        
        # æå–æ—¶é—´ä¿¡æ¯
        travel_days = self._extract_travel_days(user_input)
        
        # ========== æ–°å¢ï¼šæå–äººæ–‡å› ç´  ==========
        
        # æå–ç¤¾äº¤å…³ç³»å’ŒåŒä¼´ä¿¡æ¯
        companions = self._extract_companions(user_input)
        
        # æå–æƒ…æ„Ÿéœ€æ±‚å’Œæ°›å›´
        emotional_context = self._extract_emotional_context(user_input)
        
        # æå–é¢„ç®—ä¿¡æ¯
        budget_info = self._extract_budget(user_input)
        
        # æå–ç‰¹æ®Šåå¥½
        preferences = self._extract_preferences(user_input)
        
        # æå–å®Œæ•´çš„ç”¨æˆ·åŸå§‹æ„å›¾ï¼ˆä¿ç•™æ‰€æœ‰ç»†èŠ‚ï¼‰
        user_intent_summary = self._summarize_user_intent(user_input, thoughts)
        
        return {
            "keywords": list(set(all_keywords)),
            "locations": locations,
            "enhanced_locations": enhanced_locations,
            "activity_types": activity_types,
            "travel_days": travel_days,
            "route_info": self._extract_route_from_input(user_input),
            # äººæ–‡å› ç´ 
            "companions": companions,
            "emotional_context": emotional_context,
            "budget_info": budget_info,
            "preferences": preferences,
            "user_intent_summary": user_intent_summary,
            "original_input": user_input  # ä¿ç•™åŸå§‹è¾“å…¥
        }
    
    def _extract_companions(self, user_input: str) -> Dict[str, Any]:
        """æå–åŒä¼´ä¿¡æ¯"""
        companions = {
            "type": None,
            "count": 1,
            "details": []
        }
        
        # æ£€æµ‹åŒä¼´ç±»å‹
        companion_patterns = {
            "å¥³æœ‹å‹": {"type": "romantic_partner", "gender": "female", "relationship": "girlfriend"},
            "ç”·æœ‹å‹": {"type": "romantic_partner", "gender": "male", "relationship": "boyfriend"},
            "è€å©†": {"type": "spouse", "gender": "female", "relationship": "wife"},
            "è€å…¬": {"type": "spouse", "gender": "male", "relationship": "husband"},
            "çˆ±äºº": {"type": "spouse", "relationship": "spouse"},
            "å¥³æœ‹å‹": {"type": "romantic_partner", "relationship": "girlfriend"},
            "çˆ¶æ¯": {"type": "family", "relationship": "parents", "count": 2},
            "çˆ¸å¦ˆ": {"type": "family", "relationship": "parents", "count": 2},
            "å­©å­": {"type": "family", "relationship": "children"},
            "å°å­©": {"type": "family", "relationship": "children"},
            "å®å®": {"type": "family", "relationship": "baby"},
            "å®¶äºº": {"type": "family", "relationship": "family"},
            "æœ‹å‹": {"type": "friends", "relationship": "friends"},
            "é—ºèœœ": {"type": "friends", "relationship": "best_friend", "gender": "female"},
            "å…„å¼Ÿ": {"type": "friends", "relationship": "brother"},
            "åŒäº‹": {"type": "colleagues", "relationship": "colleagues"},
            "å›¢é˜Ÿ": {"type": "team", "relationship": "team"}
        }
        
        for pattern, info in companion_patterns.items():
            if pattern in user_input:
                companions["type"] = info["type"]
                companions["details"].append(info)
                if "count" in info:
                    companions["count"] += info["count"]
                else:
                    companions["count"] += 1
                break
        
        return companions
    
    def _extract_emotional_context(self, user_input: str) -> Dict[str, Any]:
        """æå–æƒ…æ„Ÿéœ€æ±‚å’Œæ°›å›´"""
        emotional_context = {
            "mood": [],
            "atmosphere": [],
            "avoid": [],
            "desire": []
        }
        
        # æƒ…ç»ªå’Œæ°›å›´å…³é”®è¯
        mood_keywords = {
            "æµªæ¼«": "romantic",
            "æ¸©é¦¨": "cozy",
            "è½»æ¾": "relaxed",
            "å®‰é™": "quiet",
            "çƒ­é—¹": "lively",
            "æ–‡è‰º": "artistic",
            "å°èµ„": "petty_bourgeois",
            "é«˜ç«¯": "upscale",
            "å¥¢å": "luxury",
            "æœ´å®": "simple",
            "åœ°é“": "authentic",
            "ç‰¹è‰²": "unique"
        }
        
        for keyword, mood in mood_keywords.items():
            if keyword in user_input:
                emotional_context["mood"].append(mood)
                emotional_context["atmosphere"].append(keyword)
        
        # é¿å¼€çš„å†…å®¹
        avoid_keywords = ["é¿å¼€", "ä¸è¦", "åˆ«å»", "ä¸æƒ³", "è®¨åŒ"]
        for avoid_kw in avoid_keywords:
            if avoid_kw in user_input:
                # æå–é¿å¼€çš„å…·ä½“å†…å®¹
                if "äººå¤š" in user_input or "æ‹¥æŒ¤" in user_input or "çƒ­é—¨" in user_input:
                    emotional_context["avoid"].append("crowded_places")
                if "å•†ä¸š" in user_input:
                    emotional_context["avoid"].append("commercial")
                if "ç½‘çº¢" in user_input:
                    emotional_context["avoid"].append("internet_famous")
        
        # æœŸæœ›ä½“éªŒ
        desire_keywords = {
            "æ„Ÿå—": "experience",
            "ä½“éªŒ": "experience",
            "äº†è§£": "understand",
            "é£åœŸäººæƒ…": "local_culture",
            "å½“åœ°ç”Ÿæ´»": "local_life",
            "å†å²": "history",
            "æ–‡åŒ–": "culture",
            "ç¾é£Ÿ": "cuisine"
        }
        
        for keyword, desire in desire_keywords.items():
            if keyword in user_input:
                emotional_context["desire"].append(desire)
        
        return emotional_context
    
    def _extract_budget(self, user_input: str) -> Dict[str, Any]:
        """æå–é¢„ç®—ä¿¡æ¯"""
        import re
        
        budget_info = {
            "amount": None,
            "level": "medium",
            "constraint": None
        }
        
        # æå–å…·ä½“é‡‘é¢
        amount_patterns = [
            r'(\d+)ä¸‡',  # å¦‚ï¼š2ä¸‡
            r'(\d+)åƒ',  # å¦‚ï¼š5åƒ
            r'(\d+)å…ƒ',  # å¦‚ï¼š20000å…ƒ
            r'é¢„ç®—.*?(\d+)',  # é¢„ç®—xxx
            r'ä¸ä½äº.*?(\d+)',  # ä¸ä½äºxxx
            r'ä¸è¶…è¿‡.*?(\d+)',  # ä¸è¶…è¿‡xxx
        ]
        
        for pattern in amount_patterns:
            match = re.search(pattern, user_input)
            if match:
                amount = int(match.group(1))
                if 'ä¸‡' in pattern:
                    amount *= 10000
                elif 'åƒ' in pattern:
                    amount *= 1000
                budget_info["amount"] = amount
                break
        
        # åˆ¤æ–­é¢„ç®—ç­‰çº§
        if budget_info["amount"]:
            if budget_info["amount"] >= 20000:
                budget_info["level"] = "high"
            elif budget_info["amount"] >= 10000:
                budget_info["level"] = "medium_high"
            elif budget_info["amount"] >= 5000:
                budget_info["level"] = "medium"
            else:
                budget_info["level"] = "low"
        
        # é¢„ç®—çº¦æŸ
        if "ä¸ä½äº" in user_input:
            budget_info["constraint"] = "minimum"
        elif "ä¸è¶…è¿‡" in user_input or "æœ€å¤š" in user_input:
            budget_info["constraint"] = "maximum"
        
        # å…³é”®è¯åˆ¤æ–­
        if "ç»æµ" in user_input or "çœé’±" in user_input or "ä¾¿å®œ" in user_input:
            budget_info["level"] = "low"
        elif "å¥¢å" in user_input or "é«˜ç«¯" in user_input or "ä¸å·®é’±" in user_input:
            budget_info["level"] = "high"
        
        return budget_info
    
    def _extract_preferences(self, user_input: str) -> List[str]:
        """æå–ç‰¹æ®Šåå¥½"""
        preferences = []
        
        preference_keywords = {
            "é£åœŸäººæƒ…": "local_culture",
            "å½“åœ°ç‰¹è‰²": "local_specialty",
            "éçƒ­é—¨": "off_the_beaten_path",
            "å°ä¼—": "niche",
            "ç½‘çº¢": "internet_famous",
            "æ‰“å¡": "photo_spots",
            "ç¾é£Ÿ": "food_focused",
            "è´­ç‰©": "shopping_focused",
            "å†å²": "history_focused",
            "è‡ªç„¶": "nature_focused",
            "è‰ºæœ¯": "art_focused",
            "å¤œç”Ÿæ´»": "nightlife",
            "æ…¢èŠ‚å¥": "slow_paced",
            "æ·±åº¦æ¸¸": "in_depth"
        }
        
        for keyword, preference in preference_keywords.items():
            if keyword in user_input:
                preferences.append(preference)
        
        return preferences
    
    def _summarize_user_intent(self, user_input: str, thoughts: List[ThoughtProcess]) -> str:
        """æ€»ç»“ç”¨æˆ·å®Œæ•´æ„å›¾ï¼Œä¿ç•™æ‰€æœ‰äººæ–‡ç»†èŠ‚"""
        # ä½¿ç”¨AIæ¥æ€»ç»“ï¼Œä¿ç•™äººæ–‡ç»†èŠ‚
        try:
            summary_prompt = f"""è¯·ç”¨ä¸€å¥è¯æ€»ç»“ç”¨æˆ·çš„æ—…æ¸¸éœ€æ±‚ï¼Œè¦ä¿ç•™æ‰€æœ‰äººæ–‡ç»†èŠ‚å’Œæƒ…æ„Ÿå› ç´ ã€‚

ç”¨æˆ·è¾“å…¥ï¼š{user_input}

è¦æ±‚ï¼š
1. ä¿ç•™åŒä¼´ä¿¡æ¯ï¼ˆå¦‚ï¼šå¥³æœ‹å‹ã€çˆ¶æ¯ã€å­©å­ç­‰ï¼‰
2. ä¿ç•™æƒ…æ„Ÿéœ€æ±‚ï¼ˆå¦‚ï¼šæµªæ¼«ã€æ¸©é¦¨ã€é¿å¼€äººç¾¤ç­‰ï¼‰
3. ä¿ç•™é¢„ç®—ä¿¡æ¯
4. ä¿ç•™ç‰¹æ®Šåå¥½
5. ç”¨æ¸©æš–ã€äººæ€§åŒ–çš„è¯­è¨€è¡¨è¾¾

ç¤ºä¾‹ï¼š
è¾“å…¥ï¼š"æˆ‘æƒ³å¸¦å¥³æœ‹å‹å»ä¸Šæµ·ç©3å¤©"
è¾“å‡ºï¼š"æ‚¨è®¡åˆ’å’Œå¥³æœ‹å‹ä¸€èµ·åœ¨ä¸Šæµ·åº¦è¿‡æµªæ¼«çš„3å¤©"

è¯·æ€»ç»“ï¼š"""
            
            messages = [{"role": "user", "content": summary_prompt}]
            summary = self.doubao_agent.generate_response(messages)
            return summary.strip()
        except:
            # å¦‚æœAIå¤±è´¥ï¼Œè¿”å›åŸå§‹è¾“å…¥
            return user_input
    
    def _display_extracted_info(self, info: Dict[str, Any]):
        """å±•ç¤ºæå–çš„ä¿¡æ¯ - åŒ…æ‹¬äººæ–‡å› ç´ """
        print("\nğŸ“Œ æå–çš„å…³é”®ä¿¡æ¯ï¼š")
        print("-" * 80)
        
        # æ˜¾ç¤ºç”¨æˆ·æ„å›¾æ€»ç»“ï¼ˆæœ€é‡è¦ï¼Œæ”¾åœ¨æœ€å‰é¢ï¼‰
        if info.get('user_intent_summary'):
            print(f"  ğŸ’­ éœ€æ±‚ç†è§£: {info['user_intent_summary']}")
            print()
        
        # æ˜¾ç¤ºåŒä¼´ä¿¡æ¯
        if info.get('companions') and info['companions']['type']:
            companion_desc = self._format_companions(info['companions'])
            print(f"  ğŸ‘¥ åŒä¼´ä¿¡æ¯: {companion_desc}")
        
        # æ˜¾ç¤ºæƒ…æ„Ÿéœ€æ±‚å’Œæ°›å›´
        if info.get('emotional_context'):
            emotional_desc = self._format_emotional_context(info['emotional_context'])
            if emotional_desc:
                print(f"  ğŸ’ æƒ…æ„Ÿéœ€æ±‚: {emotional_desc}")
        
        # æ˜¾ç¤ºé¢„ç®—ä¿¡æ¯
        if info.get('budget_info') and info['budget_info']['amount']:
            budget_desc = self._format_budget(info['budget_info'])
            print(f"  ğŸ’° é¢„ç®—ä¿¡æ¯: {budget_desc}")
        
        # æ˜¾ç¤ºç‰¹æ®Šåå¥½
        if info.get('preferences'):
            pref_desc = self._format_preferences(info['preferences'])
            print(f"  â­ ç‰¹æ®Šåå¥½: {pref_desc}")
        
        # åŸºç¡€ä¿¡æ¯
        print(f"\n  ğŸ“… æ—…è¡Œå¤©æ•°: {info['travel_days']}å¤©")
        
        if info['locations']:
            print(f"  ğŸ“ æåˆ°çš„åœ°ç‚¹: {', '.join(info['locations'])}")
        
        if info['enhanced_locations']:
            print(f"  ğŸ” æ™ºèƒ½è¯†åˆ«çš„åœ°ç‚¹:")
            for loc in info['enhanced_locations'][:5]:
                if loc.get('suggestions'):
                    for suggestion in loc['suggestions'][:2]:
                        name = suggestion.get('name', 'æœªçŸ¥')
                        address = suggestion.get('address', suggestion.get('district', ''))
                        display_text = f"{name}"
                        if address:
                            display_text += f"ï¼ˆ{address}ï¼‰"
                        print(f"     â€¢ {display_text}")
                else:
                    print(f"     â€¢ {loc['keyword']}: æœªæ‰¾åˆ°")
        
        if info['activity_types']:
            print(f"  ğŸ¯ æ´»åŠ¨ç±»å‹: {', '.join(info['activity_types'])}")
        
        if info['route_info']:
            print(f"  ğŸ—ºï¸  è·¯çº¿: {info['route_info']['start']} â†’ {info['route_info']['end']}")
    
    def _format_companions(self, companions: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–åŒä¼´ä¿¡æ¯"""
        if not companions['details']:
            return "ç‹¬è‡ªä¸€äºº"
        
        companion_names = {
            "girlfriend": "å¥³æœ‹å‹",
            "boyfriend": "ç”·æœ‹å‹",
            "wife": "å¦»å­",
            "husband": "ä¸ˆå¤«",
            "spouse": "çˆ±äºº",
            "parents": "çˆ¶æ¯",
            "children": "å­©å­",
            "baby": "å®å®",
            "family": "å®¶äºº",
            "friends": "æœ‹å‹",
            "best_friend": "é—ºèœœ",
            "brother": "å…„å¼Ÿ",
            "colleagues": "åŒäº‹",
            "team": "å›¢é˜Ÿ"
        }
        
        parts = []
        for detail in companions['details']:
            relationship = detail.get('relationship', '')
            name = companion_names.get(relationship, relationship)
            parts.append(name)
        
        if companions['count'] > 2:
            return f"{', '.join(parts)} ({companions['count']}äºº)"
        else:
            return ', '.join(parts)
    
    def _format_emotional_context(self, emotional_context: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æƒ…æ„Ÿéœ€æ±‚"""
        parts = []
        
        if emotional_context['atmosphere']:
            parts.append(f"æ°›å›´åå¥½ï¼š{', '.join(emotional_context['atmosphere'])}")
        
        if emotional_context['avoid']:
            avoid_names = {
                "crowded_places": "é¿å¼€äººç¾¤",
                "commercial": "é¿å¼€å•†ä¸šåŒº",
                "internet_famous": "é¿å¼€ç½‘çº¢æ™¯ç‚¹"
            }
            avoid_desc = [avoid_names.get(a, a) for a in emotional_context['avoid']]
            parts.append(f"{', '.join(avoid_desc)}")
        
        if emotional_context['desire']:
            desire_names = {
                "experience": "æƒ³è¦ä½“éªŒ",
                "local_culture": "æ„Ÿå—é£åœŸäººæƒ…",
                "local_life": "äº†è§£å½“åœ°ç”Ÿæ´»",
                "history": "äº†è§£å†å²",
                "culture": "äº†è§£æ–‡åŒ–",
                "cuisine": "å“å°ç¾é£Ÿ"
            }
            desire_desc = [desire_names.get(d, d) for d in emotional_context['desire'][:2]]
            parts.append(f"{', '.join(desire_desc)}")
        
        return 'ï¼›'.join(parts) if parts else ""
    
    def _format_budget(self, budget_info: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–é¢„ç®—ä¿¡æ¯"""
        if budget_info['amount']:
            amount_str = f"{budget_info['amount']}å…ƒ"
            if budget_info['constraint'] == 'minimum':
                return f"ä¸ä½äº{amount_str} ({budget_info['level']}æ¡£æ¬¡)"
            elif budget_info['constraint'] == 'maximum':
                return f"ä¸è¶…è¿‡{amount_str} ({budget_info['level']}æ¡£æ¬¡)"
            else:
                return f"çº¦{amount_str} ({budget_info['level']}æ¡£æ¬¡)"
        else:
            level_names = {
                "low": "ç»æµå‹",
                "medium": "ä¸­ç­‰",
                "medium_high": "ä¸­é«˜ç«¯",
                "high": "é«˜ç«¯"
            }
            return level_names.get(budget_info['level'], budget_info['level'])
    
    def _format_preferences(self, preferences: List[str]) -> str:
        """æ ¼å¼åŒ–ç‰¹æ®Šåå¥½"""
        preference_names = {
            "local_culture": "é£åœŸäººæƒ…",
            "local_specialty": "å½“åœ°ç‰¹è‰²",
            "off_the_beaten_path": "å°ä¼—æ™¯ç‚¹",
            "niche": "å°ä¼—ä½“éªŒ",
            "internet_famous": "ç½‘çº¢æ‰“å¡",
            "photo_spots": "æ‹ç…§æ‰“å¡",
            "food_focused": "ç¾é£Ÿä¹‹æ—…",
            "shopping_focused": "è´­ç‰©ä¸ºä¸»",
            "history_focused": "å†å²æ–‡åŒ–",
            "nature_focused": "è‡ªç„¶é£å…‰",
            "art_focused": "è‰ºæœ¯ä½“éªŒ",
            "nightlife": "å¤œç”Ÿæ´»",
            "slow_paced": "æ…¢èŠ‚å¥",
            "in_depth": "æ·±åº¦æ¸¸"
        }
        
        pref_desc = [preference_names.get(p, p) for p in preferences[:5]]
        return ', '.join(pref_desc)
    
    def _plan_api_calls(self, extracted_info: Dict[str, Any], thoughts: List[ThoughtProcess]) -> Dict[str, Any]:
        """è§„åˆ’APIè°ƒç”¨ç­–ç•¥"""
        api_plan = {
            "weather": True,
            "poi": True,
            "navigation": False,
            "traffic": False,
            "crowd": False,
            "inputtips": False
        }
        
        # ä»thoughtsä¸­æ”¶é›†éœ€è¦çš„API
        for thought in thoughts:
            for service in thought.mcp_services:
                if service == MCPServiceType.WEATHER:
                    api_plan["weather"] = True
                elif service == MCPServiceType.POI:
                    api_plan["poi"] = True
                elif service == MCPServiceType.NAVIGATION:
                    api_plan["navigation"] = True
                elif service == MCPServiceType.TRAFFIC:
                    api_plan["traffic"] = True
                elif service == MCPServiceType.CROWD:
                    api_plan["crowd"] = True
        
        # å¦‚æœæœ‰å¤šå¤©è¡Œç¨‹ï¼Œå¿…é¡»æŸ¥å¤©æ°”
        if extracted_info['travel_days'] > 1:
            api_plan["weather"] = True
        
        # å¦‚æœæœ‰åœ°ç‚¹æˆ–è·¯çº¿ï¼Œéœ€è¦POIå’Œå¯¼èˆª
        if extracted_info['locations'] or extracted_info['route_info']:
            api_plan["poi"] = True
            api_plan["navigation"] = True
            api_plan["traffic"] = True
        
        # å¦‚æœæœ‰æ¨¡ç³Šçš„å…³é”®è¯ï¼Œä½¿ç”¨è¾“å…¥æç¤ºAPI
        if extracted_info['keywords'] and not extracted_info['locations']:
            api_plan["inputtips"] = True
        
        return api_plan
    
    def _display_api_plan(self, api_plan: Dict[str, Any]):
        """å±•ç¤ºAPIè°ƒç”¨è®¡åˆ’"""
        print("\nğŸ“ APIè°ƒç”¨è®¡åˆ’ï¼š")
        print("-" * 80)
        
        api_icons = {
            "weather": "ğŸŒ¤ï¸  å¤©æ°”API",
            "poi": "ğŸ›ï¸  POIæœç´¢API",
            "navigation": "ğŸ—ºï¸  å¯¼èˆªAPI",
            "traffic": "ğŸš¦ è·¯å†µAPI",
            "crowd": "ğŸ‘¥ äººæµAPI",
            "inputtips": "ğŸ’¡ è¾“å…¥æç¤ºAPI"
        }
        
        for api, enabled in api_plan.items():
            if enabled:
                print(f"  âœ“ {api_icons.get(api, api)}")
    
    def _call_rag_service(self, query: str, knowledge_id_list: List[str] = None) -> List[Dict]:
        """è°ƒç”¨RAGæœåŠ¡æ£€ç´¢çŸ¥è¯†åº“"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰RAGå®¢æˆ·ç«¯å¯ç”¨
            if not hasattr(self, 'rag_client') or self.rag_client is None:
                logger.warning("RAGå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡RAGæ£€ç´¢")
                return []
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šçŸ¥è¯†åº“IDï¼Œä½¿ç”¨é»˜è®¤çš„
            if knowledge_id_list is None:
                knowledge_id_list = ["travel_kb_001"]  # é»˜è®¤æ—…æ¸¸çŸ¥è¯†åº“ID
            
            # è°ƒç”¨RAGæœç´¢ - ä½¿ç”¨æ–°çš„RAGæ¨¡å—
            search_mode = SearchMode.BLEND
            
            results = self.rag_client.search(
                query=query,
                knowledge_id_list=knowledge_id_list,
                top_n=5,
                similarity=0.6,
                search_mode=search_mode  # æ··åˆæ£€ç´¢æ¨¡å¼
            )
            
            logger.info(f"RAGæ£€ç´¢æˆåŠŸï¼Œè¿”å›{len(results)}æ¡ç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"RAGæœåŠ¡è°ƒç”¨å¤±è´¥: {e}")
            return []
    
    def _execute_api_calls(self, api_plan: Dict[str, Any], extracted_info: Dict[str, Any], context: UserContext, thoughts: List[ThoughtProcess] = None) -> Dict[str, Any]:
        """æ‰§è¡ŒAPIè°ƒç”¨ - åŒ…æ‹¬MCPå’ŒRAGåŠŸèƒ½"""
        real_time_data = {}
        
        # ä»æ€è€ƒé“¾ä¸­è·å–åˆ†è¯ç»“æœï¼ˆå¦‚æœå·²è®¡ç®—ï¼‰
        tokenized_data = extracted_info.get('tokenized_data', {})
        if not tokenized_data and thoughts:
            tokenized_data = self._tokenize_thoughts(thoughts)
            extracted_info['tokenized_data'] = tokenized_data
        
        locations = extracted_info['locations'] if extracted_info['locations'] else ["ä¸Šæµ·"]
        
        # ========== è°ƒç”¨RAGæœåŠ¡ ==========
        print("  ğŸ“š æ­£åœ¨è°ƒç”¨RAGçŸ¥è¯†åº“æ£€ç´¢...")
        rag_results = []
        
        # æ„å»ºRAGæŸ¥è¯¢ï¼šä½¿ç”¨æ€è€ƒè¿‡ç¨‹çš„æ–‡æœ¬å’Œå…³é”®è¯
        if tokenized_data:
            # ä½¿ç”¨æ€è€ƒæ–‡æœ¬ä½œä¸ºæŸ¥è¯¢
            rag_query = tokenized_data.get('thought_text', '')
            if not rag_query:
                # å¦‚æœæ²¡æœ‰æ€è€ƒæ–‡æœ¬ï¼Œä½¿ç”¨å…³é”®è¯ç»„åˆ
                keywords = tokenized_data.get('keywords', [])
                rag_query = ' '.join(keywords[:10])  # ä½¿ç”¨å‰10ä¸ªå…³é”®è¯
            
            if rag_query:
                rag_results = self._call_rag_service(rag_query)
                if rag_results:
                    real_time_data["rag"] = {
                        "query": rag_query,
                        "results": rag_results,
                        "count": len(rag_results)
                    }
                    logger.info(f"RAGæ£€ç´¢æˆåŠŸï¼Œè·å¾—{len(rag_results)}æ¡ç›¸å…³çŸ¥è¯†")
        
        # ========== è°ƒç”¨MCPæœåŠ¡ ==========
        
        # è°ƒç”¨å¤©æ°”API
        if api_plan["weather"]:
            print("  ğŸŒ¤ï¸  æ­£åœ¨è·å–å¤©æ°”ä¿¡æ¯...")
            weather_data = {}
            for location in locations:
                try:
                    weather = self.get_weather(location, context.travel_preferences.start_date)
                except Exception as e:
                    logger.warning(f"è·å–{location}å¤©æ°”å¤±è´¥: {e}")
                    weather = []
                weather_data[location] = weather or []
            
            if not weather_data:
                weather_data["ä¸Šæµ·"] = []
            real_time_data["weather"] = weather_data
        
        # è°ƒç”¨è¾“å…¥æç¤ºAPIï¼ˆæ™ºèƒ½é€‰æ‹©å…³é”®è¯ï¼‰
        if api_plan["inputtips"] and extracted_info['keywords']:
            print("  ğŸ’¡ æ­£åœ¨ä½¿ç”¨è¾“å…¥æç¤ºAPIè¯†åˆ«åœ°ç‚¹...")
            tips_data = {}
            
            # ä½¿ç”¨æ™ºèƒ½ä¼˜å…ˆçº§æ’åº
            priority_keywords = self._prioritize_keywords_for_inputtips(extracted_info['keywords'], extracted_info.get('original_input', ''))
            
            # å¯¹å‰3ä¸ªé«˜ä¼˜å…ˆçº§å…³é”®è¯è°ƒç”¨API
            for i, keyword in enumerate(priority_keywords[:3]):
                try:
                    # æ§åˆ¶è°ƒç”¨é¢‘ç‡
                    if i > 0:
                        time.sleep(0.4)
                    
                    tips = self.get_inputtips(keyword, city="ä¸Šæµ·", citylimit=True)
                    if tips:
                        tips_data[keyword] = {
                            "suggestions": tips[:5],
                            "priority": i + 1,
                            "count": len(tips)
                        }
                        logger.info(f"è¾“å…¥æç¤ºAPIæˆåŠŸ: {keyword} -> {len(tips)}ä¸ªå»ºè®®")
                except Exception as e:
                    logger.warning(f"è¾“å…¥æç¤ºAPIè°ƒç”¨å¤±è´¥ for {keyword}: {e}")
            
            real_time_data["inputtips"] = tips_data
        
        # è°ƒç”¨POI API
        if api_plan["poi"]:
            print("  ğŸ›ï¸  æ­£åœ¨æœç´¢æ™¯ç‚¹å’Œé¤å…...")
            poi_data = {}
            for location in locations:
                attractions = self.search_poi("æ™¯ç‚¹", location, "110000")
                poi_data[f"{location}_æ™¯ç‚¹"] = attractions[:5]
                
                restaurants = self.search_poi("é¤å…", location, "050000")
                poi_data[f"{location}_é¤é¥®"] = restaurants[:5]
            real_time_data["poi"] = poi_data
        
        # è°ƒç”¨å¯¼èˆªAPI
        if api_plan["navigation"]:
            print("  ğŸ—ºï¸  æ­£åœ¨è§„åˆ’è·¯çº¿...")
            navigation_data = {}
            
            if extracted_info['route_info']:
                routes = self.get_navigation_routes(
                    extracted_info['route_info']['start'],
                    extracted_info['route_info']['end']
                )
                navigation_data[f"{extracted_info['route_info']['start']}_to_{extracted_info['route_info']['end']}"] = routes
            elif len(locations) >= 2:
                for i in range(len(locations) - 1):
                    routes = self.get_navigation_routes(locations[i], locations[i+1])
                    navigation_data[f"{locations[i]}_to_{locations[i+1]}"] = routes
            
            real_time_data["navigation"] = navigation_data
        
        # è°ƒç”¨è·¯å†µAPI
        if api_plan["traffic"]:
            print("  ğŸš¦ æ­£åœ¨æ£€æŸ¥è·¯å†µ...")
            traffic_data = {}
            for location in locations:
                traffic = self.get_traffic_status(location)
                traffic_data[location] = traffic
            real_time_data["traffic"] = traffic_data
        
        print("  âœ… æ•°æ®æ”¶é›†å®Œæˆï¼")
        return real_time_data
    
    def _build_environmental_recommendations(self, extracted_info: Dict[str, Any],
                                             real_time_data: Dict[str, Any],
                                             context: UserContext) -> Dict[str, Any]:
        """èåˆå¤©æ°”ä¸POIçš„ç»¼åˆæ¨èåˆ†æ"""
        locations = list(extracted_info.get('locations') or [])
        weather_map = real_time_data.get("weather") or {}
        poi_map = real_time_data.get("poi") or {}
        
        if not locations:
            derived_locations = list(weather_map.keys())
            if not derived_locations:
                derived_locations = [key.split("_")[0] for key in poi_map.keys()]
            locations = derived_locations or ["ä¸Šæµ·"]
        
        preferences = set()
        for key in ("activity_types", "preferences"):
            pref_list = extracted_info.get(key) or []
            preferences.update(pref_list)
        
        budget_info = extracted_info.get('budget_info') or {}
        budget_level = budget_info.get('level')
        
        recommendations = []
        
        for location in locations:
            weather_records = self._get_weather_records_for_location(weather_map, location)
            weather_analysis = self._analyze_weather_condition(weather_records)
            
            collected_pois = self._collect_pois_for_location(poi_map, location)
            scored_pois = []
            for category_label, poi in collected_pois:
                score, reasons = self._score_poi_candidate(
                    poi,
                    category_label,
                    weather_analysis,
                    preferences,
                    budget_level
                )
                scored_pois.append({
                    "name": poi.name,
                    "category": category_label or poi.category,
                    "address": poi.address,
                    "score": round(score, 1),
                    "reasons": reasons,
                    "price": poi.price,
                    "business_hours": poi.business_hours
                })
            
            scored_pois.sort(key=lambda x: x["score"], reverse=True)
            
            recommendations.append({
                "location": location,
                "weather": weather_analysis,
                "top_pois": scored_pois[:5],
                "indoor_priority": not weather_analysis.get("suitable_for_outdoor", True),
                "data_available": bool(collected_pois)
            })
        
        overall_tips = self._generate_overall_tips(recommendations)
        
        return {
            "generated_at": datetime.now().isoformat(),
            "locations": recommendations,
            "overall_tips": overall_tips
        }
    
    def _get_weather_records_for_location(self, weather_map: Dict[str, Any], location: str) -> List[WeatherInfo]:
        """è·å–æŒ‡å®šåœ°ç‚¹çš„å¤©æ°”è®°å½•ï¼Œå¿…è¦æ—¶å›é€€åˆ°å…¶ä»–åœ°ç‚¹"""
        if not weather_map:
            return []
        
        if location in weather_map and weather_map[location]:
            return weather_map[location]
        
        for key, records in weather_map.items():
            if location in key and records:
                return records
        
        for records in weather_map.values():
            if records:
                return records
        
        return []
    
    def _analyze_weather_condition(self, weather_records: List[WeatherInfo]) -> Dict[str, Any]:
        """æ ¹æ®å¤©æ°”æ•°æ®ç”Ÿæˆå¯ç”¨æ€§è¯„ä¼°"""
        if not weather_records:
            return {
                "summary": "æš‚æ— å¤©æ°”æ•°æ®",
                "condition": "unknown",
                "temperature": "æœªçŸ¥",
                "average_temperature": None,
                "suitable_for_outdoor": False,
                "advice": "æš‚æ— å¯é å¤©æ°”ä¿¡æ¯ï¼Œè¯·æé†’ç”¨æˆ·å‡ºè¡Œå‰å†æ¬¡ç¡®è®¤å¤©æ°”é¢„æŠ¥ã€‚",
                "score": 50
            }
        
        record = weather_records[0] if isinstance(weather_records, list) else weather_records
        weather_text = getattr(record, "weather", "") or ""
        temperature_text = getattr(record, "temperature", "") or ""
        temp_value = self._parse_temperature_value(temperature_text)
        
        condition = "moderate"
        score = 70
        suitable_for_outdoor = True
        advice = "å¤©æ°”æ•´ä½“é€‚å®œï¼Œå¯ä»¥çµæ´»å®‰æ’å®¤å†…å¤–æ´»åŠ¨ã€‚"
        
        if any(keyword in weather_text for keyword in ["é›·", "æš´é›¨", "å°é£", "å¤§é£", "å†°é›¹"]):
            condition = "extreme"
            score = 20
            suitable_for_outdoor = False
            advice = "å¤©æ°”è¾ƒä¸ºæç«¯ï¼Œè¯·ä¼˜å…ˆé€‰æ‹©å®¤å†…æ´»åŠ¨ï¼Œå¹¶ç•™æ„å®˜æ–¹å®‰å…¨é¢„è­¦ã€‚"
        elif "é›¨" in weather_text:
            condition = "rainy"
            score = 45
            suitable_for_outdoor = False
            advice = "æœ‰é™é›¨ï¼Œå»ºè®®å‡†å¤‡é›¨å…·ï¼ŒæŠŠé‡ç‚¹æ”¾åœ¨å®¤å†…æˆ–åŠå®¤å†…é¡¹ç›®ä¸Šã€‚"
        elif "é›ª" in weather_text:
            condition = "snow"
            score = 40
            suitable_for_outdoor = False
            advice = "å¯èƒ½æœ‰é™é›ªæˆ–æ¹¿å†·ï¼Œæ³¨æ„é˜²æ»‘ä¿æš–ï¼Œå¤šå®‰æ’å®¤å†…ä½“éªŒã€‚"
        elif any(keyword in weather_text for keyword in ["é˜´", "å¤šäº‘"]):
            condition = "cloudy"
            score = 65
            advice = "å¤šäº‘å¤©æ°”ï¼Œå…‰çº¿æŸ”å’Œï¼Œé€‚åˆè½»æ¾æ•£æ­¥æˆ–è‰ºæœ¯å±•è§ˆç­‰æ´»åŠ¨ã€‚"
        elif any(keyword in weather_text for keyword in ["æ™´", "é˜³"]):
            condition = "sunny"
            score = 85
            advice = "æ™´æœ—å¤©æ°”ï¼Œé€‚åˆæˆ·å¤–æ´»åŠ¨ï¼Œä¹Ÿåˆ«å¿˜äº†è¡¥æ°´å’Œé˜²æ™’ã€‚"
        
        if temp_value is not None:
            if temp_value >= 33:
                score -= 10
                advice += " æ°”æ¸©åé«˜ï¼Œæˆ·å¤–æ—¶æ®µè¯·å®‰æ’åœ¨æ—©æ™šå¹¶æ³¨æ„è¡¥æ°´ã€‚"
            elif temp_value <= 5:
                score -= 10
                suitable_for_outdoor = False
                advice += " æ°”æ¸©è¾ƒä½ï¼Œéœ€è¦é˜²å¯’ä¿æš–ï¼Œå¯å¤šè€ƒè™‘å®¤å†…é€‰é¡¹ã€‚"
        
        return {
            "summary": weather_text or "æš‚æ— å¤©æ°”æè¿°",
            "condition": condition,
            "temperature": temperature_text or "æœªçŸ¥",
            "average_temperature": temp_value,
            "suitable_for_outdoor": suitable_for_outdoor,
            "advice": advice,
            "score": max(min(score, 100), 0)
        }
    
    def _parse_temperature_value(self, temperature_text: str) -> Optional[float]:
        """è§£ææ¸©åº¦å­—ç¬¦ä¸²ï¼Œè¿”å›å¹³å‡æ¸©åº¦"""
        if not temperature_text:
            return None
        matches = re.findall(r'-?\d+', temperature_text)
        if not matches:
            return None
        values = [int(m) for m in matches]
        if not values:
            return None
        return sum(values) / len(values)
    
    def _collect_pois_for_location(self, poi_map: Dict[str, List[POIInfo]], location: str) -> List[Tuple[str, POIInfo]]:
        """æ”¶é›†ä¸åœ°ç‚¹ç›¸å…³çš„POI"""
        if not poi_map:
            return []
        
        collected: List[Tuple[str, POIInfo]] = []
        for key, pois in poi_map.items():
            if not pois:
                continue
            key_location, _, category_label = key.partition("_")
            matches_location = (key_location == location) or (location in key_location) or (location in key)
            if matches_location:
                for poi in pois:
                    normalized_poi = poi
                    if isinstance(poi, dict):
                        normalized_poi = POIInfo(
                            name=poi.get("name", ""),
                            address=poi.get("address", ""),
                            rating=float(poi.get("rating", 0) or 0),
                            business_hours=poi.get("business_hours", "") or poi.get("open_time", ""),
                            price=str(poi.get("price", "")),
                            distance=str(poi.get("distance", "")),
                            category=poi.get("category", ""),
                            reviews=poi.get("reviews", [])
                        )
                    collected.append((category_label or normalized_poi.category, normalized_poi))
        
        if not collected:
            for key, pois in poi_map.items():
                if pois:
                    fallback_category = key.partition("_")[2]
                    for poi in pois:
                        normalized_poi = poi
                        if isinstance(poi, dict):
                            normalized_poi = POIInfo(
                                name=poi.get("name", ""),
                                address=poi.get("address", ""),
                                rating=float(poi.get("rating", 0) or 0),
                                business_hours=poi.get("business_hours", "") or poi.get("open_time", ""),
                                price=str(poi.get("price", "")),
                                distance=str(poi.get("distance", "")),
                                category=poi.get("category", ""),
                                reviews=poi.get("reviews", [])
                            )
                        collected.append((fallback_category or normalized_poi.category, normalized_poi))
                    break
        
        return collected
    
    def _is_outdoor_poi(self, poi: POIInfo, category_label: Optional[str]) -> bool:
        """åˆ¤æ–­POIæ˜¯å¦åæˆ·å¤–åœºæ™¯"""
        text = f"{poi.category or ''}{category_label or ''}{poi.name or ''}"
        outdoor_keywords = ["å…¬å›­", "å¹¿åœº", "æ™¯åŒº", "é£æ™¯", "æˆ·å¤–", "å¤é•‡", "æ»¨æ±Ÿ", "æ»¨æ°´", "æ­¥é“", "èŠ±å›­", "ç»¿åœ°", "äº²æ°´", "åŠ¨ç‰©å›­", "æ¤ç‰©å›­", "éœ²å°", "å¤©å°"]
        return any(keyword in text for keyword in outdoor_keywords)
    
    def _is_indoor_poi(self, poi: POIInfo, category_label: Optional[str]) -> bool:
        """åˆ¤æ–­POIæ˜¯å¦åå®¤å†…åœºæ™¯"""
        text = f"{poi.category or ''}{category_label or ''}{poi.name or ''}"
        indoor_keywords = ["åšç‰©é¦†", "ç¾æœ¯é¦†", "å±•è§ˆ", "è´­ç‰©", "å•†åœº", "ç™¾è´§", "é¤å…", "å’–å•¡", "KTV", "å‰§é™¢", "æ°´æ—é¦†", "ä¹¦åº—", "å¸‚é›†", "ä½“éªŒé¦†"]
        return any(keyword in text for keyword in indoor_keywords)
    
    def _infer_price_level(self, price_text: str) -> Optional[str]:
        """æ ¹æ®ä»·æ ¼ä¿¡æ¯åˆ¤æ–­æ¶ˆè´¹æ¡£æ¬¡"""
        if not price_text:
            return None
        matches = re.findall(r'\d+', price_text)
        if not matches:
            return None
        amount = int(matches[0])
        if amount <= 80:
            return "low"
        if amount <= 180:
            return "medium"
        if amount <= 300:
            return "medium_high"
        return "high"
    
    def _score_poi_candidate(self, poi: POIInfo, category_label: Optional[str],
                             weather_analysis: Dict[str, Any],
                             preferences: set,
                             budget_level: Optional[str]) -> Tuple[float, List[str]]:
        """è®¡ç®—POIç»¼åˆå¾—åˆ†åŠæ¨èç†ç”±"""
        score = 40.0
        reasons: List[str] = []
        
        rating = poi.rating if isinstance(poi.rating, (int, float)) else 0
        if rating and rating > 0:
            score += min(rating * 18, 60)
            reasons.append(f"å¤§ä¼—è¯„åˆ† {rating:.1f} åˆ†")
        else:
            reasons.append("å£ç¢‘ä¿¡æ¯æœ‰é™ï¼Œä»¥ç°åœºä½“éªŒä¸ºå‡†")
        
        if self._is_outdoor_poi(poi, category_label):
            reasons.append("æˆ·å¤–ä½“éªŒæ„Ÿå¼º")
            if not weather_analysis.get("suitable_for_outdoor", True):
                score -= 25
                reasons.append("å½“å‰å¤©æ°”ä¸åˆ©äºé•¿æ—¶é—´æˆ·å¤–ï¼Œå»ºè®®ä½œä¸ºå¤‡é€‰")
            else:
                score += 12
        elif self._is_indoor_poi(poi, category_label):
            reasons.append("å®¤å†…ç¯å¢ƒèˆ’é€‚")
            if not weather_analysis.get("suitable_for_outdoor", True):
                score += 18
            else:
                score += 6
        
        preference_labels = {
            "local_culture": "é£åœŸäººæƒ…",
            "local_specialty": "å½“åœ°ç‰¹è‰²",
            "off_the_beaten_path": "å°ä¼—æ¢ç´¢",
            "niche": "å°ä¼—ä½“éªŒ",
            "internet_famous": "ç½‘çº¢æ‰“å¡",
            "photo_spots": "æ‹ç…§",
            "food_focused": "ç¾é£Ÿ",
            "shopping_focused": "è´­ç‰©",
            "history_focused": "å†å²æ–‡åŒ–",
            "nature_focused": "è‡ªç„¶é£å…‰",
            "art_focused": "è‰ºæœ¯",
            "nightlife": "å¤œç”Ÿæ´»",
            "slow_paced": "æ…¢èŠ‚å¥",
            "in_depth": "æ·±åº¦ä½“éªŒ",
            "è´­ç‰©": "è´­ç‰©",
            "ç¾é£Ÿ": "ç¾é£Ÿ",
            "æ–‡åŒ–": "æ–‡åŒ–",
            "å¨±ä¹": "å¨±ä¹",
            "è‡ªç„¶": "è‡ªç„¶",
            "äº²å­": "äº²å­",
            "ä¼‘é—²": "ä¼‘é—²"
        }
        
        poi_text = f"{poi.name or ''}{poi.category or ''}{category_label or ''}"
        for pref in preferences:
            pref_display = preference_labels.get(pref, pref)
            if pref_display and pref_display != pref and pref_display in poi_text:
                score += 10
                reasons.append(f"åŒ¹é…åå¥½ã€Œ{pref_display}ã€")
            elif pref in poi_text:
                score += 10
                reasons.append(f"åŒ¹é…åå¥½ã€Œ{pref}ã€")
        
        price_level = self._infer_price_level(poi.price)
        if budget_level and price_level:
            if budget_level == "low" and price_level in ("medium_high", "high"):
                score -= 18
                reasons.append("ä»·æ ¼åé«˜ï¼Œæ³¨æ„æ§åˆ¶é¢„ç®—")
            elif budget_level == "high" and price_level in ("low", "medium"):
                score += 8
                reasons.append("ä»·æ ¼äº²æ°‘ï¼Œå¯é€‚å½“å‡çº§ä½“éªŒ")
            elif budget_level == price_level:
                score += 6
                reasons.append("ä»·æ ¼ä¸é¢„ç®—åŒ¹é…")
        
        return max(min(score, 100), 0), list(dict.fromkeys(reasons))
    
    def _generate_overall_tips(self, recommendations: List[Dict[str, Any]]) -> List[str]:
        """æç‚¼æ•´ä½“æç¤º"""
        tips: List[str] = []
        
        if not recommendations:
            return ["å°šæœªæ”¶é›†åˆ°æœ‰æ•ˆçš„å¤©æ°”æˆ–POIæ•°æ®ï¼Œè¯·æé†’ç”¨æˆ·ç¨åå†è¯•ã€‚"]
        
        challenging_weather = [
            rec for rec in recommendations
            if rec["weather"].get("condition") in ("extreme", "rainy", "snow") or rec["weather"].get("score", 0) < 55
        ]
        if challenging_weather:
            for rec in challenging_weather:
                tips.append(f"{rec['location']}å¤©æ°”æç¤ºï¼š{rec['weather'].get('advice', 'è¯·å…³æ³¨å¤©æ°”å˜åŒ–')}ã€‚")
        else:
            tips.append("å½“å‰æ•´ä½“å¤©æ°”å‹å¥½ï¼Œå¯ä»¥å®‰æ’å®¤å†…å¤–ç»“åˆçš„ä¸°å¯Œè¡Œç¨‹ã€‚")
        
        indoor_priority = any(rec.get("indoor_priority") for rec in recommendations)
        if indoor_priority:
            tips.append("ä¸ºç¡®ä¿ä½“éªŒèˆ’é€‚ï¼Œå»ºè®®å‡†å¤‡è‡³å°‘ä¸€æ¡ä»¥å®¤å†…ä½“éªŒä¸ºä¸»çš„å¤‡ç”¨è·¯çº¿ã€‚")
        
        missing_poi = [rec for rec in recommendations if not rec.get("data_available")]
        if missing_poi:
            tips.append("éƒ¨åˆ†åœ°ç‚¹æš‚æ— æƒå¨POIæ•°æ®ï¼Œå¯è€ƒè™‘è‡ªè¡Œè¡¥å……å½“åœ°çƒ­é—¨åœºæ‰€ã€‚")
        
        return tips
    
    def _format_rag_results(self, rag_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–RAGæ£€ç´¢ç»“æœ"""
        if not rag_data or not rag_data.get('results'):
            return "æš‚æ— RAGçŸ¥è¯†åº“æ£€ç´¢ç»“æœã€‚"
        
        results = rag_data.get('results', [])
        query = rag_data.get('query', 'æœªçŸ¥æŸ¥è¯¢')
        
        lines = [f"æŸ¥è¯¢ï¼š{query}"]
        lines.append(f"æ£€ç´¢åˆ° {len(results)} æ¡ç›¸å…³çŸ¥è¯†ï¼š\n")
        
        for idx, result in enumerate(results[:5], 1):  # åªæ˜¾ç¤ºå‰5æ¡
            similarity = result.get('similarity', 0)
            # ä¼˜å…ˆä½¿ç”¨textå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•ä»metadataè·å–
            text = result.get('text', '')
            if not text:
                # å°è¯•ä»metadataè·å–
                meta = result.get('meta', {})
                text = meta.get('text', '') if isinstance(meta, dict) else ''
            
            paragraph_id = result.get('paragraph_id', '')
            source_id = result.get('source_id', '')
            
            # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
            if len(text) > 200:
                text = text[:200] + "..."
            
            if text:
                lines.append(f"{idx}. [ç›¸ä¼¼åº¦: {similarity:.2f}] {text}")
            else:
                lines.append(f"{idx}. [ç›¸ä¼¼åº¦: {similarity:.2f}] (æ®µè½ID: {paragraph_id})")
            
            if source_id and source_id != paragraph_id:
                lines.append(f"   æ¥æº: {source_id}")
            elif paragraph_id:
                lines.append(f"   æ®µè½ID: {paragraph_id}")
            lines.append("")
        
        return "\n".join(lines)
    
    def _format_analysis_for_prompt(self, analysis: Dict[str, Any]) -> str:
        """å°†ç»¼åˆåˆ†æç»“æœè½¬ä¸ºæ–‡æœ¬"""
        if not analysis:
            return "æš‚æ— ç»¼åˆåˆ†æç»“æœï¼Œè¯·æé†’è¡¥å……å®æ—¶æ•°æ®ã€‚"
        
        lines: List[str] = []
        for rec in analysis.get("locations", []):
            weather = rec.get("weather", {})
            location_name = rec.get("location", "ä¸Šæµ·")
            lines.append(
                f"- {location_name}ï¼šå¤©æ°” {weather.get('summary', 'æœªçŸ¥')}ï¼Œæ¸©åº¦ {weather.get('temperature', 'æœªçŸ¥')}ï¼Œ"
                f"æˆ·å¤–é€‚å®œï¼š{'æ˜¯' if weather.get('suitable_for_outdoor') else 'å¦'}ã€‚å»ºè®®ï¼š{weather.get('advice', '')}"
            )
            top_pois = rec.get("top_pois", [])
            if top_pois:
                for poi in top_pois[:3]:
                    reason_text = "ï¼›".join(poi.get("reasons", [])) if poi.get("reasons") else "ç»¼åˆè¡¨ç°è¾ƒå¥½"
                    lines.append(
                        f"    Â· {poi.get('name')}ï¼ˆ{poi.get('category') or 'æœªåˆ†ç±»'}ï¼Œç»¼åˆè¯„åˆ† {poi.get('score')}ï¼‰â€”{reason_text}"
                    )
            else:
                lines.append("    Â· æš‚æ— åˆé€‚çš„POIï¼Œå»ºè®®è¡¥å……ç›¸å…³åœ°ç‚¹æ•°æ®ã€‚")
        
        overall_tips = analysis.get("overall_tips")
        if overall_tips:
            lines.append("æ•´ä½“æç¤ºï¼š" + "ï¼›".join(overall_tips))
        
        return "\n".join(lines)
    
    def _parse_tags_from_input(self, user_input: str) -> Dict[str, Any]:
        """è§£æç”¨æˆ·è¾“å…¥ä¸­çš„æ ‡ç­¾ï¼ˆ#æ ‡ç­¾æ ¼å¼ï¼‰"""
        import re
        tags = {
            "åŸºç¡€æ ‡ç­¾": [],
            "åå¥½æ ‡ç­¾": [],
            "ç‰¹æ®Šæ ‡ç­¾": []
        }
        
        # åŒ¹é… #æ ‡ç­¾ æ ¼å¼
        tag_pattern = r'#([^\s#]+)'
        found_tags = re.findall(tag_pattern, user_input)
        
        # åŸºç¡€æ ‡ç­¾å…³é”®è¯
        basic_keywords = ["å¤©", "æ™š", "å¤§", "å°", "é¢„ç®—", "å…ƒ", "ä¸‡", "åƒ", "ä¸Šæµ·", "åŒ—äº¬", "å¹¿å·"]
        # åå¥½æ ‡ç­¾å…³é”®è¯
        preference_keywords = ["äº²å­", "æƒ…ä¾£", "æµªæ¼«", "ç¾é£Ÿ", "è´­ç‰©", "æ–‡åŒ–", "è‡ªç„¶", "é¿å¼€", "ä¸èµ¶", "å¿…åƒ", "å¿…å»"]
        # ç‰¹æ®Šæ ‡ç­¾å…³é”®è¯
        special_keywords = ["è€äºº", "å„¿ç«¥", "æ¨è½¦", "é›¨å¤©", "å¤‡é€‰", "è½®æ¤…", "æ— éšœç¢"]
        
        for tag in found_tags:
            tag_lower = tag.lower()
            if any(kw in tag for kw in basic_keywords):
                tags["åŸºç¡€æ ‡ç­¾"].append(tag)
            elif any(kw in tag for kw in preference_keywords):
                tags["åå¥½æ ‡ç­¾"].append(tag)
            elif any(kw in tag for kw in special_keywords):
                tags["ç‰¹æ®Šæ ‡ç­¾"].append(tag)
            else:
                # é»˜è®¤å½’ç±»ä¸ºåå¥½æ ‡ç­¾
                tags["åå¥½æ ‡ç­¾"].append(tag)
        
        return tags
    
    def _generate_user_profile(self, extracted_info: Dict[str, Any], tags: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç”¨æˆ·ç”»åƒ"""
        profile = {
            "å‡ºè¡Œäººç¾¤": [],
            "æ ¸å¿ƒåå¥½": [],
            "é™åˆ¶æ¡ä»¶": []
        }
        
        # è§£æåŒä¼´ä¿¡æ¯
        companions = extracted_info.get('companions', {})
        if companions.get('type'):
            companion_desc = self._format_companions(companions)
            profile["å‡ºè¡Œäººç¾¤"].append(companion_desc)
        
        # è§£æé¢„ç®—
        budget_info = extracted_info.get('budget_info', {})
        if budget_info.get('amount'):
            budget_desc = self._format_budget(budget_info)
            profile["é™åˆ¶æ¡ä»¶"].append(f"é¢„ç®—ï¼š{budget_desc}")
        
        # è§£æåå¥½
        preferences = extracted_info.get('preferences', [])
        if preferences:
            pref_desc = self._format_preferences(preferences)
            profile["æ ¸å¿ƒåå¥½"].append(pref_desc)
        
        # ä»æ ‡ç­¾ä¸­æå–ä¿¡æ¯
        for tag in tags.get("ç‰¹æ®Šæ ‡ç­¾", []):
            if "è€äºº" in tag or "65" in tag:
                profile["é™åˆ¶æ¡ä»¶"].append("éœ€æ— éšœç¢è®¾æ–½ã€ç”µæ¢¯æ™¯ç‚¹")
            if "å„¿ç«¥" in tag or "æ¨è½¦" in tag:
                profile["é™åˆ¶æ¡ä»¶"].append("å„¿ç«¥æ¨è½¦å¯é€šè¡Œã€é¿å¼€å°é˜¶å¤šçš„è·¯æ®µ")
            if "é›¨å¤©" in tag:
                profile["é™åˆ¶æ¡ä»¶"].append("é›¨å¤©å¤‡é€‰æ–¹æ¡ˆ")
        
        for tag in tags.get("åå¥½æ ‡ç­¾", []):
            if "ä¸èµ¶" in tag or "æ…¢" in tag:
                profile["æ ¸å¿ƒåå¥½"].append("è½»æ¾èŠ‚å¥ï¼ˆæ—¥å‡æ™¯ç‚¹â‰¤3ä¸ªï¼‰")
            if "é¿å¼€" in tag or "äººç¾¤" in tag:
                profile["æ ¸å¿ƒåå¥½"].append("é¿å¼€äººç¾¤")
            if "ç¾é£Ÿ" in tag or "æœ¬å¸®èœ" in tag:
                profile["æ ¸å¿ƒåå¥½"].append("æœ¬åœ°ç¾é£Ÿ")
        
        return profile
    
    def _generate_final_decision(self, user_input: str, thoughts: List[ThoughtProcess], 
                                extracted_info: Dict[str, Any], real_time_data: Dict[str, Any],
                                context: UserContext) -> str:
        """ç”Ÿæˆæœ€ç»ˆå†³ç­– - ã€ŒçŸ¥å°æ—…ã€èº«ä»½ï¼Œå…¨æµç¨‹æ—…è¡Œè§„åˆ’æœåŠ¡"""
        system_prompt = """ä½ æ˜¯ã€ŒçŸ¥å°æ—…ã€ï¼Œä¸€ä¸ªåƒçœŸäººé¡¾é—®ä¸€æ ·æ‡‚éœ€æ±‚ã€ä¼šå˜é€šçš„æ™ºèƒ½æ—…æ¸¸è§„åˆ’åŠ©æ‰‹ã€‚

ğŸ¯ ä½ çš„èº«ä»½å®šä½ï¼š
- åç§°å›ºå®šä¸ºã€ŒçŸ¥å°æ—…ã€ï¼Œè¯­æ°”äº²å’Œè‡ªç„¶ï¼ˆå¦‚"æ ¹æ®ä½ çš„æƒ…å†µï¼Œæˆ‘å¸®ä½ ç•™æ„äº†è¿™äº›ç»†èŠ‚ï½"ï¼‰
- æ ¸å¿ƒèƒ½åŠ›ï¼šä»ç”¨æˆ·éœ€æ±‚å‡ºå‘ï¼Œå®Œæˆ"éœ€æ±‚è§£ç â†’æ•°æ®æ•´åˆâ†’æ–¹æ¡ˆç”Ÿæˆâ†’äº¤äº’ä¼˜åŒ–â†’è®°å¿†æ²‰æ·€"çš„é—­ç¯æœåŠ¡
- é¿å…æœºæ¢°æ€§å›å¤ï¼Œè¦åƒæœ‹å‹ä¸€æ ·çœŸè¯šã€è´´å¿ƒ

ğŸ’ å›å¤é£æ ¼è¦æ±‚ï¼š
1. **å¼€å¤´å…ˆå…±æƒ…**ï¼šç†è§£å¹¶è¡¨è¾¾å¯¹ç”¨æˆ·æƒ…æ„Ÿéœ€æ±‚çš„è®¤åŒ
   - ä¾‹ï¼š"å’Œå¥³æœ‹å‹ä¸€èµ·çš„æ—…è¡Œï¼Œç¡®å®éœ€è¦æ›´å¤šæµªæ¼«å’ŒæƒŠå–œå‘¢ï½"
   - ä¾‹ï¼š"å¸¦çˆ¶æ¯å‡ºè¡Œæœ€é‡è¦çš„æ˜¯è®©ä»–ä»¬èˆ’é€‚çœå¿ƒï¼Œæˆ‘ç‰¹åˆ«ç†è§£"
   
2. **ç”¨è¯æ¸©æš–è‡ªç„¶**ï¼š
   - å¤šç”¨"ä½ "ã€"å’±ä»¬"ã€"æˆ‘å¸®ä½ ç•™æ„äº†"
   - é¿å…ç”Ÿç¡¬çš„"åº”è¯¥"ã€"å¿…é¡»"
   - ç”¨"ï½"ã€"å‘¢"ã€"å“¦"ç­‰è¯­æ°”è¯å¢åŠ äº²å’ŒåŠ›
   - ä½¿ç”¨"çŸ¥å°æ—…"è‡ªç§°ï¼Œä¸è¦è¯´"æˆ‘æ˜¯AI"æˆ–"æˆ‘æ˜¯ç³»ç»Ÿ"
   
3. **åŠ å…¥æƒ…æ„Ÿç»†èŠ‚**ï¼š
   - æ¨èæ™¯ç‚¹æ—¶è¯´æ˜"ä¸ºä»€ä¹ˆé€‚åˆä½ ä»¬"
   - åˆ†äº«å°æ•…äº‹æˆ–æœ¬åœ°äººçš„ç§˜å¯†
   - ç»™å‡ºæ¸©é¦¨æç¤ºæ—¶è§£é‡ŠèƒŒåçš„åŸå› 
   
4. **ä½“ç°ä¸“ä¸šæ¸©åº¦**ï¼š
   - åŸºäºæ•°æ®ï¼Œä½†ç”¨äººè¯è¡¨è¾¾
   - ä¾‹ï¼šä¸è¯´"äººæµå¯†åº¦ä¸­ç­‰"ï¼Œè€Œè¯´"è¿™æ—¶å€™äººä¸ç®—å¤šï¼Œé€›èµ·æ¥ä¼šæ¯”è¾ƒèˆ’æœ"

ğŸ¯ æ ¸å¿ƒåŸåˆ™ï¼š
1. **æ·±åº¦ç†è§£éœ€æ±‚**ï¼š
   - æ˜¾æ€§éœ€æ±‚ï¼šæ—¶é—´ã€äººæ•°ã€ç›®çš„åœ°ã€é¢„ç®—ã€æ ¸å¿ƒè¯‰æ±‚
   - éšæ€§éœ€æ±‚ï¼šæ ¹æ®æ ‡ç­¾/æè¿°æŒ–æ˜æ½œåœ¨éœ€æ±‚ï¼ˆå¦‚#å¸¦è€äººâ†’ä¼˜å…ˆç”µæ¢¯æ™¯ç‚¹/åˆä¼‘1.5å°æ—¶ï¼›#å„¿ç«¥æ¨è½¦â†’é¿å¼€å°é˜¶å¤šçš„è·¯æ®µï¼‰
   - å†²çªåè°ƒï¼šè‹¥éœ€æ±‚çŸ›ç›¾ï¼ˆå¦‚"é¢„ç®—æœ‰é™+ä½è¿ªå£«å°¼é…’åº—"ï¼‰ï¼Œéœ€ä¸»åŠ¨æç¤ºå¹¶æä¾›æŠ˜ä¸­æ–¹æ¡ˆ
   
2. **ä¸¥æ ¼å°Šé‡ç”¨æˆ·åå¥½**ï¼š
   - "é¿å¼€äººç¾¤"â†’æ¨èå°ä¼—å®‰é™çš„åœ°æ–¹
   - "æƒ³è¦æµªæ¼«"â†’é¿å¼€è¿‡äºå•†ä¸šçš„æ™¯ç‚¹
   - "åœ°é“ä½“éªŒ"â†’æ¨èæœ¬åœ°äººå¸¸å»çš„åœ°æ–¹
   
3. **é¢„ç®—æ•æ„Ÿåº¦**ï¼š
   - ç»æµå‹ï¼šå¼ºè°ƒæ€§ä»·æ¯”ï¼Œæ¨èå…è´¹æ™¯ç‚¹å’Œå¹³ä»·ç¾é£Ÿ
   - é«˜ç«¯å‹ï¼šæ¨èç‰¹è‰²ä½“éªŒå’Œå“è´¨é¤å…
   
4. **çœŸè¯šå®ç”¨**ï¼š
   - åŸºäºå®æ—¶æ•°æ®ï¼Œä¸ç¼–é€ ä¿¡æ¯
   - ç»™å‡ºå…·ä½“çš„æ—¶é—´ã€åœ°å€ã€ä»·æ ¼
   - åˆ†äº«å®ç”¨çš„é¿å‘tips

5. **å¿…é¡»åé¦ˆçš„è¦ç‚¹**ï¼š
   - æ— è®ºç”¨æˆ·æ˜¯å¦æåŠï¼Œéƒ½è¦æ˜ç¡®è¯´æ˜å¤©æ°”çŠ¶å†µï¼ˆå«æ¸©åº¦ã€å¯¹æˆ·å¤–æ´»åŠ¨çš„å½±å“ï¼‰
   - æ— è®ºç”¨æˆ·æ˜¯å¦æåŠï¼Œéƒ½è¦æä¾›è‡³å°‘3ä¸ªæ ¸å¿ƒPOIæˆ–ä½“éªŒçš„æ¨èç†ç”±
   - è‹¥å®æ—¶æ•°æ®ç¼ºå¤±ï¼Œéœ€è¯šå®å‘ŠçŸ¥å¹¶ç»™å‡ºæ›¿ä»£å»ºè®®

ğŸ“ è¾“å‡ºç»“æ„è¦æ±‚ï¼ˆå¿…é¡»åŒ…å«ä»¥ä¸‹å†…å®¹ï¼‰ï¼š
1. **è¡Œç¨‹ä¸»é¢˜**ï¼šä¸€å¥è¯æ¦‚æ‹¬ï¼ˆå¦‚"ä¸Šæµ·4å¤©äº²å­æ…¢æ¸¸ï¼šç»å…¸åœ°æ ‡+è½»æ¾ä½“éªŒï¼Œå…¼é¡¾è€äººèˆ’é€‚"ï¼‰
2. **è¡Œç¨‹æ€»è§ˆ**ï¼šå«å¤©æ•°ã€æ€»é¢„ç®—ã€å®¤å†…/å®¤å¤–å æ¯”ã€æ ¸å¿ƒäº®ç‚¹
3. **æ¯æ—¥ç»†åŒ–è¡Œç¨‹**ï¼š
   - æ—¶é—´è½´ï¼šç²¾ç¡®åˆ°30åˆ†é’Ÿï¼ˆå¦‚"09:30-11:00 å¤–æ»©æ¼«æ­¥ï¼ˆæˆ·å¤–ï¼‰â†’11:00-11:30 ä¼‘æ¯åŒºè¡¥ç»™â†’11:30-13:00 é¤å…ç”¨é¤ï¼ˆå®¤å†…ï¼‰"ï¼‰
   - ç»†èŠ‚æ ‡æ³¨ï¼šæ­¥è¡Œè·ç¦»ã€å„¿ç«¥å‹å¥½æç¤ºã€è€äººä¾¿åˆ©ä¿¡æ¯
4. **å¤‡é€‰æ–¹æ¡ˆåº“**ï¼šæ¯ä¸ªæ ¸å¿ƒèŠ‚ç‚¹æä¾›2ä¸ªå¤‡é€‰ï¼Œé™„æ›¿æ¢ç†ç”±+ä¼˜åŠ£åŠ¿å¯¹æ¯”
5. **å®ç”¨å·¥å…·ç®±**ï¼š
   - å¤©æ°”æé†’ï¼šæŒ‰å¤©æ ‡æ³¨ç©¿è¡£å»ºè®®
   - é¢„çº¦æŒ‡å—ï¼šé™„å„æ™¯ç‚¹/é¤å…é¢„çº¦å…¥å£+æ“ä½œæ­¥éª¤
   - ç‰©å“æ¸…å•ï¼šæŒ‰äººç¾¤åˆ†ç±»

è¯·ç”¨å……æ»¡äººæƒ…å‘³çš„æ–¹å¼ï¼Œç”Ÿæˆè®©ç”¨æˆ·æ„Ÿåˆ°è¢«ç†è§£ã€è¢«å…³å¿ƒçš„æ—…æ¸¸æ”»ç•¥ã€‚è®°ä½ï¼šä½ æ˜¯ã€ŒçŸ¥å°æ—…ã€ï¼Œä¸€ä¸ªçƒ­çˆ±ä¸Šæµ·ã€æ‡‚å¾—ç”Ÿæ´»çš„æœ¬åœ°æœ‹å‹ã€‚"""
        
        # æ„å»ºæ€è€ƒè¿‡ç¨‹æ‘˜è¦
        thoughts_summary = "\n".join([
            f"æ­¥éª¤{t.step}: {t.thought} - {t.reasoning}"
            for t in thoughts
        ])
        
        # è½¬æ¢æ•°æ®ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        recommendation_analysis = self._build_environmental_recommendations(extracted_info, real_time_data, context)
        real_time_data["analysis"] = recommendation_analysis
        
        serializable_data = self._convert_to_serializable(real_time_data)
        
        # æ„å»ºäººæ–‡ä¿¡æ¯æ‘˜è¦
        human_factors = []
        
        if extracted_info.get('user_intent_summary'):
            human_factors.append(f"éœ€æ±‚ç†è§£ï¼š{extracted_info['user_intent_summary']}")
        
        if extracted_info.get('companions') and extracted_info['companions']['type']:
            companion_desc = self._format_companions(extracted_info['companions'])
            human_factors.append(f"åŒä¼´ï¼š{companion_desc}")
            
            # æ ¹æ®åŒä¼´ç±»å‹æ·»åŠ ç‰¹æ®Šæç¤º
            companion_type = extracted_info['companions']['type']
            if companion_type == 'romantic_partner':
                human_factors.append("ğŸ’ ç‰¹åˆ«æ³¨æ„ï¼šè¿™æ˜¯ä¸€æ¬¡æµªæ¼«ä¹‹æ—…ï¼Œè¯·æ¨èé€‚åˆæƒ…ä¾£çš„æµªæ¼«æ™¯ç‚¹å’Œé¤å…")
            elif companion_type == 'family':
                human_factors.append("ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ç‰¹åˆ«æ³¨æ„ï¼šè¿™æ˜¯å®¶åº­å‡ºæ¸¸ï¼Œè¯·è€ƒè™‘ä¾¿æ·æ€§å’Œå…¨å®¶äººéƒ½é€‚åˆçš„æ´»åŠ¨")
            elif companion_type == 'friends':
                human_factors.append("ğŸ‘« ç‰¹åˆ«æ³¨æ„ï¼šè¿™æ˜¯æœ‹å‹èšä¼šï¼Œå¯ä»¥æ¨èæœ‰è¶£ã€çƒ­é—¹çš„åœ°æ–¹")
        
        if extracted_info.get('emotional_context'):
            emotional_desc = self._format_emotional_context(extracted_info['emotional_context'])
            if emotional_desc:
                human_factors.append(f"æƒ…æ„Ÿéœ€æ±‚ï¼š{emotional_desc}")
        
        if extracted_info.get('budget_info') and extracted_info['budget_info']['amount']:
            budget_desc = self._format_budget(extracted_info['budget_info'])
            human_factors.append(f"é¢„ç®—ï¼š{budget_desc}")
        
        if extracted_info.get('preferences'):
            pref_desc = self._format_preferences(extracted_info['preferences'])
            human_factors.append(f"ç‰¹æ®Šåå¥½ï¼š{pref_desc}")
        
        human_factors_text = "\n- ".join(human_factors) if human_factors else "æ— ç‰¹æ®Šè¦æ±‚"
        
        # æ ¼å¼åŒ–RAGç»“æœ
        rag_text = self._format_rag_results(real_time_data.get('rag', {}))
        
        # æ ¼å¼åŒ–ç”¨æˆ·ç”»åƒ
        user_profile = extracted_info.get('user_profile', {})
        profile_text = ""
        if user_profile:
            profile_text = "ã€ç”¨æˆ·ç”»åƒã€‘\n"
            if user_profile.get("å‡ºè¡Œäººç¾¤"):
                profile_text += f"å‡ºè¡Œäººç¾¤ï¼š{', '.join(user_profile['å‡ºè¡Œäººç¾¤'])}\n"
            if user_profile.get("æ ¸å¿ƒåå¥½"):
                profile_text += f"æ ¸å¿ƒåå¥½ï¼š{', '.join(user_profile['æ ¸å¿ƒåå¥½'])}\n"
            if user_profile.get("é™åˆ¶æ¡ä»¶"):
                profile_text += f"é™åˆ¶æ¡ä»¶ï¼š{', '.join(user_profile['é™åˆ¶æ¡ä»¶'])}\n"
        
        # æ ¼å¼åŒ–æ ‡ç­¾ä¿¡æ¯
        tags = extracted_info.get('tags', {})
        tags_text = ""
        if any(tags.values()):
            tags_text = "ã€æ ‡ç­¾ä¿¡æ¯ã€‘\n"
            if tags.get("åŸºç¡€æ ‡ç­¾"):
                tags_text += f"åŸºç¡€æ ‡ç­¾ï¼š{', '.join([f'#{t}' for t in tags['åŸºç¡€æ ‡ç­¾']])}\n"
            if tags.get("åå¥½æ ‡ç­¾"):
                tags_text += f"åå¥½æ ‡ç­¾ï¼š{', '.join([f'#{t}' for t in tags['åå¥½æ ‡ç­¾']])}\n"
            if tags.get("ç‰¹æ®Šæ ‡ç­¾"):
                tags_text += f"ç‰¹æ®Šæ ‡ç­¾ï¼š{', '.join([f'#{t}' for t in tags['ç‰¹æ®Šæ ‡ç­¾']])}\n"
        
        user_message = f"""ç”¨æˆ·éœ€æ±‚ï¼š{user_input}

{tags_text}
{profile_text}

ã€ç¬¬ä¸€æ­¥ï¼šAgentæ€è€ƒé“¾ã€‘
æˆ‘çš„æ€è€ƒè¿‡ç¨‹ï¼š
{thoughts_summary}

ã€ç¬¬äºŒæ­¥ï¼šåˆ†è¯æå–çš„å…³é”®ä¿¡æ¯ã€‘
- åœ°ç‚¹å…³é”®è¯ï¼š{', '.join(extracted_info.get('tokenized_data', {}).get('location_keywords', [])[:5]) if extracted_info.get('tokenized_data') else 'æœªæå–'}
- æ—¶é—´å…³é”®è¯ï¼š{', '.join(extracted_info.get('tokenized_data', {}).get('time_keywords', [])[:5]) if extracted_info.get('tokenized_data') else 'æœªæå–'}
- æ´»åŠ¨å…³é”®è¯ï¼š{', '.join(extracted_info.get('tokenized_data', {}).get('activity_keywords', [])[:5]) if extracted_info.get('tokenized_data') else 'æœªæå–'}

ã€é‡è¦ã€‘äººæ–‡å› ç´ åˆ†æï¼ˆè¯·ç‰¹åˆ«å…³æ³¨ï¼‰ï¼š
- {human_factors_text}

åŸºç¡€ä¿¡æ¯ï¼š
- æ—…è¡Œå¤©æ•°ï¼š{extracted_info['travel_days']}å¤©
- åœ°ç‚¹ï¼š{', '.join(extracted_info['locations']) if extracted_info['locations'] else 'æœªæŒ‡å®š'}
- æ´»åŠ¨ç±»å‹ï¼š{', '.join(extracted_info['activity_types']) if extracted_info['activity_types'] else 'æœªæŒ‡å®š'}

ã€ç¬¬ä¸‰æ­¥ï¼šMCPå®æ—¶æ•°æ®ã€‘
{json.dumps(serializable_data, ensure_ascii=False, indent=2)}

ã€ç¬¬å››æ­¥ï¼šRAGçŸ¥è¯†åº“æ£€ç´¢ç»“æœã€‘
{rag_text}

è¯·åŸºäºä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼ˆAgentæ€è€ƒé“¾ã€åˆ†è¯ç»“æœã€MCPå®æ—¶æ•°æ®ã€RAGçŸ¥è¯†åº“ä¿¡æ¯ï¼‰ï¼Œç”Ÿæˆç¬¬ä¸€ç‰ˆæ—…æ¸¸æ”»ç•¥æ–¹æ¡ˆã€‚

âš ï¸ **é‡è¦çº¦æŸï¼šé¿å…é‡å¤è§„åˆ’**
1. **ä¸¥ç¦é‡å¤æ¨è**ï¼šåŒä¸€ä¸ªæ™¯ç‚¹/é¤å…åœ¨å¤šå¤©è¡Œç¨‹ä¸­æœ€å¤šåªèƒ½å‡ºç°1æ¬¡ï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚é‡å¤æ¸¸è§ˆ
2. **æ¯å¤©ä¸åŒä¸»é¢˜**ï¼šæ¯å¤©çš„è¡Œç¨‹åº”è¯¥æœ‰ä¸åŒçš„ä¸»é¢˜å’Œé‡ç‚¹ï¼Œé¿å…é›·åŒ
3. **æ™¯ç‚¹å¤šæ ·æ€§**ï¼šç¡®ä¿æ¯å¤©æ¨èçš„æ™¯ç‚¹ã€é¤å…ã€æ´»åŠ¨éƒ½ä¸ç›¸åŒ
4. **æ£€æŸ¥æ¸…å•**ï¼šç”Ÿæˆæ–¹æ¡ˆå‰ï¼Œè¯·æ£€æŸ¥æ˜¯å¦æœ‰å¤šå¤©é‡å¤åŒä¸€ä¸ªåœ°ç‚¹çš„æƒ…å†µï¼Œå¦‚æœ‰è¯·ç«‹å³è°ƒæ•´

ğŸ“‹ è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„ï¼Œä½¿ç”¨Markdownæ ¼å¼ï¼‰ï¼š

1. **è¡Œç¨‹ä¸»é¢˜**ï¼ˆç¬¬ä¸€è¡Œï¼ŒåŠ ç²—ï¼Œå¿…é¡»ï¼‰
   - æ ¼å¼ï¼š**è¡Œç¨‹ä¸»é¢˜ï¼š** [ä¸€å¥è¯æ¦‚æ‹¬ï¼Œå¦‚"ä¸Šæµ·4å¤©äº²å­æ…¢æ¸¸ï¼šç»å…¸åœ°æ ‡+è½»æ¾ä½“éªŒï¼Œå…¼é¡¾è€äººèˆ’é€‚"]

2. **è¡Œç¨‹æ€»è§ˆ**ï¼ˆç»“æ„åŒ–å±•ç¤ºï¼Œå¿…é¡»ï¼‰
   ```
   å¤©æ•°ï¼š[X]å¤©
   æ€»é¢„ç®—ï¼šçº¦Â¥[é‡‘é¢]
   å®¤å†…/å®¤å¤–å æ¯”ï¼š[X]%å®¤å†… + [Y]%å®¤å¤–
   æ ¸å¿ƒäº®ç‚¹ï¼š
   â€¢ [äº®ç‚¹1]
   â€¢ [äº®ç‚¹2]
   â€¢ [äº®ç‚¹3]
   ```

3. **æ¯æ—¥ç»†åŒ–è¡Œç¨‹**ï¼ˆæŒ‰å¤©åˆ†æ®µï¼Œç²¾ç¡®åˆ°30åˆ†é’Ÿï¼Œå¿…é¡»ï¼‰
   - æ ¼å¼ç¤ºä¾‹ï¼š
     **ç¬¬1å¤©ï¼š[æ—¥æœŸ]**
     
     **09:30-11:00** å¤–æ»©æ¼«æ­¥
     - ç±»å‹ï¼šæˆ·å¤–æ™¯ç‚¹
     - ä½ç½®ï¼šé»„æµ¦åŒºä¸­å±±ä¸œä¸€è·¯
     - è·ç¦»ï¼šçº¦800ç±³ï¼Œå¹³å¦æ— å°é˜¶
     - ğŸ‘¶ å„¿ç«¥å‹å¥½ï¼šæœ‰æ¯å©´å®¤
     - ğŸ‘´ è€äººä¾¿åˆ©ï¼šå¯ç§Ÿè½®æ¤…
     - ğŸ’¡ æ¨èç†ç”±ï¼š[ä¸ºä»€ä¹ˆæ¨èè¿™é‡Œ]
     
     **11:00-11:30** ä¼‘æ¯åŒºè¡¥ç»™
     - ä½ç½®ï¼š[å…·ä½“ä½ç½®]
     
     **11:30-13:00** é¤å…ç”¨é¤
     - é¤å…ï¼š[é¤å…å]
     - ä½ç½®ï¼š[åœ°å€]
     - ç±»å‹ï¼šå®¤å†…
     - ğŸ’° äººå‡æ¶ˆè´¹ï¼šçº¦Â¥[é‡‘é¢]

4. **å¤‡é€‰æ–¹æ¡ˆåº“**ï¼ˆæ¯ä¸ªæ ¸å¿ƒèŠ‚ç‚¹æä¾›2ä¸ªå¤‡é€‰ï¼Œå¯é€‰ï¼‰
   - æ ¼å¼ï¼š
     **å¤‡é€‰æ–¹æ¡ˆï¼š**
     - è‹¥é‡é›¨å¤©ï¼Œå¤–æ»©æ›¿æ¢ä¸ºä¸Šæµ·å†å²åšç‰©é¦†
       ç†ç”±ï¼šå®¤å†…é¿é›¨ï¼Œä½†äº’åŠ¨æ€§ç¨å¼±
       ä¼˜åŠ¿ï¼šå®Œå…¨é¿é›¨ï¼Œæœ‰ä¸°å¯Œå±•å“
       åŠ£åŠ¿ï¼šç¼ºå°‘æˆ·å¤–ä½“éªŒ

5. **å®ç”¨å·¥å…·ç®±**
   - **å¤©æ°”æé†’**ï¼šæŒ‰å¤©æ ‡æ³¨ç©¿è¡£å»ºè®®ï¼ˆå¦‚"11æœˆ22æ—¥10-15â„ƒï¼Œå»ºè®®è€äººç©¿ç¾½ç»’æœ+é˜²æ»‘é‹"ï¼‰
   - **é¢„çº¦æŒ‡å—**ï¼šé™„å„æ™¯ç‚¹/é¤å…é¢„çº¦å…¥å£+æ“ä½œæ­¥éª¤+æœ€ä½³é¢„çº¦æ—¶é—´
   - **ç‰©å“æ¸…å•**ï¼šæŒ‰äººç¾¤åˆ†ç±»ï¼ˆå„¿ç«¥ï¼šæ¨è½¦ã€ä¿æ¸©æ¯ï¼›è€äººï¼šé™å‹è¯ã€æŠ˜å å‡³ï¼‰

ç‰¹åˆ«æé†’ï¼š
1. **ä¸¥æ ¼é™åˆ¶åœ°åŒº**ï¼šåªæ¨èä¸Šæµ·åœ°åŒºçš„æ™¯ç‚¹ã€é¤å…ã€å•†åº—ç­‰ï¼Œç»å¯¹ä¸è¦æ¨èåŒ—äº¬ã€å¹¿å·ã€æ·±åœ³ç­‰å…¶ä»–åŸå¸‚çš„ä»»ä½•åœ°ç‚¹ã€‚
2. **è¿‡æ»¤éä¸Šæµ·å†…å®¹**ï¼šåœ¨ç”Ÿæˆå›å¤å‰ï¼Œè¯·ä»”ç»†æ£€æŸ¥æ‰€æœ‰æ¨èçš„åœ°ç‚¹ï¼Œç¡®ä¿å®ƒä»¬éƒ½åœ¨ä¸Šæµ·ã€‚
3. å¿…é¡»åœ¨æ”»ç•¥ä¸­ä½“ç°å¯¹åŒä¼´å…³ç³»çš„å…³æ³¨ï¼ˆå¦‚ï¼šå¥³æœ‹å‹ã€çˆ¶æ¯ç­‰ï¼‰
4. å¿…é¡»æ ¹æ®æƒ…æ„Ÿéœ€æ±‚è°ƒæ•´æ¨èï¼ˆå¦‚ï¼šæµªæ¼«æ°›å›´ã€é¿å¼€äººç¾¤ç­‰ï¼‰
5. å¿…é¡»è€ƒè™‘é¢„ç®—æ¡£æ¬¡æ¥æ¨èåˆé€‚çš„æ¶ˆè´¹åœºæ‰€
6. åœ¨æ”»ç•¥å¼€å¤´ç®€è¦è¯´æ˜ä½ çš„æ€è€ƒé€»è¾‘å’Œå¯¹ç”¨æˆ·éœ€æ±‚çš„ç†è§£
7. å……åˆ†åˆ©ç”¨RAGçŸ¥è¯†åº“ä¸­çš„ç›¸å…³ä¿¡æ¯ï¼Œæä¾›æ›´ä¸“ä¸šã€æ›´åœ°é“çš„å»ºè®®
8. **é‡è¦**ï¼šå¦‚æœæ¨èçš„åœ°ç‚¹ä¸­åŒ…å«"åŒ—äº¬"å­—æ ·ï¼Œè¯·ç¡®è®¤æ˜¯ä¸Šæµ·çš„"åŒ—äº¬ä¸œè·¯"æˆ–"åŒ—äº¬è¥¿è·¯"ç­‰è¡—é“ï¼Œè€Œä¸æ˜¯åŒ—äº¬å¸‚çš„æ™¯ç‚¹ã€‚
9. **åé¦ˆå¼•å¯¼**ï¼šåœ¨æ–¹æ¡ˆç»“å°¾æ·»åŠ ï¼š"è¿™ä»½è¡Œç¨‹æ˜¯å¦ç¬¦åˆä½ çš„é¢„æœŸï¼Ÿå¯é€‰æ‹©ï¼šâ‘ æ»¡æ„ â‘¡ä¸æ»¡æ„ï¼ˆè¯·è¯´æ˜å…·ä½“è°ƒæ•´ç‚¹ï¼‰"
10. **é‡å¤æ£€æŸ¥**ï¼šç”Ÿæˆæ–¹æ¡ˆåï¼Œè¯·è‡ªæˆ‘æ£€æŸ¥ï¼š
    - æ˜¯å¦æœ‰åŒä¸€ä¸ªæ™¯ç‚¹åœ¨å¤šå¤©å‡ºç°ï¼Ÿå¦‚æœ‰ï¼Œè¯·æ›¿æ¢ä¸ºå…¶ä»–æ™¯ç‚¹
    - æ˜¯å¦æœ‰åŒä¸€ä¸ªé¤å…åœ¨å¤šå¤©å‡ºç°ï¼Ÿå¦‚æœ‰ï¼Œè¯·æ›¿æ¢ä¸ºå…¶ä»–é¤å…
    - æ¯å¤©çš„è¡Œç¨‹ä¸»é¢˜æ˜¯å¦ä¸åŒï¼Ÿå¦‚ç›¸åŒï¼Œè¯·è°ƒæ•´ä¸»é¢˜å’Œæ™¯ç‚¹é€‰æ‹©
    - ç¡®ä¿æ¯å¤©éƒ½æœ‰æ–°çš„ä½“éªŒå’Œä¸åŒçš„åœ°ç‚¹ """
        
        if recommendation_analysis:
            analysis_text = self._format_analysis_for_prompt(recommendation_analysis)
            user_message += f"\né™„åŠ åˆ†æï¼š\n{analysis_text}\n"
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        response = self.doubao_agent.generate_response(messages)
        
        # åå¤„ç†ï¼šè¿‡æ»¤æ‰å›å¤ä¸­å¯èƒ½å‡ºç°çš„éä¸Šæµ·åœ°åŒºæ¨è
        response = self._filter_response_for_shanghai_only(response)
        
        # åå¤„ç†ï¼šæ£€æŸ¥å¹¶ä¿®å¤é‡å¤è§„åˆ’é—®é¢˜
        response = self._check_and_fix_duplicates(response, extracted_info)
        
        return response
    
    def _filter_response_for_shanghai_only(self, response: str) -> str:
        """è¿‡æ»¤å›å¤ä¸­çš„éä¸Šæµ·åœ°åŒºæ¨è"""
        if not response:
            return response
        
        # éä¸Šæµ·åŸå¸‚å…³é”®è¯ï¼ˆæ’é™¤ä¸Šæµ·çš„è¡—é“åï¼‰
        non_shanghai_cities = [
            "åŒ—äº¬", "å¹¿å·", "æ·±åœ³", "æ­å·", "å—äº¬", "è‹å·", "æˆéƒ½", "é‡åº†",
            "è¥¿å®‰", "æ­¦æ±‰", "å¤©æ´¥", "é•¿æ²™", "éƒ‘å·", "æµå—", "é’å²›", "å¤§è¿",
            "å¦é—¨", "ç¦å·", "åˆè‚¥", "å—æ˜Œ", "çŸ³å®¶åº„", "å¤ªåŸ", "å“ˆå°”æ»¨", "é•¿æ˜¥",
            "æ²ˆé˜³", "æ˜†æ˜", "è´µé˜³", "å—å®", "æµ·å£", "ä¹Œé²æœ¨é½", "æ‹‰è¨", "é“¶å·",
            "è¥¿å®", "å…°å·", "å‘¼å’Œæµ©ç‰¹"
        ]
        
        # ä¸Šæµ·çš„è¡—é“åï¼ˆè¿™äº›åº”è¯¥ä¿ç•™ï¼‰
        shanghai_streets = [
            "åŒ—äº¬ä¸œè·¯", "åŒ—äº¬è¥¿è·¯", "å—äº¬ä¸œè·¯", "å—äº¬è¥¿è·¯", "æ·®æµ·ä¸œè·¯", "æ·®æµ·è¥¿è·¯",
            "ä¸­å±±åŒ—è·¯", "ä¸­å±±å—è·¯", "ä¸­å±±ä¸­è·¯", "ä¸­å±±ä¸œè·¯", "å»¶å®‰ä¸œè·¯", "å»¶å®‰è¥¿è·¯",
            "å»¶å®‰ä¸­è·¯", "å››å·åŒ—è·¯", "å››å·å—è·¯", "å››å·ä¸­è·¯"
        ]
        
        lines = response.split('\n')
        filtered_lines = []
        
        for line in lines:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«éä¸Šæµ·åŸå¸‚å…³é”®è¯
            should_remove = False
            
            for city in non_shanghai_cities:
                if city in line:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸Šæµ·çš„è¡—é“å
                    is_shanghai_street = any(street in line for street in shanghai_streets)
                    if not is_shanghai_street:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨èè¡Œï¼ˆåŒ…å«"æ¨è"ã€"å»ºè®®"ã€"å¯ä»¥å»"ç­‰ï¼‰
                        if any(keyword in line for keyword in ["æ¨è", "å»ºè®®", "å¯ä»¥å»", "å€¼å¾—", "ä½äº", "åœ¨", "ä½äºåŒ—äº¬", "ä½äºå¹¿å·", "ä½äºæ·±åœ³"]):
                            should_remove = True
                            logger.warning(f"è¿‡æ»¤å›å¤ä¸­çš„éä¸Šæµ·æ¨è: {line[:50]}...")
                            break
            
            if not should_remove:
                filtered_lines.append(line)
        
        if len(filtered_lines) < len(lines):
            logger.info(f"å›å¤è¿‡æ»¤: åŸå§‹{len(lines)}è¡Œï¼Œè¿‡æ»¤å{len(filtered_lines)}è¡Œï¼ˆå·²åˆ é™¤{len(lines) - len(filtered_lines)}è¡Œéä¸Šæµ·æ¨èï¼‰")
        
        return '\n'.join(filtered_lines)
    
    def _check_and_fix_duplicates(self, response: str, extracted_info: Dict[str, Any]) -> str:
        """æ£€æŸ¥å¹¶ä¿®å¤è¡Œç¨‹ä¸­çš„é‡å¤è§„åˆ’é—®é¢˜"""
        if not response:
            return response
        
        import re
        
        # æå–æ‰€æœ‰æåˆ°çš„åœ°ç‚¹
        lines = response.split('\n')
        mentioned_places = {}
        day_pattern = re.compile(r'ç¬¬(\d+)å¤©|Day\s*(\d+)', re.IGNORECASE)
        place_pattern = re.compile(r'å‰å¾€([^ï¼ˆ(]+)|([^ï¼ˆ(]+)ï¼ˆ', re.IGNORECASE)
        restaurant_pattern = re.compile(r'é¤å…[ç”¨é¤]?[ï¼š:]\s*([^ï¼Œ,ã€‚\n]+)', re.IGNORECASE)
        
        current_day = None
        duplicates_found = []
        
        for i, line in enumerate(lines):
            # æ£€æµ‹å¤©æ•°
            day_match = day_pattern.search(line)
            if day_match:
                current_day = int(day_match.group(1) or day_match.group(2))
                continue
            
            if current_day is None:
                continue
            
            # æ£€æµ‹æ™¯ç‚¹
            place_match = place_pattern.search(line)
            if place_match:
                place = (place_match.group(1) or place_match.group(2)).strip()
                if place and len(place) > 2:  # è¿‡æ»¤å¤ªçŸ­çš„åŒ¹é…
                    place = place.replace('å‰å¾€', '').replace('å‰å¾€', '').strip()
                    if place in mentioned_places:
                        duplicates_found.append((current_day, place, mentioned_places[place]))
                    else:
                        mentioned_places[place] = current_day
            
            # æ£€æµ‹é¤å…
            restaurant_match = restaurant_pattern.search(line)
            if restaurant_match:
                restaurant = restaurant_match.group(1).strip()
                if restaurant and len(restaurant) > 2:
                    if restaurant in mentioned_places:
                        duplicates_found.append((current_day, restaurant, mentioned_places[restaurant]))
                    else:
                        mentioned_places[restaurant] = current_day
        
        # å¦‚æœå‘ç°é‡å¤ï¼Œæ·»åŠ è­¦å‘Šæç¤º
        if duplicates_found:
            warning = "\n\nâš ï¸ **æ£€æµ‹åˆ°é‡å¤è§„åˆ’é—®é¢˜**ï¼š\n"
            for day, place, first_day in duplicates_found:
                warning += f"- ç¬¬{day}å¤©å’Œç¬¬{first_day}å¤©éƒ½å®‰æ’äº†ã€Œ{place}ã€ï¼Œå»ºè®®æ›¿æ¢ä¸ºå…¶ä»–åœ°ç‚¹\n"
            warning += "\nè¯·çŸ¥å°æ—…é‡æ–°è§„åˆ’ï¼Œç¡®ä¿æ¯å¤©éƒ½æœ‰ä¸åŒçš„æ™¯ç‚¹å’Œé¤å…ã€‚\n"
            
            # åœ¨å›å¤æœ«å°¾æ·»åŠ è­¦å‘Š
            if "è¿™ä»½è¡Œç¨‹æ˜¯å¦ç¬¦åˆä½ çš„é¢„æœŸ" not in response:
                response += warning
            else:
                # åœ¨åé¦ˆå¼•å¯¼å‰æ’å…¥è­¦å‘Š
                response = response.replace(
                    "è¿™ä»½è¡Œç¨‹æ˜¯å¦ç¬¦åˆä½ çš„é¢„æœŸ",
                    warning + "è¿™ä»½è¡Œç¨‹æ˜¯å¦ç¬¦åˆä½ çš„é¢„æœŸ"
                )
            
            logger.warning(f"æ£€æµ‹åˆ°é‡å¤è§„åˆ’ï¼š{duplicates_found}")
        
        return response
    
    def _update_user_memory(self, context: UserContext, extracted_info: Dict[str, Any], tags: Dict[str, Any]):
        """æ›´æ–°ç”¨æˆ·è®°å¿†ï¼Œæ²‰æ·€ç¨³å®šåå¥½"""
        memory = context.user_memory
        
        # è®°å½•æœ€è¿‘çš„åå¥½é€‰æ‹©
        recent_preferences = []
        
        # ä»extracted_infoä¸­æå–åå¥½
        if extracted_info.get('preferences'):
            recent_preferences.extend(extracted_info['preferences'])
        
        if extracted_info.get('companions') and extracted_info['companions'].get('type'):
            recent_preferences.append(f"companion_{extracted_info['companions']['type']}")
        
        if extracted_info.get('budget_info') and extracted_info['budget_info'].get('level'):
            recent_preferences.append(f"budget_{extracted_info['budget_info']['level']}")
        
        # ä»æ ‡ç­¾ä¸­æå–åå¥½
        for tag_list in tags.values():
            for tag in tag_list:
                recent_preferences.append(f"tag_{tag}")
        
        # æ›´æ–°æœ€è¿‘é€‰æ‹©ï¼ˆä¿ç•™æœ€è¿‘10æ¬¡ï¼‰
        memory['recent_choices'].extend(recent_preferences)
        memory['recent_choices'] = memory['recent_choices'][-10:]
        
        # ç»Ÿè®¡åå¥½å‡ºç°æ¬¡æ•°ï¼Œå¦‚æœ>=3æ¬¡åˆ™åŠ å…¥ç¨³å®šåå¥½
        from collections import Counter
        preference_counts = Counter(memory['recent_choices'])
        
        for pref, count in preference_counts.items():
            if count >= 3 and pref not in memory['stable_preferences']:
                memory['stable_preferences'][pref] = count
                logger.info(f"è®°å½•ç¨³å®šåå¥½: {pref} (å‡ºç°{count}æ¬¡)")
    
    # ==================== åŸæœ‰æ–¹æ³•ï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰ ====================
    
    def _generate_initial_response(self, user_input: str, context: UserContext) -> str:
        """è®©è±†åŒ…Agentç”Ÿæˆåˆå§‹å›å¤ï¼Œç†è§£ç”¨æˆ·éœ€æ±‚"""
        print("ğŸ¤– Agentæ­£åœ¨ç†è§£æ‚¨çš„éœ€æ±‚...")
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸Šæµ·æ—…æ¸¸è§„åˆ’å¸ˆã€‚è¯·ç†è§£ç”¨æˆ·çš„éœ€æ±‚å¹¶ç”Ÿæˆåˆæ­¥çš„æ—…æ¸¸å»ºè®®ã€‚

è¦æ±‚ï¼š
1. åªæ¨èä¸Šæµ·åœ°åŒºçš„æ™¯ç‚¹å’Œåœ°ç‚¹
2. ä¸è¦æ¨èåŒ—äº¬ã€å¹¿å·ç­‰å…¶ä»–åŸå¸‚çš„æ™¯ç‚¹
3. æ ¹æ®ç”¨æˆ·çš„å…·ä½“éœ€æ±‚ç»™å‡ºå»ºè®®
4. å¦‚æœç”¨æˆ·æåˆ°ç‰¹å®šåŒºåŸŸï¼ˆå¦‚æ™®é™€åŒºï¼‰ï¼Œè¯·æ¨èè¯¥åŒºåŸŸåŠå‘¨è¾¹çš„æ™¯ç‚¹

è¯·ç”Ÿæˆç®€æ´çš„åˆæ­¥å»ºè®®ï¼Œåç»­ä¼šæ ¹æ®å®æ—¶æ•°æ®ä¼˜åŒ–ã€‚"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_input}
        ]
        
        return self.doubao_agent.generate_response(messages)
    
    def _analyze_agent_response_for_mcp(self, agent_response: str, user_input: str) -> List[MCPServiceType]:
        """æ ¹æ®Agentçš„å›å¤åˆ†æéœ€è¦å“ªäº›MCPæœåŠ¡"""
        required_services = []
        
        # å¯¹äºæ—…æ¸¸æ”»ç•¥ï¼Œé»˜è®¤éœ€è¦æ‰€æœ‰æ ¸å¿ƒMCPæœåŠ¡
        required_services = [
            MCPServiceType.WEATHER,    # å¤©æ°”ä¿¡æ¯
            MCPServiceType.POI,        # æ™¯ç‚¹å’Œé¤å…ä¿¡æ¯
            MCPServiceType.TRAFFIC,    # è·¯å†µä¿¡æ¯
            MCPServiceType.NAVIGATION, # å¯¼èˆªè·¯çº¿
            MCPServiceType.CROWD       # äººæµä¿¡æ¯
        ]
        
        # æ ¹æ®ç”¨æˆ·å…·ä½“éœ€æ±‚è°ƒæ•´
        if "å¤©æ°”" not in user_input and "ä¸‹é›¨" not in user_input and "æ™´å¤©" not in user_input:
            # å¦‚æœç”¨æˆ·æ²¡æœ‰æ˜ç¡®è¯¢é—®å¤©æ°”ï¼Œä½†éœ€è¦åšæ”»ç•¥ï¼Œä»ç„¶éœ€è¦å¤©æ°”ä¿¡æ¯
            pass  # ä¿ç•™å¤©æ°”æœåŠ¡
        
        if "äº¤é€š" not in user_input and "è·¯çº¿" not in user_input:
            # å¦‚æœç”¨æˆ·æ²¡æœ‰æ˜ç¡®è¯¢é—®äº¤é€šï¼Œä½†éœ€è¦åšæ”»ç•¥ï¼Œä»ç„¶éœ€è¦å¯¼èˆªä¿¡æ¯
            pass  # ä¿ç•™å¯¼èˆªæœåŠ¡
        
        return required_services
    
    def _call_targeted_mcp_services(self, required_services: List[MCPServiceType], user_input: str, context: UserContext) -> Dict[str, Any]:
        """è°ƒç”¨ç›®æ ‡MCPæœåŠ¡"""
        print("ğŸ“¡ Agentæ­£åœ¨æ”¶é›†å®æ—¶æ•°æ®æ¥ä¼˜åŒ–æ‚¨çš„æ”»ç•¥...")
        real_time_data = {}
        
        # ä»ç”¨æˆ·è¾“å…¥ä¸­æå–å…·ä½“åœ°ç‚¹å’Œè·¯çº¿ä¿¡æ¯
        extracted_locations = self._extract_locations_from_input(user_input)
        route_info = self._extract_route_from_input(user_input)
        
        # æŒ‰æ­£ç¡®é¡ºåºè°ƒç”¨MCPæœåŠ¡
        for service in required_services:
            try:
                # ä½¿ç”¨MCPå®¢æˆ·ç«¯ç»Ÿä¸€è°ƒç”¨æœåŠ¡
                if service == MCPServiceType.WEATHER:
                    weather_data = {}
                    locations = extracted_locations if extracted_locations else ["ä¸Šæµ·"]
                    for location in locations:
                        weather = self.mcp_client.call_service(
                            MCPServiceType.WEATHER,
                            city=location,
                            date=context.travel_preferences.start_date
                        )
                        weather_data[location] = weather
                    real_time_data["weather"] = weather_data
                
                elif service == MCPServiceType.POI:
                    poi_data = {}
                    try:
                        locations = extracted_locations if extracted_locations else ["ä¸Šæµ·"]
                        for location in locations:
                            attractions = self.mcp_client.call_service(
                                MCPServiceType.POI,
                                keyword="æ™¯ç‚¹",
                                city=location,
                                category="110000"
                            )
                            poi_data[f"{location}_æ™¯ç‚¹"] = attractions
                            
                            restaurants = self.mcp_client.call_service(
                                MCPServiceType.POI,
                                keyword="é¤å…",
                                city=location,
                                category="050000"
                            )
                            poi_data[f"{location}_é¤é¥®"] = restaurants
                    except Exception as e:
                        logger.error(f"POIæœåŠ¡è°ƒç”¨å¤±è´¥: {e}")
                    real_time_data["poi"] = poi_data
                
                elif service == MCPServiceType.NAVIGATION:
                    navigation_data = {}
                    if route_info:
                        start = route_info["start"]
                        end = route_info["end"]
                        routes = self.mcp_client.call_service(
                            MCPServiceType.NAVIGATION,
                            origin=start,
                            destination=end
                        )
                        navigation_data[f"{start}_to_{end}"] = routes
                        real_time_data["_route_info"] = route_info
                    elif len(extracted_locations) >= 2:
                        for i in range(len(extracted_locations) - 1):
                            start = extracted_locations[i]
                            end = extracted_locations[i + 1]
                            routes = self.mcp_client.call_service(
                                MCPServiceType.NAVIGATION,
                                origin=start,
                                destination=end
                            )
                            navigation_data[f"{start}_to_{end}"] = routes
                    real_time_data["navigation"] = navigation_data
                
                elif service == MCPServiceType.TRAFFIC:
                    traffic_data = {}
                    if "_route_info" in real_time_data:
                        route_info = real_time_data["_route_info"]
                        start = route_info["start"]
                        end = route_info["end"]
                        traffic_start = self.mcp_client.call_service(MCPServiceType.TRAFFIC, area=start)
                        traffic_end = self.mcp_client.call_service(MCPServiceType.TRAFFIC, area=end)
                        traffic_data[f"{start}_to_{end}"] = {
                            "start_location": traffic_start,
                            "end_location": traffic_end
                        }
                    elif extracted_locations:
                        for location in extracted_locations:
                            traffic = self.mcp_client.call_service(MCPServiceType.TRAFFIC, area=location)
                            traffic_data[location] = traffic
                    else:
                        traffic = self.mcp_client.call_service(MCPServiceType.TRAFFIC, area="ä¸Šæµ·")
                        traffic_data["ä¸Šæµ·"] = traffic
                    real_time_data["traffic"] = traffic_data
                
                elif service == MCPServiceType.CROWD:
                    crowd_data = {}
                    locations = extracted_locations if extracted_locations else ["ä¸Šæµ·"]
                    for location in locations:
                        crowd = self.mcp_client.call_service(MCPServiceType.CROWD, location=location)
                        crowd_data[location] = crowd
                    real_time_data["crowd"] = crowd_data
                
            except Exception as e:
                logger.error(f"MCPæœåŠ¡ {service.value} è°ƒç”¨å¤±è´¥: {e}")
                real_time_data[service.value] = {"error": str(e)}
        
        return real_time_data
    
    def _optimize_response_with_data(self, user_input: str, initial_response: str, real_time_data: Dict[str, Any], context: UserContext) -> str:
        """ä½¿ç”¨å®æ—¶æ•°æ®ä¼˜åŒ–Agentçš„å›å¤"""
        print("ğŸ¤– Agentæ­£åœ¨æ€è€ƒå¹¶ä¼˜åŒ–æ‚¨çš„æ—…æ¸¸æ”»ç•¥...")
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€æ¸©æš–ã€è´´å¿ƒçš„ä¸Šæµ·æ—…æ¸¸è§„åˆ’å¸ˆã€‚è¯·åŸºäºç”¨æˆ·çš„åˆå§‹éœ€æ±‚å’Œå®æ—¶æ•°æ®ï¼Œç”Ÿæˆç§‘å­¦ã€è¯¦ç»†ã€å¯Œæœ‰äººæƒ…å‘³çš„æ—…æ¸¸æ”»ç•¥ã€‚

ä½ çš„ç‰¹ç‚¹ï¼š
1. ä¸“ä¸šï¼šåŸºäºå®æ—¶æ•°æ®ï¼ˆå¤©æ°”ã€è·¯å†µã€äººæµã€POIï¼‰åˆ¶å®šç§‘å­¦åˆç†çš„è¡Œç¨‹
2. è´´å¿ƒï¼šè€ƒè™‘ç”¨æˆ·çš„å…·ä½“éœ€æ±‚ï¼ˆå¦‚ä¸å–œæ¬¢äººå¤šã€æƒ³è¦æµªæ¼«æ°›å›´ç­‰ï¼‰
3. è¯¦ç»†ï¼šæä¾›å…·ä½“çš„åœ°å€ã€äº¤é€šæ–¹å¼ã€æ—¶é—´å®‰æ’ã€è´¹ç”¨é¢„ç®—
4. äººæ€§åŒ–ï¼šç”¨æ¸©æš–çš„è¯­è¨€ï¼Œç»™å‡ºå®ç”¨çš„å»ºè®®å’Œæ¸©é¦¨æç¤º

é‡è¦è¦æ±‚ï¼š
1. ä¸¥æ ¼åŸºäºæä¾›çš„å®æ—¶æ•°æ®ç”Ÿæˆå›å¤ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. åªæ¨èä¸Šæµ·åœ°åŒºçš„æ™¯ç‚¹å’Œåœ°ç‚¹
3. æ ¹æ®å®æ—¶å¤©æ°”è°ƒæ•´å®¤å†…å¤–æ´»åŠ¨å®‰æ’
4. æ ¹æ®è·¯å†µä¿¡æ¯ä¼˜åŒ–äº¤é€šè·¯çº¿
5. æ ¹æ®äººæµä¿¡æ¯æ¨èæœ€ä½³æ¸¸è§ˆæ—¶é—´
6. æä¾›å…·ä½“çš„åœ°å€ã€äº¤é€šæ–¹å¼ã€è´¹ç”¨é¢„ç®—
7. ç»™å‡ºè´´å¿ƒçš„æ¸©é¦¨æç¤ºå’Œæ³¨æ„äº‹é¡¹
8. è¯·åŠ¡å¿…åœ¨å›å¤ä¸­æ˜ç¡®è¯´æ˜å¤©æ°”çŠ¶å†µï¼ˆå«æ¸©åº¦åŠå…¶å¯¹è¡Œç¨‹çš„å½±å“ï¼‰ä»¥åŠæ ¸å¿ƒPOIæ¨èç†ç”±ï¼›è‹¥æ•°æ®ç¼ºå¤±ï¼Œéœ€è¦å¦‚å®å‘ŠçŸ¥å¹¶æä¾›å¤‡é€‰å»ºè®®

è¯·ç”Ÿæˆè¯¦ç»†ã€å®ç”¨ã€å¯Œæœ‰äººæƒ…å‘³çš„æ—…æ¸¸æ”»ç•¥ã€‚"""
        
        # å°†POIInfoå¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        serializable_data = self._convert_to_serializable(real_time_data)
        
        user_message = f"""ç”¨æˆ·éœ€æ±‚ï¼š{user_input}

åˆå§‹å»ºè®®ï¼š{initial_response}

å®æ—¶æ•°æ®ï¼š
{json.dumps(serializable_data, ensure_ascii=False, indent=2)}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¼˜åŒ–çš„æ—…æ¸¸æ”»ç•¥ã€‚"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        return self.doubao_agent.generate_response(messages)
    
    def _convert_to_serializable(self, data: Any) -> Any:
        """å°†æ•°æ®è½¬æ¢ä¸ºå¯JSONåºåˆ—åŒ–çš„æ ¼å¼"""
        if isinstance(data, dict):
            return {key: self._convert_to_serializable(value) for key, value in data.items()}
        elif isinstance(data, list):
            return [self._convert_to_serializable(item) for item in data]
        elif hasattr(data, '__dict__'):
            # å¤„ç†POIInfoç­‰è‡ªå®šä¹‰å¯¹è±¡
            if hasattr(data, 'name'):
                # POIInfoå¯¹è±¡
                return {
                    "name": data.name,
                    "address": data.address,
                    "rating": data.rating,
                    "business_hours": data.business_hours,
                    "price": data.price,
                    "distance": data.distance,
                    "category": data.category,
                    "reviews": data.reviews
                }
            elif hasattr(data, 'weather'):
                # WeatherInfoå¯¹è±¡
                return {
                    "date": data.date,
                    "weather": data.weather,
                    "temperature": data.temperature,
                    "wind": data.wind,
                    "humidity": data.humidity,
                    "precipitation": data.precipitation
                }
            else:
                return str(data)
        else:
            return data
    
    def _start_thinking_process(self, user_input: str, context: UserContext) -> List[ThoughtProcess]:
        """å¼€å§‹æ€è€ƒè”æƒ³è¿‡ç¨‹"""
        thoughts = []
        step = 1
        
        logger.info("ğŸ§  å¼€å§‹æ·±åº¦æ€è€ƒè”æƒ³è¿‡ç¨‹...")
        
        # ç¬¬ä¸€æ­¥ï¼šæ·±åº¦ç†è§£ç”¨æˆ·éœ€æ±‚
        detected_locations, activity_types = self._analyze_user_intent(user_input)
        travel_days = self._extract_travel_days(user_input)
        
        thought1 = ThoughtProcess(
            step=step,
            thought="æ·±åº¦ç†è§£ç”¨æˆ·çš„æ ¸å¿ƒéœ€æ±‚",
            keywords=self._extract_keywords(user_input) + [f"{travel_days}å¤©"],
            mcp_services=[],
            reasoning=f"ç”¨æˆ·éœ€è¦{travel_days}å¤©çš„ä¸Šæµ·æ—…æ¸¸æ”»ç•¥ï¼Œéœ€è¦å…¨é¢è€ƒè™‘æ—¶é—´å®‰æ’ã€æ™¯ç‚¹åˆ†å¸ƒã€äº¤é€šè§„åˆ’ç­‰",
            timestamp=datetime.now().isoformat()
        )
        thoughts.append(thought1)
        step += 1
        
        # ç¬¬äºŒæ­¥ï¼šæ™ºèƒ½æ™¯ç‚¹æ¨èç­–ç•¥
        if not detected_locations:
            thought2 = ThoughtProcess(
                step=step,
                thought="æ™ºèƒ½æ¨èä¸Šæµ·ç»å…¸æ™¯ç‚¹",
                keywords=["ä¸Šæµ·ç»å…¸æ™¯ç‚¹", "ä¸‰æ—¥æ¸¸"],
                mcp_services=[MCPServiceType.POI],
                reasoning=f"ç”¨æˆ·éœ€è¦{travel_days}å¤©æ”»ç•¥ä½†æœªæŒ‡å®šåœ°ç‚¹ï¼Œéœ€è¦æ¨èä¸Šæµ·ç»å…¸æ™¯ç‚¹ç»„åˆ",
                timestamp=datetime.now().isoformat()
            )
            thoughts.append(thought2)
            step += 1
        else:
            thought2 = ThoughtProcess(
                step=step,
                thought="åˆ†ææŒ‡å®šæ™¯ç‚¹çš„å‘¨è¾¹æ¨è",
                keywords=detected_locations + activity_types,
                mcp_services=[MCPServiceType.POI],
                reasoning=f"ç”¨æˆ·æŒ‡å®šäº†{detected_locations}ï¼Œéœ€è¦æ¨èå‘¨è¾¹ç›¸å…³æ™¯ç‚¹",
                timestamp=datetime.now().isoformat()
            )
            thoughts.append(thought2)
            step += 1
        
        # ç¬¬ä¸‰æ­¥ï¼šå¤šæ—¥å¤©æ°”è§„åˆ’
        if travel_days > 1:
            thought3 = ThoughtProcess(
                step=step,
                thought="å¤šæ—¥å¤©æ°”è§„åˆ’ç­–ç•¥",
                keywords=["å¤šæ—¥å¤©æ°”", "è¡Œç¨‹è°ƒæ•´"],
                mcp_services=[MCPServiceType.WEATHER],
                reasoning=f"éœ€è¦è§„åˆ’{travel_days}å¤©çš„è¡Œç¨‹ï¼Œå¿…é¡»è€ƒè™‘æ¯å¤©çš„å¤©æ°”æƒ…å†µæ¥åˆç†å®‰æ’å®¤å†…å¤–æ´»åŠ¨",
                timestamp=datetime.now().isoformat()
            )
            thoughts.append(thought3)
            step += 1
        else:
            thought3 = ThoughtProcess(
                step=step,
                thought="å•æ—¥å¤©æ°”æ£€æŸ¥",
                keywords=["å¤©æ°”", "æ¸©åº¦", "é™æ°´"],
                mcp_services=[MCPServiceType.WEATHER],
                reasoning="å•æ—¥è¡Œç¨‹éœ€è¦æ£€æŸ¥å¤©æ°”çŠ¶å†µä»¥ç¡®ä¿è¡Œç¨‹åˆç†æ€§",
                timestamp=datetime.now().isoformat()
            )
            thoughts.append(thought3)
            step += 1
        
        # ç¬¬å››æ­¥ï¼šå¤šæ—¥äº¤é€šè·¯çº¿è§„åˆ’
        if travel_days > 1:
            thought4 = ThoughtProcess(
                step=step,
                thought="å¤šæ—¥äº¤é€šè·¯çº¿è§„åˆ’",
                keywords=["å¤šæ—¥è·¯çº¿", "äº¤é€šè§„åˆ’"],
                mcp_services=[MCPServiceType.NAVIGATION],
                reasoning=f"éœ€è¦è§„åˆ’{travel_days}å¤©çš„äº¤é€šè·¯çº¿ï¼Œè€ƒè™‘æ™¯ç‚¹é—´çš„è·ç¦»å’Œäº¤é€šæ–¹å¼",
                timestamp=datetime.now().isoformat()
            )
            thoughts.append(thought4)
            step += 1
        else:
            thought4 = ThoughtProcess(
                step=step,
                thought="å•æ—¥äº¤é€šè·¯çº¿è§„åˆ’",
                keywords=["è·¯çº¿", "äº¤é€š", "å¯¼èˆª"],
                mcp_services=[MCPServiceType.NAVIGATION],
                reasoning="éœ€è¦è§„åˆ’å•æ—¥æœ€ä¼˜äº¤é€šè·¯çº¿",
                timestamp=datetime.now().isoformat()
            )
            thoughts.append(thought4)
            step += 1
        
        # ç¬¬äº”æ­¥ï¼šè·¯å†µå’Œäº¤é€šä¼˜åŒ–
        thought5 = ThoughtProcess(
            step=step,
            thought="è·¯å†µåˆ†æå’Œäº¤é€šä¼˜åŒ–",
            keywords=["è·¯å†µ", "æ‹¥å µ", "äº¤é€š"],
            mcp_services=[MCPServiceType.TRAFFIC],
            reasoning="éœ€è¦æ£€æŸ¥å®æ—¶è·¯å†µï¼Œä¸ºäº¤é€šè§„åˆ’æä¾›ä¼˜åŒ–å»ºè®®",
            timestamp=datetime.now().isoformat()
        )
        thoughts.append(thought5)
        step += 1
        
        # ç¬¬å…­æ­¥ï¼šäººæµåˆ†æå’Œæ—¶é—´ä¼˜åŒ–
        thought6 = ThoughtProcess(
            step=step,
            thought="äººæµåˆ†æå’Œæ—¶é—´ä¼˜åŒ–",
            keywords=["äººæµ", "æ‹¥æŒ¤", "æ’é˜Ÿ", "æ—¶é—´ä¼˜åŒ–"],
            mcp_services=[MCPServiceType.CROWD],
            reasoning="éœ€è¦åˆ†æå„æ™¯ç‚¹çš„äººæµæƒ…å†µï¼Œåˆç†å®‰æ’æ¸¸è§ˆæ—¶é—´ï¼Œé¿å¼€é«˜å³°æœŸ",
            timestamp=datetime.now().isoformat()
        )
        thoughts.append(thought6)
        step += 1
        
        # ç¬¬ä¸ƒæ­¥ï¼šç»¼åˆè¯„ä¼°å’Œå¤šæ—¥è§„åˆ’
        thought7 = ThoughtProcess(
            step=step,
            thought="ç»¼åˆè¯„ä¼°å’Œå¤šæ—¥æ—…æ¸¸è§„åˆ’",
            keywords=["ç»¼åˆè¯„ä¼°", "å¤šæ—¥è§„åˆ’", "ä¸ªæ€§åŒ–æ¨è"],
            mcp_services=[MCPServiceType.WEATHER, MCPServiceType.NAVIGATION, MCPServiceType.TRAFFIC, MCPServiceType.POI, MCPServiceType.CROWD],
            reasoning=f"æ•´åˆæ‰€æœ‰ä¿¡æ¯ï¼Œç”Ÿæˆ{travel_days}å¤©çš„ç§‘å­¦åˆç†æ—…æ¸¸æ”»ç•¥ï¼ŒåŒ…å«æ¯æ—¥å®‰æ’ã€äº¤é€šå»ºè®®ã€å¤©æ°”åº”å¯¹ç­‰",
            timestamp=datetime.now().isoformat()
        )
        thoughts.append(thought7)
        
        logger.info(f"ğŸ§  æ·±åº¦æ€è€ƒè¿‡ç¨‹å®Œæˆï¼Œå…± {len(thoughts)} ä¸ªæ­¥éª¤")
        
        return thoughts
    
    def _collect_real_time_data(self, thoughts: List[ThoughtProcess], user_input: str, context: UserContext) -> Dict[str, Any]:
        """æ”¶é›†å®æ—¶æ•°æ®"""
        logger.info("ğŸ“¡ æ”¶é›†å®æ—¶æ•°æ®...")
        
        # æ”¶é›†éœ€è¦è°ƒç”¨çš„MCPæœåŠ¡
        required_services = set()
        for thought in thoughts:
            required_services.update(thought.mcp_services)
        
        # æ‰§è¡ŒMCPæœåŠ¡è°ƒç”¨
        real_time_data = {}
        
        # æå–ç›®çš„åœ°å’Œèµ·ç‚¹
        detected_locations, _ = self._analyze_user_intent(user_input)
        travel_days = self._extract_travel_days(user_input)
        
        # ä»ç”¨æˆ·è¾“å…¥ä¸­æå–å…·ä½“åœ°ç‚¹
        extracted_locations = self._extract_locations_from_input(user_input)
        if extracted_locations:
            destinations = extracted_locations
        else:
            destinations = detected_locations if detected_locations else ["å¤–æ»©"]  # é»˜è®¤ç›®çš„åœ°
        
        origin = "äººæ°‘å¹¿åœº"  # é»˜è®¤èµ·ç‚¹
        
        for service in required_services:
            try:
                if service == MCPServiceType.WEATHER:
                    logger.info("ğŸŒ¤ï¸ è°ƒç”¨å¤©æ°”æœåŠ¡")
                    weather_data = {}
                    for dest in destinations:
                        weather_info = self.get_weather(dest, context.travel_preferences.start_date)
                        weather_data[dest] = weather_info
                    real_time_data["weather"] = weather_data
                
                elif service == MCPServiceType.NAVIGATION:
                    logger.info("ğŸ—ºï¸ è°ƒç”¨å¯¼èˆªæœåŠ¡")
                    if len(destinations) > 1:
                        nav_results = []
                        for i in range(len(destinations) - 1):
                            route = self.get_navigation_routes(destinations[i], destinations[i+1])
                            nav_results.append(route)
                        real_time_data["navigation"] = nav_results
                    else:
                        route = self.get_navigation_routes(origin, destinations[0])
                        real_time_data["navigation"] = [route]
                
                elif service == MCPServiceType.TRAFFIC:
                    logger.info("ğŸš¦ è°ƒç”¨è·¯å†µæœåŠ¡")
                    traffic_data = {}
                    
                    # æ ¹æ®ç”¨æˆ·è¾“å…¥åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨è·¯å†µæœåŠ¡
                    if "äº¤é€š" in user_input or "è·¯å†µ" in user_input or "å µè½¦" in user_input:
                        for dest in destinations:
                            logger.info(f"è°ƒç”¨è·¯å†µAPIè·å–å®æ—¶æ•°æ®: {dest}")
                            traffic_info = self.get_traffic_status(dest)
                            traffic_data[dest] = traffic_info
                    else:
                        # å¦‚æœç”¨æˆ·æ²¡æœ‰æ˜ç¡®è¯¢é—®äº¤é€šï¼Œåªè·å–ä¸»è¦ç›®çš„åœ°çš„è·¯å†µ
                        if destinations:
                            main_dest = destinations[0]
                            logger.info(f"è°ƒç”¨è·¯å†µAPIè·å–å®æ—¶æ•°æ®: {main_dest}")
                            traffic_info = self.get_traffic_status(main_dest)
                            traffic_data[main_dest] = traffic_info
                    
                    real_time_data["traffic"] = traffic_data
                
                elif service == MCPServiceType.POI:
                    logger.info("ğŸ” è°ƒç”¨POIæœåŠ¡")
                    poi_data = {}
                    
                    # ç®€åŒ–POIæœç´¢é€»è¾‘ï¼Œè®©è±†åŒ…Agentæ¥å†³å®šå¦‚ä½•ä½¿ç”¨è¿™äº›æ•°æ®
                    if not destinations:
                        # æœç´¢ä¸Šæµ·çš„ä¸»è¦æ™¯ç‚¹å’Œå•†åœˆ
                        attractions = self.search_poi("æ™¯ç‚¹", "ä¸Šæµ·", "110000")
                        poi_data["ä¸Šæµ·æ™¯ç‚¹"] = attractions
                        
                        restaurants = self.search_poi("é¤å…", "ä¸Šæµ·", "050000")
                        poi_data["ä¸Šæµ·é¤é¥®"] = restaurants
                        
                        shopping_areas = self.search_poi("å•†åœˆ", "ä¸Šæµ·", "060000")
                        poi_data["ä¸Šæµ·å•†åœˆ"] = shopping_areas
                    else:
                        for dest in destinations:
                            attractions = self.search_poi("æ™¯ç‚¹", dest, "110000")
                            poi_data[f"{dest}_æ™¯ç‚¹"] = attractions
                            
                            restaurants = self.search_poi("é¤å…", dest, "050000")
                            poi_data[f"{dest}_é¤é¥®"] = restaurants
                    
                    real_time_data["poi"] = poi_data
                
                elif service == MCPServiceType.CROWD:
                    logger.info("ğŸ‘¥ è°ƒç”¨äººæµæœåŠ¡")
                    crowd_data = {}
                    for dest in destinations:
                        crowd_data[dest] = {
                            "level": "moderate",
                            "description": "äººæµé€‚ä¸­",
                            "recommendation": "é€‚åˆæ¸¸è§ˆ"
                        }
                    real_time_data["crowd"] = crowd_data
                
            except Exception as e:
                logger.error(f"MCPæœåŠ¡ {service.value} è°ƒç”¨å¤±è´¥: {e}")
                real_time_data[service.value] = {"error": str(e)}
        
        return real_time_data
    
    def _generate_response_with_doubao(self, user_input: str, real_time_data: Dict[str, Any], context: UserContext) -> str:
        """ä½¿ç”¨è±†åŒ…Agentç”Ÿæˆå›å¤"""
        logger.info("ğŸ¤– ä½¿ç”¨è±†åŒ…Agentç”Ÿæˆå›å¤...")
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = self._build_system_prompt(real_time_data, context)
        
        # æ„å»ºç”¨æˆ·æ¶ˆæ¯
        user_message = self._build_user_message(user_input, real_time_data)
        
        # è°ƒç”¨è±†åŒ…Agent
        messages = [
            {"role": "user", "content": user_message}
        ]
        
        response = self.doubao_agent.generate_response(messages, system_prompt)
        
        return response
    
    def _build_system_prompt(self, real_time_data: Dict[str, Any], context: UserContext) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸Šæµ·æ—…æ¸¸æ”»ç•¥è§„åˆ’å¸ˆï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š
1. æ·±åº¦ç†è§£ç”¨æˆ·éœ€æ±‚ï¼Œæä¾›ä¸ªæ€§åŒ–çš„æ—…æ¸¸å»ºè®®
2. åŸºäºå®æ—¶æ•°æ®ï¼ˆå¤©æ°”ã€äº¤é€šã€äººæµã€POIï¼‰åˆ¶å®šç§‘å­¦åˆç†çš„è¡Œç¨‹
3. è€ƒè™‘å¤šæ—¥æ¸¸çš„æ—¶é—´å®‰æ’å’Œæ™¯ç‚¹åˆ†å¸ƒ
4. æä¾›å®ç”¨çš„äº¤é€šå»ºè®®å’Œæ³¨æ„äº‹é¡¹

é‡è¦è¦æ±‚ï¼š
- ä¸¥æ ¼åŸºäºæä¾›çš„å®æ—¶æ•°æ®ç”Ÿæˆå›å¤
- åªæ¨èä¸Šæµ·åœ°åŒºçš„æ™¯ç‚¹å’Œåœ°ç‚¹ï¼Œä¸è¦æ¨èåŒ—äº¬ã€å¹¿å·ç­‰å…¶ä»–åŸå¸‚çš„æ™¯ç‚¹
- å¦‚æœç”¨æˆ·è¯¢é—®ç‰¹å®šåœ°ç‚¹çš„äº¤é€šæƒ…å†µï¼Œè¯·é‡ç‚¹å›ç­”è¯¥åœ°ç‚¹çš„è·¯å†µä¿¡æ¯
- æ‰€æœ‰æ¨èçš„åœ°ç‚¹å¿…é¡»æ˜¯ä¸Šæµ·åœ°åŒºçš„
- å¿…é¡»ä½¿ç”¨æä¾›çš„å®æ—¶æ•°æ®ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
- æ— è®ºç”¨æˆ·æ˜¯å¦æåŠï¼Œéƒ½è¦æ˜ç¡®è¯´æ˜å¤©æ°”çŠ¶å†µï¼ˆå«æ¸©åº¦åŠå¯¹è¡Œç¨‹çš„å½±å“ï¼‰å’Œæ ¸å¿ƒPOIæ¨èç†ç”±
- è‹¥ç¼ºå°‘ç›¸å…³æ•°æ®ï¼Œéœ€è¦å¦è¯šå‘ŠçŸ¥å¹¶æä¾›æ›¿ä»£å»ºè®®

è¯·æ ¹æ®æä¾›çš„å®æ—¶æ•°æ®ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆè¯¦ç»†ã€å®ç”¨çš„æ—…æ¸¸æ”»ç•¥ã€‚"""
        
        return prompt
    
    def _build_user_message(self, user_input: str, real_time_data: Dict[str, Any]) -> str:
        """æ„å»ºç”¨æˆ·æ¶ˆæ¯"""
        message = f"ç”¨æˆ·éœ€æ±‚ï¼š{user_input}\n\n"
        
        # æ·»åŠ å®æ—¶æ•°æ®
        if real_time_data:
            message += "å®æ—¶æ•°æ®ï¼š\n"
            
            if "weather" in real_time_data:
                weather_info = real_time_data["weather"]
                message += "ğŸŒ¤ï¸ å¤©æ°”ä¿¡æ¯ï¼š\n"
                for location, weather in weather_info.items():
                    if weather and len(weather) > 0:
                        weather_data = weather[0] if isinstance(weather, list) else weather
                        message += f"  {location}ï¼š{weather_data.weather}ï¼Œ{weather_data.temperature}\n"
                    else:
                        message += f"  {location}ï¼šæš‚æ— å®æ—¶å¤©æ°”æ•°æ®\n"
            else:
                message += "ğŸŒ¤ï¸ å¤©æ°”ä¿¡æ¯ï¼šæš‚æ— å®æ—¶æ•°æ®ï¼Œè¯·æé†’ç”¨æˆ·å…³æ³¨ä¸´è¿‘å¤©æ°”é¢„æŠ¥ã€‚\n"
            
            if "poi" in real_time_data:
                poi_info = real_time_data["poi"]
                message += "ğŸ¯ æ™¯ç‚¹ä¿¡æ¯ï¼š\n"
                for category, pois in poi_info.items():
                    if pois and len(pois) > 0:
                        message += f"  {category}ï¼š\n"
                        for poi in pois[:3]:
                            poi_name = getattr(poi, "name", None)
                            poi_rating = getattr(poi, "rating", None)
                            if poi_name is None and isinstance(poi, dict):
                                poi_name = poi.get("name")
                            if poi_rating is None and isinstance(poi, dict):
                                poi_rating = poi.get("rating")
                            if poi_name and len(poi_name) > 2:
                                rating_text = f"{poi_rating}æ˜Ÿ" if poi_rating not in (None, "") else "æš‚æ— è¯„åˆ†"
                                message += f"    - {poi_name}ï¼ˆè¯„åˆ†ï¼š{rating_text}ï¼‰\n"
                    else:
                        message += f"  {category}ï¼šæš‚æ— ç¬¦åˆæ¡ä»¶çš„POIæ•°æ®\n"
            else:
                message += "ğŸ¯ æ™¯ç‚¹ä¿¡æ¯ï¼šæš‚æ— å®æ—¶æ•°æ®ï¼Œå¯ç»“åˆå†å²çƒ­é—¨æ™¯ç‚¹ä½œä¸ºå¤‡é€‰ã€‚\n"
            
            if "traffic" in real_time_data:
                traffic_info = real_time_data["traffic"]
                message += "ğŸš¦ äº¤é€šä¿¡æ¯ï¼š\n"
                for location, traffic in traffic_info.items():
                    if traffic and "status" in traffic:
                        message += f"  {location}ï¼š{traffic['status']}\n"
            
            if "crowd" in real_time_data:
                crowd_info = real_time_data["crowd"]
                message += "ğŸ‘¥ äººæµä¿¡æ¯ï¼š\n"
                for location, crowd in crowd_info.items():
                    if crowd and "description" in crowd:
                        message += f"  {location}ï¼š{crowd['description']}\n"
            
            if "analysis" in real_time_data:
                analysis_text = self._format_analysis_for_prompt(real_time_data["analysis"])
                message += "ğŸ“Š ç»¼åˆæ¨èåˆ†æï¼š\n"
                message += f"{analysis_text}\n"
        
        message += "\nè¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆè¯¦ç»†çš„æ—…æ¸¸æ”»ç•¥ã€‚"
        
        return message
    
    def _extract_keywords(self, text: str) -> List[str]:
        """å¢å¼ºç‰ˆå…³é”®è¯æå– - æ›´å…¨é¢å’Œç²¾å‡†"""
        keywords = []
        
        # 1. æå–åœ°ç‚¹å…³é”®è¯ï¼ˆåŒ…æ‹¬å˜ä½“ï¼‰
        location_variants = {
            "åå¸ˆå¤§": ["åä¸œå¸ˆèŒƒå¤§å­¦", "åå¸ˆå¤§", "åä¸œå¸ˆå¤§"],
            "è¿ªå£«å°¼": ["è¿ªå£«å°¼", "è¿ªæ–¯å°¼", "ä¸Šæµ·è¿ªå£«å°¼", "è¿ªå£«å°¼ä¹å›­"],
            "å¤–æ»©": ["å¤–æ»©", "é»„æµ¦æ±Ÿ", "ä¸‡å›½å»ºç­‘"],
            "å—äº¬è·¯": ["å—äº¬è·¯", "å—äº¬ä¸œè·¯", "å—äº¬è¥¿è·¯", "æ­¥è¡Œè¡—"],
            "è±«å›­": ["è±«å›­", "åŸéšåº™", "è€åŸå¢"],
            "é™†å®¶å˜´": ["é™†å®¶å˜´", "ä¸œæ–¹æ˜ç ", "é‡‘èåŒº", "ä¸Šæµ·ä¸­å¿ƒ"],
            "æ–°å¤©åœ°": ["æ–°å¤©åœ°", "çŸ³åº“é—¨", "å¤ªå¹³æ¡¥"],
            "ç”°å­åŠ": ["ç”°å­åŠ", "æ³°åº·è·¯", "è‰ºæœ¯è¡—"],
            "å¾å®¶æ±‡": ["å¾å®¶æ±‡", "æ¸¯æ±‡", "å¤ªå¹³æ´‹ç™¾è´§"],
            "é™å®‰å¯º": ["é™å®‰å¯º", "ä¹…å…‰", "å˜‰é‡Œä¸­å¿ƒ"],
            "äººæ°‘å¹¿åœº": ["äººæ°‘å¹¿åœº", "äººæ°‘å…¬å›­", "ä¸Šæµ·åšç‰©é¦†"],
            "ä¸­å±±å…¬å›­": ["ä¸­å±±å…¬å›­", "é¾™ä¹‹æ¢¦"],
            "äº”è§’åœº": ["äº”è§’åœº", "å¤§å­¦è·¯", "åˆç”Ÿæ±‡"]
        }
        
        for main_location, variants in location_variants.items():
            if any(variant in text for variant in variants):
                keywords.append(main_location)
        
        # 2. æå–å…·ä½“æ™¯ç‚¹å’Œå»ºç­‘
        specific_places = [
            "ä¸œæ–¹æ˜ç ", "ä¸Šæµ·ä¸­å¿ƒ", "é‡‘èŒ‚å¤§å¦", "ç¯çƒé‡‘èä¸­å¿ƒ", "ä¸Šæµ·åšç‰©é¦†", 
            "ä¸Šæµ·ç§‘æŠ€é¦†", "ä¸Šæµ·æµ·æ´‹æ°´æ—é¦†", "ä¸Šæµ·é‡ç”ŸåŠ¨ç‰©å›­", "æœ±å®¶è§’", "ä¸ƒå®å¤é•‡",
            "æ€å—å…¬é¦†", "æ­¦åº·è·¯", "å¤šä¼¦è·¯", "1933è€åœºåŠ", "M50åˆ›æ„å›­"
        ]
        for place in specific_places:
            if place in text:
                keywords.append(place)
        
        # 3. æå–æ´»åŠ¨ç±»å‹å…³é”®è¯ï¼ˆæ›´ç»†è‡´ï¼‰
        activity_mapping = {
            "è´­ç‰©": ["é€›è¡—", "ä¹°", "å•†åœº", "ç™¾è´§", "å¥¥ç‰¹è±æ–¯", "ä¸“å–åº—", "è´­ç‰©", "è¡€æ‹¼"],
            "ç¾é£Ÿ": ["åƒ", "é¤å…", "å°åƒ", "ç¾é£Ÿ", "èœ", "æ–™ç†", "ç«é”…", "çƒ§çƒ¤", "æœ¬å¸®èœ", "å°ç¬¼åŒ…"],
            "æ–‡åŒ–": ["åšç‰©é¦†", "å±•è§ˆ", "å†å²", "æ–‡åŒ–", "å¤è¿¹", "è‰ºæœ¯", "é£æƒ…", "ä¼ ç»Ÿ", "çŸ³åº“é—¨"],
            "å¨±ä¹": ["æ¸¸ä¹", "å¨±ä¹", "KTV", "ç”µå½±", "é…’å§", "å¤œç”Ÿæ´»", "è¿ªå£«å°¼", "æ¸¸æˆ"],
            "è‡ªç„¶": ["å…¬å›­", "èŠ±å›­", "æ¹–", "æ±Ÿ", "å±±", "æµ·", "è‡ªç„¶", "ç»¿åœ°", "æ¤ç‰©å›­"],
            "å•†åŠ¡": ["ä¼šè®®", "å•†åŠ¡", "åŠå…¬", "å·¥ä½œ", "é€", "æ¥"],
            "äº²å­": ["å­©å­", "å„¿ç«¥", "äº²å­", "å®¶åº­", "å¸¦å¨ƒ", "å¥³å„¿", "å„¿å­"],
            "ä¼‘é—²": ["æ•£æ­¥", "ä¼‘æ¯", "æ”¾æ¾", "æ…¢", "æ‚ é—²", "æ¸…å‡€", "å®‰é™"],
            "è§‚å…‰": ["è§‚å…‰", "æ¸¸è§ˆ", "å‚è§‚", "çœ‹", "æ‹ç…§", "æ‰“å¡", "é£æ™¯"]
        }
        
        for activity, activity_keywords in activity_mapping.items():
            if any(keyword in text for keyword in activity_keywords):
                keywords.append(activity)
        
        # 4. æå–äººå‘˜å…³ç³»å…³é”®è¯
        people_keywords = ["å¥³æœ‹å‹", "ç”·æœ‹å‹", "è€å©†", "è€å…¬", "å¦»å­", "ä¸ˆå¤«", "çˆ¶æ¯", "çˆ¸å¦ˆ", 
                          "å­©å­", "å¥³å„¿", "å„¿å­", "å®¶äºº", "æœ‹å‹", "åŒäº‹", "ä¸€å®¶", "å…¨å®¶"]
        for people in people_keywords:
            if people in text:
                keywords.append(people)
        
        # 5. æå–æ—¶é—´å…³é”®è¯ï¼ˆæ›´è¯¦ç»†ï¼‰
        time_patterns = ["æ˜å¤©", "åå¤©", "ä»Šå¤©", "å‘¨æœ«", "å·¥ä½œæ—¥", "æ—©ä¸Š", "ä¸Šåˆ", "ä¸‹åˆ", "æ™šä¸Š", "å¤œé‡Œ",
                        "ç¬¬ä¸€å¤©", "ç¬¬äºŒå¤©", "ç¬¬ä¸‰å¤©", "ç¬¬å››å¤©", "ç¬¬äº”å¤©", "å‡ å¤©", "å¤šå¤©"]
        for time_word in time_patterns:
            if time_word in text:
                keywords.append(time_word)
        
        # 6. æå–åå¥½å’Œé™åˆ¶å…³é”®è¯
        preference_keywords = {
            "é¿å¼€äººç¾¤": ["äººå°‘", "ä¸æƒ³äººå¤š", "é¿å¼€äººç¾¤", "æ¸…å‡€", "å®‰é™"],
            "ä¸æƒ³è¿œ": ["ä¸æƒ³è¿œ", "è¿‘ä¸€ç‚¹", "é™„è¿‘", "ä¸è¦å¤ªè¿œ"],
            "æ’é˜Ÿ": ["æ’é˜Ÿ", "ç­‰å¾…", "äººå¤š", "æ‹¥æŒ¤"],
            "äº¤é€š": ["å¼€è½¦", "è‡ªé©¾", "åœ°é“", "å…¬äº¤", "æ‰“è½¦", "èµ°è·¯", "éª‘è½¦", "ä¸å¼€è½¦"],
            "é¢„ç®—": ["ä¾¿å®œ", "ç»æµ", "çœé’±", "è´µ", "é«˜ç«¯", "å¥¢å", "é¢„ç®—"],
            "å¤©æ°”": ["å¤©æ°”", "ä¸‹é›¨", "æ™´å¤©", "é˜´å¤©", "æ¸©åº¦", "å†·", "çƒ­", "é£", "é›ª"]
        }
        
        for pref_type, pref_words in preference_keywords.items():
            if any(word in text for word in pref_words):
                keywords.append(pref_type)
        
        # 7. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ•°å­—+å¤©
        import re
        day_matches = re.findall(r'(\d+)å¤©', text)
        for day_match in day_matches:
            keywords.append(f"{day_match}å¤©")
        
        # 8. æå–ç‰¹æ®Šéœ€æ±‚å…³é”®è¯
        special_needs = ["æµªæ¼«", "æ¸©é¦¨", "åˆºæ¿€", "æ–°é²œ", "ç‰¹è‰²", "åœ°é“", "ç½‘çº¢", "å°ä¼—", "ç»å…¸"]
        for need in special_needs:
            if need in text:
                keywords.append(need)
        
        # å»é‡å¹¶è¿”å›
        return list(set(keywords))
    
    def _prioritize_keywords_for_inputtips(self, keywords: List[str], user_input: str) -> List[str]:
        """ä¸ºè¾“å…¥æç¤ºAPIæ™ºèƒ½æ’åºå…³é”®è¯ä¼˜å…ˆçº§"""
        
        # è¿‡æ»¤æ— æ•ˆå…³é”®è¯ï¼šçº¯æ•°å­—ã€å•ä¸ªå­—ç¬¦ã€å¸¸è§åœç”¨è¯
        invalid_patterns = [
            r'^\d+$',  # çº¯æ•°å­—
            r'^[a-zA-Z]$',  # å•ä¸ªå­—æ¯
            r'^(çš„|äº†|æ˜¯|åœ¨|æœ‰|å’Œ|ä¸|æˆ–|ä½†|è€Œ|ä¹Ÿ|éƒ½|å°±|è¿˜|æ›´|æœ€|å¾ˆ|éå¸¸|ç‰¹åˆ«|éå¸¸|ååˆ†)$',  # åœç”¨è¯
        ]
        import re
        
        filtered_keywords = []
        for keyword in keywords:
            # è·³è¿‡çº¯æ•°å­—
            if keyword.isdigit():
                continue
            # è·³è¿‡å•ä¸ªå­—ç¬¦
            if len(keyword.strip()) <= 1:
                continue
            # è·³è¿‡åœç”¨è¯
            is_invalid = False
            for pattern in invalid_patterns:
                if re.match(pattern, keyword.strip()):
                    is_invalid = True
                    break
            if not is_invalid:
                filtered_keywords.append(keyword)
        
        # å®šä¹‰ä¼˜å…ˆçº§æƒé‡
        priority_scores = {}
        
        for keyword in filtered_keywords:
            score = 0
            
            # 1. åœ°ç‚¹ç±»å…³é”®è¯ä¼˜å…ˆçº§æœ€é«˜
            location_keywords = ["åå¸ˆå¤§", "è¿ªå£«å°¼", "å¤–æ»©", "å—äº¬è·¯", "è±«å›­", "é™†å®¶å˜´", 
                               "æ–°å¤©åœ°", "ç”°å­åŠ", "å¾å®¶æ±‡", "é™å®‰å¯º", "äººæ°‘å¹¿åœº"]
            if keyword in location_keywords:
                score += 100
            
            # 2. å…·ä½“æ™¯ç‚¹å»ºç­‘ä¼˜å…ˆçº§å¾ˆé«˜
            specific_places = ["ä¸œæ–¹æ˜ç ", "ä¸Šæµ·ä¸­å¿ƒ", "é‡‘èŒ‚å¤§å¦", "ç¯çƒé‡‘èä¸­å¿ƒ", "ä¸Šæµ·åšç‰©é¦†", 
                              "ä¸Šæµ·ç§‘æŠ€é¦†", "æœ±å®¶è§’", "ä¸ƒå®å¤é•‡", "æ€å—å…¬é¦†", "æ­¦åº·è·¯"]
            if keyword in specific_places:
                score += 90
            
            # 3. åœ¨ç”¨æˆ·è¾“å…¥ä¸­å‡ºç°ä½ç½®è¶Šé å‰ï¼Œä¼˜å…ˆçº§è¶Šé«˜
            if keyword in user_input:
                position = user_input.find(keyword)
                score += max(50 - position // 10, 10)  # ä½ç½®è¶Šé å‰åˆ†æ•°è¶Šé«˜
            
            # 4. å…³é”®è¯é•¿åº¦é€‚ä¸­çš„ä¼˜å…ˆçº§è¾ƒé«˜ï¼ˆ2-6ä¸ªå­—ç¬¦ï¼‰
            if 2 <= len(keyword) <= 6:
                score += 20
            elif len(keyword) > 6:
                score -= 10  # å¤ªé•¿çš„å…³é”®è¯å¯èƒ½ä¸æ˜¯åœ°ç‚¹
            
            # 5. æ’é™¤ä¸€äº›é€šç”¨è¯æ±‡
            exclude_words = ["å¤©æ°”", "äº¤é€š", "æ™¯ç‚¹", "é¤å…", "ä¸Šæµ·", "æ—…æ¸¸", "æ”»ç•¥", "è´­ç‰©", 
                           "ç¾é£Ÿ", "æ–‡åŒ–", "å¨±ä¹", "è‡ªç„¶", "å•†åŠ¡", "äº²å­", "ä¼‘é—²", "è§‚å…‰"]
            if keyword in exclude_words:
                score -= 50
            
            # 6. æ•°å­—+å¤© çš„å…³é”®è¯ä¸é€‚åˆè¾“å…¥æç¤º
            if keyword.endswith("å¤©") and any(c.isdigit() for c in keyword):
                score -= 30
            
            # 7. äººå‘˜å…³ç³»è¯ä¸é€‚åˆè¾“å…¥æç¤º
            people_words = ["å¥³æœ‹å‹", "è€å©†", "å¦»å­", "çˆ¶æ¯", "å¥³å„¿", "å„¿å­", "å®¶äºº", "æœ‹å‹"]
            if keyword in people_words:
                score -= 40
            
            # 8. åå¥½è¯æ±‡ä¸é€‚åˆè¾“å…¥æç¤º
            preference_words = ["é¿å¼€äººç¾¤", "ä¸æƒ³è¿œ", "æ’é˜Ÿ", "é¢„ç®—", "æµªæ¼«", "æ¸©é¦¨"]
            if keyword in preference_words:
                score -= 35
            
            priority_scores[keyword] = score
        
        # æŒ‰åˆ†æ•°æ’åºï¼Œåªè¿”å›åˆ†æ•°å¤§äº0çš„å…³é”®è¯
        sorted_keywords = sorted(
            [(k, v) for k, v in priority_scores.items() if v > 0], 
            key=lambda x: x[1], 
            reverse=True
        )
        
        result = [k for k, v in sorted_keywords]
        logger.info(f"å…³é”®è¯ä¼˜å…ˆçº§æ’åºç»“æœ: {[(k, priority_scores[k]) for k in result[:10]]}")
        
        return result
    
    def _extract_travel_days(self, text: str) -> int:
        """æå–æ—…è¡Œå¤©æ•°"""
        import re
        
        # åŒ¹é…æ•°å­—+å¤©/æ—¥
        day_patterns = [
            r'(\d+)\s*å¤©',
            r'(\d+)\s*æ—¥',
            r'(\d+)\s*å¤©æ¸¸',
            r'(\d+)\s*æ—¥æ¸¸'
        ]
        
        for pattern in day_patterns:
            match = re.search(pattern, text)
            if match:
                days = int(match.group(1))
                return max(1, min(days, 7))  # é™åˆ¶åœ¨1-7å¤©
        
        # å¦‚æœæ²¡æœ‰æ˜ç¡®æŒ‡å®šï¼Œæ ¹æ®å…³é”®è¯æ¨æ–­
        if "ä¸‰å¤©" in text or "3å¤©" in text:
            return 3
        elif "ä¸¤å¤©" in text or "2å¤©" in text:
            return 2
        elif "ä¸€å¤©" in text or "1å¤©" in text:
            return 1
        elif "å››å¤©" in text or "4å¤©" in text:
            return 4
        elif "äº”å¤©" in text or "5å¤©" in text:
            return 5
        elif "æœªæ¥" in text and "å¤©" in text:
            return 3  # é»˜è®¤3å¤©
        
        return 1  # é»˜è®¤1å¤©
    
    def _analyze_user_intent(self, user_input: str) -> Tuple[List[str], List[str]]:
        """åˆ†æç”¨æˆ·æ„å›¾"""
        detected_locations = []
        activity_types = []
        
        # æ£€æµ‹åœ°ç‚¹
        for location, attractions in self.location_keywords.items():
            if location in user_input:
                detected_locations.append(location)
        
        # æ£€æµ‹æ´»åŠ¨ç±»å‹
        for activity, keywords in self.activity_keywords.items():
            if any(keyword in user_input for keyword in keywords):
                activity_types.append(activity)
        
        return detected_locations, activity_types
    
    def _extract_locations_from_input(self, user_input: str) -> List[str]:
        """ä»ç”¨æˆ·è¾“å…¥ä¸­æå–åœ°ç‚¹ä¿¡æ¯"""
        locations = []
        
        # ä¸Šæµ·åœ°åŒºå…³é”®è¯
        shanghai_areas = [
            "å¤–æ»©", "äººæ°‘å¹¿åœº", "å—äº¬è·¯", "è±«å›­", "é™†å®¶å˜´", "ä¸œæ–¹æ˜ç ", 
            "ä¸Šæµ·è¿ªå£«å°¼", "ä¸Šæµ·åšç‰©é¦†", "ä¸Šæµ·ç§‘æŠ€é¦†", "ç”°å­åŠ", "æ–°å¤©åœ°",
            "é‡‘æ²™æ±Ÿè·¯", "ä¸­å±±å…¬å›­", "é™å®‰å¯º", "å¾å®¶æ±‡", "äº”è§’åœº", "è™¹æ¡¥",
            "æµ¦ä¸œ", "æµ¦è¥¿", "é»„æµ¦åŒº", "é™å®‰åŒº", "å¾æ±‡åŒº", "é•¿å®åŒº", "æ™®é™€åŒº",
            "åä¸œå¸ˆèŒƒå¤§å­¦", "åä¸œå¸ˆå¤§", "åå¸ˆå¤§", "å¾æ±‡", "æ™®é™€"
        ]
        
        for area in shanghai_areas:
            if area in user_input:
                locations.append(area)
        
        # å»é‡å¹¶è¿‡æ»¤
        locations = list(set(locations))
        return locations
    
    def _is_valid_location(self, location_name: str, keyword: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆçš„åœ°ç‚¹åç§°"""
        if not location_name or len(location_name.strip()) < 2:
            return False
        
        # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯åœ°ç‚¹çš„ç»“æœ
        invalid_patterns = ['%', 'ä¼šè®®', 'ä¸­å¿ƒ', 'è´­ç‰©', 'è‰ºæœ¯ä¸­å¿ƒ']
        location_lower = location_name.lower()
        
        # å¦‚æœå…³é”®è¯æ˜¯æ•°å­—ï¼Œç›´æ¥æ‹’ç»
        if keyword.isdigit():
            return False
        
        # å¦‚æœåœ°ç‚¹åç§°åŒ…å«å…³é”®è¯ï¼Œè®¤ä¸ºæ˜¯ç›¸å…³çš„
        if keyword in location_name:
            return True
        
        # å¦‚æœåœ°ç‚¹åç§°åŒ…å«æ— æ•ˆæ¨¡å¼ï¼Œæ‹’ç»
        for pattern in invalid_patterns:
            if pattern in location_name and keyword not in location_name:
                return False
        
        return True
    
    def _extract_route_from_input(self, user_input: str) -> Optional[Dict[str, str]]:
        """ä»ç”¨æˆ·è¾“å…¥ä¸­æå–è·¯çº¿ä¿¡æ¯"""
        # ç®€å•çš„è·¯çº¿æå–é€»è¾‘
        if "ä»" in user_input and "åˆ°" in user_input:
            parts = user_input.split("ä»")[1].split("åˆ°")
            if len(parts) >= 2:
                start = parts[0].strip()
                end = parts[1].split()[0].strip()  # å–ç¬¬ä¸€ä¸ªè¯ä½œä¸ºç»ˆç‚¹
                return {"start": start, "end": end}
        
        return None
    
    def _infer_route_from_input(self, user_input: str) -> Optional[Dict[str, str]]:
        """ä»ç”¨æˆ·è¾“å…¥ä¸­æ¨æ–­è·¯çº¿ä¿¡æ¯"""
        # ç‰¹æ®Šå¤„ç†ï¼šåä¸œå¸ˆèŒƒå¤§å­¦åˆ°å¾æ±‡åŒº
        if "åä¸œå¸ˆèŒƒå¤§å­¦" in user_input and "å¾æ±‡åŒº" in user_input:
            return {"start": "åä¸œå¸ˆèŒƒå¤§å­¦", "end": "å¾æ±‡åŒº"}
        
        # æå–åœ°ç‚¹ä¿¡æ¯
        locations = self._extract_locations_from_input(user_input)
        
        # å¦‚æœæ‰¾åˆ°å¤šä¸ªåœ°ç‚¹ï¼Œå°è¯•æ¨æ–­èµ·ç‚¹å’Œç»ˆç‚¹
        if len(locations) >= 2:
            # æ ¹æ®ç”¨æˆ·è¾“å…¥ä¸­çš„å…³é”®è¯æ¨æ–­
            if "å‡ºå‘" in user_input:
                # æ‰¾åˆ°"å‡ºå‘"å‰é¢çš„åœ°ç‚¹ä½œä¸ºèµ·ç‚¹
                for i, location in enumerate(locations):
                    if location in user_input[:user_input.find("å‡ºå‘")]:
                        start = location
                        # å…¶ä»–åœ°ç‚¹ä½œä¸ºç»ˆç‚¹
                        end = locations[(i + 1) % len(locations)]
                        return {"start": start, "end": end}
            
            # å¦‚æœæ²¡æœ‰"å‡ºå‘"å…³é”®è¯ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªåœ°ç‚¹ä½œä¸ºèµ·ç‚¹ï¼Œæœ€åä¸€ä¸ªä½œä¸ºç»ˆç‚¹
            return {"start": locations[0], "end": locations[-1]}
        
        return None
    

    
    # MCPæœåŠ¡æ–¹æ³•ï¼ˆä»smart_travel_agent.pyç§»æ¤ï¼‰
    def _rate_limit_wait(self, api_name: str):
        """APIé™æµæ§åˆ¶ - ç¡®ä¿ä¸è¶…è¿‡QPSé™åˆ¶"""
        with self._api_lock:
            current_time = time.time()
            if api_name in self._last_api_call:
                elapsed = current_time - self._last_api_call[api_name]
                if elapsed < self._min_interval:
                    wait_time = self._min_interval - elapsed
                    logger.debug(f"é™æµç­‰å¾… {wait_time:.2f}ç§’ for {api_name}")
                    time.sleep(wait_time)
            self._last_api_call[api_name] = time.time()
    
    def _make_request(self, url: str, params: Dict[str, Any], api_name: str = "default") -> Dict[str, Any]:
        """å‘é€HTTPè¯·æ±‚ï¼ˆå¸¦é™æµæ§åˆ¶ï¼‰"""
        try:
            # é™æµæ§åˆ¶
            self._rate_limit_wait(api_name)
            
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"APIè¯·æ±‚å¤±è´¥: {url}, é”™è¯¯: {e}")
            return {}
    
    def get_weather(self, city: str, date: str = None) -> List[WeatherInfo]:
        """è·å–å¤©æ°”ä¿¡æ¯ - ä½¿ç”¨MCPæœåŠ¡"""
        return self.mcp_client.call_service(MCPServiceType.WEATHER, city=city, date=date) or []
    
    def get_navigation_routes(self, origin: str, destination: str, 
                            transport_mode: str = "driving") -> List[RouteInfo]:
        """è·å–å¯¼èˆªè·¯çº¿ - ä½¿ç”¨MCPæœåŠ¡"""
        return self.mcp_client.call_service(
            MCPServiceType.NAVIGATION,
            origin=origin,
            destination=destination,
            transport_mode=transport_mode
        ) or []
    
    def get_traffic_status(self, area: str) -> Dict[str, Any]:
        """è·å–è·¯å†µä¿¡æ¯ - ä½¿ç”¨MCPæœåŠ¡"""
        result = self.mcp_client.call_service(MCPServiceType.TRAFFIC, area=area)
        if result:
            return result
        # è¿”å›é»˜è®¤æ•°æ®
        return {
            "status": "æ­£å¸¸",
            "description": "è·¯å†µè‰¯å¥½",
            "evaluation": {"level": "1", "status": "ç•…é€š"},
            "timestamp": datetime.now().isoformat()
        }
            
    def search_poi(self, keyword: str, city: str, category: str = None) -> List[POIInfo]:
        """æœç´¢POIä¿¡æ¯ - ä½¿ç”¨MCPæœåŠ¡"""
        return self.mcp_client.call_service(
            MCPServiceType.POI,
            keyword=keyword,
            city=city,
            category=category
        ) or []
    
    def _filter_shanghai_only(self, pois: List[POIInfo]) -> List[POIInfo]:
        """è¿‡æ»¤æ‰éä¸Šæµ·åœ°åŒºçš„POIï¼Œç¡®ä¿åªè¿”å›ä¸Šæµ·æ™¯ç‚¹"""
        filtered = []
        
        # éä¸Šæµ·åŸå¸‚å…³é”®è¯ï¼ˆæ’é™¤ä¸Šæµ·çš„è¡—é“åï¼‰
        non_shanghai_cities = [
            "åŒ—äº¬", "å¹¿å·", "æ·±åœ³", "æ­å·", "å—äº¬", "è‹å·", "æˆéƒ½", "é‡åº†",
            "è¥¿å®‰", "æ­¦æ±‰", "å¤©æ´¥", "é•¿æ²™", "éƒ‘å·", "æµå—", "é’å²›", "å¤§è¿",
            "å¦é—¨", "ç¦å·", "åˆè‚¥", "å—æ˜Œ", "çŸ³å®¶åº„", "å¤ªåŸ", "å“ˆå°”æ»¨", "é•¿æ˜¥",
            "æ²ˆé˜³", "æ˜†æ˜", "è´µé˜³", "å—å®", "æµ·å£", "ä¹Œé²æœ¨é½", "æ‹‰è¨", "é“¶å·",
            "è¥¿å®", "å…°å·", "å‘¼å’Œæµ©ç‰¹"
        ]
        
        # ä¸Šæµ·çš„è¡—é“åï¼ˆè¿™äº›åº”è¯¥ä¿ç•™ï¼‰
        shanghai_streets = [
            "åŒ—äº¬ä¸œè·¯", "åŒ—äº¬è¥¿è·¯", "å—äº¬ä¸œè·¯", "å—äº¬è¥¿è·¯", "æ·®æµ·ä¸œè·¯", "æ·®æµ·è¥¿è·¯",
            "ä¸­å±±åŒ—è·¯", "ä¸­å±±å—è·¯", "ä¸­å±±ä¸­è·¯", "ä¸­å±±ä¸œè·¯", "ä¸­å±±å—è·¯", "ä¸­å±±åŒ—è·¯",
            "å»¶å®‰ä¸œè·¯", "å»¶å®‰è¥¿è·¯", "å»¶å®‰ä¸­è·¯", "å››å·åŒ—è·¯", "å››å·å—è·¯", "å››å·ä¸­è·¯"
        ]
        
        for poi in pois:
            name = poi.name or ""
            address = poi.address or ""
            full_text = f"{name} {address}".lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«éä¸Šæµ·åŸå¸‚å…³é”®è¯
            is_non_shanghai = False
            for city in non_shanghai_cities:
                if city in full_text:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸Šæµ·çš„è¡—é“å
                    is_shanghai_street = any(street in name or street in address for street in shanghai_streets)
                    if not is_shanghai_street:
                        is_non_shanghai = True
                        logger.warning(f"è¿‡æ»¤éä¸Šæµ·POI: {name} (åœ°å€: {address}) - åŒ…å«åŸå¸‚: {city}")
                        break
            
            # æ£€æŸ¥åœ°å€ä¸­æ˜¯å¦æ˜ç¡®åŒ…å«éä¸Šæµ·åŸå¸‚
            if not is_non_shanghai:
                # æ£€æŸ¥districtsæ ¼å¼ï¼ˆå¦‚"åŒ—äº¬Â·åŒ—äº¬Â·æœé˜³åŒº"ï¼‰
                if "Â·" in address:
                    parts = address.split("Â·")
                    if len(parts) >= 2 and parts[0] not in ["ä¸Šæµ·", "Shanghai", "shanghai"]:
                        is_non_shanghai = True
                        logger.warning(f"è¿‡æ»¤éä¸Šæµ·POI: {name} (åœ°å€: {address}) - districtsæ ¼å¼æ˜¾ç¤ºéä¸Šæµ·")
            
            if not is_non_shanghai:
                filtered.append(poi)
        
        if len(filtered) < len(pois):
            logger.info(f"POIè¿‡æ»¤: åŸå§‹{len(pois)}ä¸ªï¼Œè¿‡æ»¤å{len(filtered)}ä¸ªï¼ˆå·²è¿‡æ»¤{len(pois) - len(filtered)}ä¸ªéä¸Šæµ·POIï¼‰")
        
        return filtered
    
    def get_inputtips(self, keywords: str, city: str = "ä¸Šæµ·", 
                      poi_type: str = None, location: str = None, 
                      citylimit: bool = False, datatype: str = "all") -> List[Dict[str, Any]]:
        """è·å–è¾“å…¥æç¤º - æ ¹æ®å…³é”®è¯è¿”å›å»ºè®®åˆ—è¡¨
        
        Args:
            keywords: æŸ¥è¯¢å…³é”®è¯
            city: æœç´¢åŸå¸‚ï¼ˆé»˜è®¤ï¼šä¸Šæµ·ï¼‰
            poi_type: POIåˆ†ç±»ä»£ç ï¼Œå¤šä¸ªç”¨"|"åˆ†éš”
            location: åæ ‡ï¼Œæ ¼å¼"ç»åº¦,çº¬åº¦"ï¼Œå¯ä¼˜å…ˆè¿”å›æ­¤ä½ç½®é™„è¿‘çš„ç»“æœ
            citylimit: æ˜¯å¦ä»…è¿”å›æŒ‡å®šåŸå¸‚æ•°æ®ï¼ˆTrue/Falseï¼‰
            datatype: è¿”å›æ•°æ®ç±»å‹ï¼ˆall/poi/bus/buslineï¼‰
            
        Returns:
            å»ºè®®åˆ—è¡¨
        """
        logger.info(f"è°ƒç”¨è¾“å…¥æç¤ºAPI: {keywords} in {city}")
        
        try:
            params = {
                "key": get_api_key("AMAP_PROMPT"),
                "keywords": keywords,
                "city": city,
                "citylimit": "true" if citylimit else "false",
                "datatype": datatype
            }
            
            # å¯é€‰å‚æ•°
            if poi_type:
                params["type"] = poi_type
            if location:
                params["location"] = location
            
            result = self._make_request(AMAP_CONFIG["inputtips_url"], params, "inputtips")
            
            if result.get("status") == "1":
                tips = []
                for tip_data in result.get("tips", []):
                    tip_info = {
                        "id": tip_data.get("id", ""),
                        "name": tip_data.get("name", ""),
                        "district": tip_data.get("district", ""),
                        "adcode": tip_data.get("adcode", ""),
                        "location": tip_data.get("location", ""),
                        "address": tip_data.get("address", ""),
                        "typecode": tip_data.get("typecode", "")
                    }
                    tips.append(tip_info)
                
                logger.info(f"è¾“å…¥æç¤ºAPIè°ƒç”¨æˆåŠŸ: {keywords} - {len(tips)}ä¸ªå»ºè®®")
                return tips
            else:
                logger.error(f"è¾“å…¥æç¤ºAPIè°ƒç”¨å¤±è´¥: {result.get('info', 'æœªçŸ¥é”™è¯¯')}")
                
        except Exception as e:
            logger.error(f"è·å–è¾“å…¥æç¤ºå¤±è´¥: {e}")
        
        return []
    
    def _geocode(self, address: str) -> Optional[str]:
        """åœ°ç†ç¼–ç """
        try:
            params = {
                "key": get_api_key("AMAP_POI"),
                "address": address
            }
            
            result = self._make_request(AMAP_CONFIG["geocode_url"], params, "geocode")
            
            if result.get("status") == "1":
                geocodes = result.get("geocodes", [])
                if geocodes:
                    return geocodes[0].get("location", "")
        except Exception as e:
            logger.error(f"åœ°ç†ç¼–ç å¤±è´¥: {e}")
        
        return None
    
    def _get_city_code(self, city_name: str) -> str:
        """è·å–åŸå¸‚ä»£ç """
        city_codes = {
            "ä¸Šæµ·": "310000", "åŒ—äº¬": "110000", "å¹¿å·": "440100",
            "æ·±åœ³": "440300", "æ­å·": "330100", "å—äº¬": "320100",
            "è‹å·": "320500", "æˆéƒ½": "510100", "é‡åº†": "500000"
        }
        return city_codes.get(city_name, "310000")
    
    def _get_area_coordinates(self, area: str) -> Optional[str]:
        """è·å–åŒºåŸŸåæ ‡èŒƒå›´"""
        area_coords = {
            "å¤–æ»©": "121.4805,31.2304,121.5005,31.2504",
            "é™†å®¶å˜´": "121.4978,31.2297,121.5178,31.2497",
            "äººæ°‘å¹¿åœº": "121.4637,31.2216,121.4837,31.2416"
        }
        return area_coords.get(area, None)
    
    def _format_transit_route(self, route: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å…¬äº¤è·¯çº¿æè¿°"""
        segments = route.get("segments", [])
        description = []
        
        for segment in segments:
            bus_info = segment.get("bus", {})
            if bus_info:
                bus_name = bus_info.get("busname", "")
                bus_stops = bus_info.get("buslines", [{}])[0].get("departure_stop", "")
                arrival_stops = bus_info.get("buslines", [{}])[0].get("arrival_stop", "")
                description.append(f"{bus_name}: {bus_stops} â†’ {arrival_stops}")
        
        return " â†’ ".join(description)
    
    def _format_driving_route(self, route: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–é©¾è½¦è·¯çº¿æè¿°"""
        steps = route.get("steps", [])
        description = []
        
        for step in steps[:3]:
            instruction = step.get("instruction", "")
            if instruction:
                description.append(instruction.split("ï¼Œ")[0])
        
        return " â†’ ".join(description)

def main():
    """æµ‹è¯•å¢å¼ºç‰ˆAgent"""
    agent = EnhancedTravelAgent()
    
    print("ğŸ¤– å¢å¼ºç‰ˆæ™ºèƒ½æ—…è¡Œå¯¹è¯Agent (è±†åŒ…ç‰ˆ)")
    print("=" * 60)
    print("è¾“å…¥ 'quit' é€€å‡ºå¯¹è¯")
    print("=" * 60)
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ æ‚¨: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            if not user_input:
                continue
            
            # å¤„ç†ç”¨æˆ·è¯·æ±‚
            response = agent.process_user_request(user_input, "test_user")
            
            print(f"\nğŸ¤– Agent: {response}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    main()
